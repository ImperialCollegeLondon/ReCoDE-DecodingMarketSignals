{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#decoding-market-signals","title":"Decoding Market Signals","text":""},{"location":"#leveraging-candlestick-patterns-machine-learning-and-alpha-signals-for-enhanced-trading-strategy-analysis","title":"Leveraging candlestick patterns, machine learning and alpha signals for enhanced trading strategy analysis","text":""},{"location":"#description","title":"Description","text":"<p>This project aims to rigorously back-test a trading strategy, focusing on evaluating the informational value of  candlestick patterns. Utilising <code>Python</code>, a pipeline of functions will systematically scan and evaluate  the components of the S&amp;P 500 stock market index for patterns upon which one can base a trading decision.</p> <p>Advanced functionalities of the <code>Pandas</code> library will be employed to load, manipulate and store detailed statistical  data, particularly <code>method chains</code>, <code>multi-indexed data frames</code> and <code>user-defined functions</code> acting on rows and columns of data frames.</p> <p>The project's core involves assessing the predictive capabilities of these trading signals using nuanced binary classification performance  metrics, thereby determining their practical applicability. Additionally, a logistic regression model will be deployed  to explore the intersection of finance and machine learning.  This phase aims to ascertain whether machine learning algorithms can outperform traditional methods in predicting market  movements based on identified signals.</p> <p>This multifaceted project integrates financial analysis, data science, and machine learning,  promising insights with both academic and practical implications.  Its methodologically sound approach, coupled with detailed documentation and learning annotations,  is designed to make it an exemplary contribution to the ReCoDE initiative,  showcasing the transformative potential of research computing and data science in interdisciplinary research.</p>"},{"location":"#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Setting up a custom computational environment for financial data science.</li> <li>Making a custom technical analysis library written in C++ work with recent Python. </li> <li>Obtaining and pre-processing high-quality financial data.</li> <li>Using Pandas' best-practices like method-chaining, and multi-index data frames for data manipulation  </li> <li>Independently testing and analysing trading actions proposed on a hypothesis.</li> </ul> Task Time Reading 4 hours Practising 6 hours"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Foundational knowledge of Python and Pandas.</li> <li>An interest in stock markets and trading signals.</li> <li>An interest in statistical analysis and hypothesis testing.</li> <li>Resilience in troubleshooting and adapting older libraries to work with recent Python versions.</li> <li>Particularly, we will make use of a library called <code>ta-lib</code> that contains a pattern-recognition library detecting candlestick patterns in Open-High-Low-Close <code>(OHCL)</code> data.</li> <li>Familiarity with Jupyter notebooks, type annotations, and automation.</li> </ul>"},{"location":"#academic","title":"Academic","text":"<p>The repository is self-contained. Additional references are provided in the Jupyter notebooks. When we move form a single-stock analysis to the whole investment-universe, the resulting data frames  become too large to work with on a standard machine leading to Kernel crashes. They are themselves not dangerous to the hardware of the computer at all, and one can mitigate this by selecting a subset of the data. If you wish to run the  code on all the data, you need a potent machine, or alternatively execute the code on the HPC facilities.  That does not hinder you from getting started, though.</p>"},{"location":"#system","title":"System","text":"<p>A recent mid-class laptop is sufficient to follow along the code. The more data you wish to analyse, the more RAM it should have. The code was developed on a Linux machine. </p> <p>In this code exemplary, we make use of <code>Python 3.11</code>.  Identifying the candlestick patterns in financial markets data is obtained by using a library that is called <code>ta-lib</code>. It works well for our task, but its Python wrapper is no longer maintained. If you are comfortable with an older version of Python, precisely  <code>Python 3.8</code> or <code>Python 3.9</code>, or just want to get started, it is straightforward to install an older version using <code>pip</code> or <code>conda</code>. </p> <p><code>ta-lib</code> was tested to be installable from <code>pypi</code> on <code>Python 3.8</code> and <code>3.9</code>. If you just want to get started, use <code>Python 3.8</code>. <code>ta-lib</code> can then be installed using <code>pip install TA-Lib</code>.  Alternatively, if you want to make use of the <code>conda</code> package manager, use <code>conda install ta-lib</code> For <code>Python 3.9</code>, the author observed on a Linux operating system, that <code>conda install ta-lib</code> worked straightforward,  whereas <code>pip install ta-lib</code> did not.</p> <p>If you want to make use of later versions of Python such as the environment this project was developed on, precisely  <code>Python v. 3.11</code>, the process is more involved and requires compiling <code>ta-lib's C++</code> files from source.</p> <p>We now encountered two common problems, we frequently face as computer and data scientists: i) Making <code>legacy</code> code run on modern systems, ii) Facing multiple choices of which package manager to use. If you are just interested on getting started, use <code>Python 3.8</code> and skip the following section.</p> <p>For the interested reader, is follows some background information on i) and ii). Issue i) is commonly encountered in practice, especially in larger corporations operating with custom software, whose author's stopped  maintaining their code. There is no silver bullet on working with legacy code and custom problems often need tailor-made solutions. For this project, the author wrote a <code>shell</code> script that is custom-made to set up <code>ta-lib</code> for <code>Python 3.11</code>. Whenever you are not sure whether a solution works and you have reasons to believe your attempt is error-prone, might have side effects, or spoil the operating system, it is advisable  to work from within a virtual environment, for example using <code>Docker</code>, before employing a working solution on the user's machine.  Discussing <code>Docker</code> is beyond the scope of this documentation, however.</p> <p>The <code>shell script</code> can also be directly applied to work on macOS as the latter uses the <code>Z shell</code> by default.  The <code>Z shell</code>, is also known as <code>zsh</code> and is a <code>Unix shell</code> that is built on top of <code>bash</code>. Hence, compatibility  is likely and the script should run without reservation.  For Windows users the <code>shell script</code> can also be modified to work on the <code>Windows shell</code> or <code>PowerShell</code>. The equivalent of a Linux <code>shell script</code> on Windows is a <code>batch script</code> and the commands expressed have to be translated  to make them compatible on Windows.</p> <p>Let us now quickly address issue ii): If your Python environment is set up using miniconda (recommended), see also <code>https://docs.conda.io/projects/miniconda/en/latest/</code>, both, <code>conda</code> and <code>pip</code> are installed by default, and you can make use of both them.  If your Python environment is set up using the source files from <code>https://www.python.org/</code> you might have to install <code>pip</code> separately and cannot use the benefits of conda.</p> <p>What then is the difference between <code>pip</code> and <code>conda</code>? <code>Pip</code> is a package manager specifically designed for Python packages. It primarily focuses on installing and managing Python libraries and packages from the <code>Python Package Index (PyPI)</code>. Pip is used for managing Python dependencies within a Python environment. On the other hand, <code>Conda</code> is a more comprehensive package manager and environment manager. While it can manage Python packages, it is not limited to Python and can handle packages and libraries from various programming languages. Conda is often used to create isolated environments that can include different versions of Python and non-Python dependencies. It can manage both Python packages and system-level packages and is capable of handling complex dependency resolution.</p> <p>Managing and detecting version conflicts of a large Python setup is again a topic on its own. Granted <code>conda</code> is diligent, but slow, the  reader is encouraged to look into promising alternatives like <code>mamba</code>, which is a package manager written in <code>C++</code> and hence more performant  than <code>conda</code>, although less tested.</p> <p>A final note regarding code-formatting. To comply with the PEP-8 style guide for Python code, <code>https://peps.python.org/pep-0008/</code>, we make use of a code-formatter, that automatically spots issues concerning spacing and style. It is applied on code that runs error-free and ensures style consistency. There are several open-source code-formatters out there and arguably the most popular are <code>black</code>, see <code>https://github.com/psf/black</code>, and <code>Ruff</code>, see <code>https://docs.astral.sh/ruff/formatter/</code>. The former is well-tested, however the latter is more performant and has recently gained increasing attention. Hence, we make use of Ruff.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#windowsmacos","title":"Windows/MacOS","text":"<p>For Windows and MacOS based machines, you will have to install the <code>ta-lib</code> library separately. Instructions to do so can be found here. Once installed, you can create a virtual environment as suggested below and use <code>pip install ta-lib</code> to install the required Python wrapper, and <code>pip install -r requirements.txt</code> to install the other dependencies. </p>"},{"location":"#linux","title":"Linux","text":"<p>The following code was tested on <code>Ubuntu 22.04.4 LTS</code> using <code>Python 3.11</code>. For other Linux distributions you need to modify the commands  that install software on your system. For example, on <code>Fedora</code>, the default package manager is <code>dnf</code> rather than Ubuntu's <code>apt</code>. Also, you might modify <code>python3.11 -m venv MarketSignals</code> to whatever python version you have. Typically, one can also use the default system Python. The first  line in the below sequence then reads <code>python -m venv MarketSignals</code> rather than <code>python3.11 -m venv MarketSignals</code>. However, on my setup, I opted to install a more recent version of Python , here 3.11. I then have to explicitly call this version to create a virtual environment. Besides, I opted to leave Ubuntu's default system Python  alone. The reason is somewhat more intricate. In short, in contrast to Windows, Ubuntu's bootloader <code>grub</code> critically depends on this system Python. If, for any reason you spoil this system Python, you cannot just delete and re-install it. Should you try to do, your system will not  boot anymore. Hence, I leave the system Python untouched and instead opted for a separate Python version explicitly set for development work.  This <code>dev work Python</code>, I can modify, fine-tune, delete, and re-install without ever even touching the system Python, making it safer to experiment. </p> <p>Execute the following sequences in your Linux terminal to set up your Python environment needed to run the <code>DecodingMarketSignals</code> repository. <pre><code>python3.11 -m venv MarketSignals  # create a virtual environment\nsource MarketSignals/bin/activate  # activate the virtual environment\nsudo apt-get install python3.11-dev  # installs the Python 3.11 development files on your Ubuntu or Debian-based Linux system.\npip install -r requirements.txt  # install dependencies via pip \nchmod +x install-talib.sh  # set execution rights for the shell script to install TA-lib from source\n./install-talib.sh  # run the script installing TA-lib  \njupyter-notebook  # optional: launch an instance of Jupyter notebook and run the examples (assumes you downloaded the CRSP data already).\n</code></pre></p> <p>Start by opening and reading through the Jupyter notebook. All essential steps are separated in respective sub-sections. Once you have an understanding of the overall goal, you can start setting up your Python environment along `ta-lib and  either replicate the results or apply the techniques demonstrated to your own data. The reader is encouraged to apply the methods outlined  on their own data from different markets, for instance, the futures and forex markets. </p>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>.\n\n\u251c\u2500\u2500 data\n\n\u2502   \u251c\u2500\u2500 SP500_daily_data_1980_to_2023.csv.gz (to be downloaded by the user via WRDS)\n\n\u2502   \u2514\u2500\u2500 SP500_tickers_one_per_line.txt\n\n\u251c\u2500\u2500 figures\n\n|   \u251c\u2500\u2500 candlestick_anatomy.png\n\n|   \u251c\u2500\u2500 WRDS_overview.png\n\n|   \u251c\u2500\u2500 CRSP1_select_quarterly.png\n\n|   \u251c\u2500\u2500 ...\n\n\u2502   \u2514\u2500\u2500 WRDS_overview.png\n\n\u251c\u2500\u2500 notebooks\n\n|   \u251c\u2500\u2500 BSquant.py\n\n|   \u251c\u2500\u2500 1_obtaining_financial_data.ipynb\n\n|   \u251c\u2500\u2500 2_single_stock_case.ipynb\n\n|   \u251c\u2500\u2500 3_SP500_case.ipynb\n\n\u251c\u2500\u2500 install-talib.sh\n\n\u251c\u2500\u2500 requirements.txt\n\n\u251c\u2500\u2500 mkdocs.yml\n\n\u251c\u2500\u2500 LICENSE.md\n\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>"},{"location":"1_obtaining_financial_data/","title":"Obtaining financial data","text":"In\u00a0[5]: Copied! <pre>from IPython.display import Image\nimport datetime\nimport contextlib\nfrom typing import Generator\nimport wrds\n</pre> from IPython.display import Image import datetime import contextlib from typing import Generator import wrds In\u00a0[6]: Copied! <pre>Image(filename=\"./../figures/WRDS_overview.png\")\n</pre> Image(filename=\"./../figures/WRDS_overview.png\") Out[6]: <ol> <li>Under <code>Quarterly Update</code>, click on <code>Stock - Version 2</code>.</li> </ol> In\u00a0[7]: Copied! <pre>Image(filename=\"./../figures/CRSP1_select_quarterly.png\")\n</pre> Image(filename=\"./../figures/CRSP1_select_quarterly.png\") Out[7]: <ol> <li>Under <code>Stock - Version 2</code>, click on <code>Daily Stock File</code>.</li> </ol> In\u00a0[8]: Copied! <pre>Image(filename=\"./../figures/CRSP2.png\")\n</pre> Image(filename=\"./../figures/CRSP2.png\") Out[8]: <p>Finally, you can see a mask that allows you to select a date range of your choosing. For this exemplar, I go all the way back from <code>1980-01-01</code> to the most recent date available at the time of writing, <code>2023-12-29</code>.</p> <p>Below, see a mask to select the format of the identifier of the stock, or company codes, you wish to query data for. I suggest using <code>Ticker (ticker)</code>.</p> <pre><code>    To help, you will find a list of tickers for the S&amp;P 500 components as of the end of 2023 in the text file `data/SP500_tickers_one_per_line.txt` in the file directory of this exemplar. If you work for a bank or financial institution, they will ususlly have an *Operations* team that tracks any tickers at any exchange they trade on. Tickers, as well as index components can change in time, as members will be included and others excluded from an index. Hence, the composition of the ticker list can change. I obtained the tickers from https://www.slickcharts.com/sp500 on 4 December 2023, which is a public available source. I encourage you to get this data from the web, comparing them to the tickers contained and updating any changes.</code></pre> <p>Now back to WRDS. If you want to query data for more than one stock, upload the file mentioned above using the Upload a plain text file (.txt), having one code per line option.</p> <p>You can then go ahead and select the query variables you wish to query for each of the tickers. For the purpose of this exemplar, we need to obtain the following:</p> <ul> <li>Daily Price (DlyPrc)</li> <li>Daily Open (DlyOpen)</li> <li>Daily High (DlyHigh)</li> <li>Daily Low (DlyLow)</li> <li>Daily Close (DlyClose)</li> <li>Daily Volume (DlyVol)</li> <li>Daily Price Volume (DlyPrcVol)</li> </ul> <p>You can choose the output file format, and compression options, which is suggested for larger data requests. For this exemplar we use <code>CSV</code> compressed as  <code>gzip</code> with dates formatted as <code>YYYY-MM-DD</code>.</p> <p>Clicking on <code>Submit Form</code> will query the data, taking you to a window that will eventually provide a download link for your data. If notifications are configured, you can receive an email notifying you once the process is complete. With this data you can progress importing it to the program of choice to conduct your research on the data.</p> <p>For other ways to query data through WRDS, see below, otherwise move through the rest of this exemplar.</p> In\u00a0[9]: Copied! <pre>Image(filename=\"./../figures/CRSP3_select_data.png\")\n</pre> Image(filename=\"./../figures/CRSP3_select_data.png\") Out[9]: In\u00a0[13]: Copied! <pre># when working with databases, it is of crucial importance to close the connection one we are done.\n# Invoking db.close() would do, but this is easily forgotten. Hence, we make use of a context manager which takes\n# care of this issue automatically for us. We can use the contextmanager decorator to obtain this behaviour.\n\n\n@contextlib.contextmanager\ndef wrds_connection(wrds_username: str) -&gt; Generator[wrds.Connection, None, None]:\n    db = None\n    try:\n        db = wrds.Connection(wrds_username=wrds_username)\n        yield db\n    finally:\n        if db is not None:\n            db.close()\n</pre> # when working with databases, it is of crucial importance to close the connection one we are done. # Invoking db.close() would do, but this is easily forgotten. Hence, we make use of a context manager which takes # care of this issue automatically for us. We can use the contextmanager decorator to obtain this behaviour.   @contextlib.contextmanager def wrds_connection(wrds_username: str) -&gt; Generator[wrds.Connection, None, None]:     db = None     try:         db = wrds.Connection(wrds_username=wrds_username)         yield db     finally:         if db is not None:             db.close() In\u00a0[14]: Copied! <pre># this will be the SQL query we run. Going into details regarding the language is beyon the scope, however,\n# it is a reasonably human-readable language.\n\nSTART_DATE = \"1900-01-01\"\nEND_DATE = \"2023-12-29\"\n\nsplist_query = \"\"\"\nSELECT *\nFROM crsp.dsp500list\nWHERE ending &gt;= %(start_date)s AND ending &lt;= %(end_date)s;\n\"\"\"\n</pre> # this will be the SQL query we run. Going into details regarding the language is beyon the scope, however, # it is a reasonably human-readable language.  START_DATE = \"1900-01-01\" END_DATE = \"2023-12-29\"  splist_query = \"\"\" SELECT * FROM crsp.dsp500list WHERE ending &gt;= %(start_date)s AND ending &lt;= %(end_date)s; \"\"\" <p>In order to execute the below cell, you need to pass your <code>wrds_username</code> as an argument.</p> In\u00a0[15]: Copied! <pre># using the context manager for secure and clean database connection handling.\n# Also take care to use your very own wrds_username that was assigned to you oce you applied for a WRDS account\n# and call the below with wrds_connection(wrds_username='your_individually_assigned_username')\n\nwith wrds_connection(wrds_username=\"YOUR_WRDS_USERNAME\") as db:\n    splist_data = db.raw_sql(\n        splist_query, params={\"start_date\": START_DATE, \"end_date\": END_DATE}\n    )\n</pre> # using the context manager for secure and clean database connection handling. # Also take care to use your very own wrds_username that was assigned to you oce you applied for a WRDS account # and call the below with wrds_connection(wrds_username='your_individually_assigned_username')  with wrds_connection(wrds_username=\"YOUR_WRDS_USERNAME\") as db:     splist_data = db.raw_sql(         splist_query, params={\"start_date\": START_DATE, \"end_date\": END_DATE}     ) <pre>WRDS recommends setting up a .pgpass file.\nYou can create this file yourself at any time with the create_pgpass_file() function.\nLoading library list...\nDone\n</pre> In\u00a0[17]: Copied! <pre># filtering the data for the specific date for which we need the S&amp;P500 components\nsplist_data = (\n    splist_data[\n        splist_data[\"ending\"] == datetime.datetime.strptime(END_DATE, \"%Y-%m-%d\").date()\n    ]\n    .sort_values(by=\"start\")\n    .reset_index(drop=True)\n)\n\nsplist_data\n</pre> # filtering the data for the specific date for which we need the S&amp;P500 components splist_data = (     splist_data[         splist_data[\"ending\"] == datetime.datetime.strptime(END_DATE, \"%Y-%m-%d\").date()     ]     .sort_values(by=\"start\")     .reset_index(drop=True) )  splist_data Out[17]: permno start ending 0 14541 1925-12-31 2023-12-29 1 15069 1925-12-31 2023-12-29 2 10145 1925-12-31 2023-12-29 3 21573 1925-12-31 2023-12-29 4 11404 1925-12-31 2023-12-29 ... ... ... ... 498 32942 2023-10-18 2023-12-29 499 92203 2023-10-18 2023-12-29 500 90720 2023-12-18 2023-12-29 501 18576 2023-12-18 2023-12-29 502 79094 2023-12-18 2023-12-29 <p>503 rows \u00d7 3 columns</p>"},{"location":"1_obtaining_financial_data/#obtaining-financial-data","title":"Obtaining financial data\u00b6","text":"<p>This notebook explains how to obtain high-quality price data at daily frequency from US listed stocks.</p>"},{"location":"1_obtaining_financial_data/#getting-data-wrds-crsp-and-tickers-for-the-sp-500-universe","title":"Getting data: WRDS, CRSP and tickers for the S&amp;P 500 universe.\u00b6","text":"<p>For this project, we make use of <code>CRSP</code> data.</p> <p>The acronym <code>CRSP</code> stands for <code>Center for Research in Security Prices</code> and you can find more information about them here: https://www.crsp.org/ They are a respected data provider that are behind a paywall. Unfortunately, high-quality financial data is known to be unaffordably expensive, but luckily Imperial College has a license. Precisely, you can access <code>CRSP</code> data via <code>WRDS</code>, being short for <code>Wharton Research Data Services</code>. WRDS understand themselves to be \"the global standard for business research\". We will not question the details of this statement, but they certainly do offer data that you also can make use for you university projects and dissertation all the way up to PhD and Postdoc research. You can find more information about them here: https://wrds-www.wharton.upenn.edu.</p> <p>In order to access <code>CRSP</code> data via <code>WRDS</code>, you need to follow these steps:</p> <ol> <li><p>Apply for a <code>WRDS</code> account via Imperial College's library services here: https://www.imperial.ac.uk/admin-services/library/subject-support/business/wrds/</p> </li> <li><p>Log on your <code>WRDS</code> account: https://wrds-www.wharton.upenn.edu/login/</p> </li> <li><p>Upon logging in, you will see a menu that looks like the following. Click on <code>CRSP</code>.</p> </li> </ol>"},{"location":"1_obtaining_financial_data/#querying-wrds-data-directly-from-within-python","title":"Querying <code>WRDS</code> data directly from within <code>Python</code>\u00b6","text":"<p>Alternatively, you can also query data directly from within Python using <code>WRDS's</code> PyPi indexed Python package <code>wrds</code> that you can install using <code>pip install wrds</code>. The sample script below shows how you can set up a connection and query data from <code>MSFT</code> and <code>AAPL</code> using <code>SQL</code> commands.</p> <pre><code>!pip install wrds  # https://pypi.org/project/wrds/\nimport wrds\ndb = wrds.Connection(wrds_username='your_individually_assigned_username')  # input your credential\n\nprint(db.list_libraries())  # prints accessible data sets\n\nsql_query =\\\n\"\"\"\nSELECT permno, date, prc, vol, askhi, bidlo\nFROM crsp.dsf\nWHERE permno IN (10107, 14593) -- Example PERMNOs for Microsoft and Apple\nAND date &gt;= '2020-01-01' AND date &lt;= '2020-12-31';\n\"\"\"\ndata = db.raw_sql(sql_query)\n</code></pre>"},{"location":"1_obtaining_financial_data/#appendix","title":"Appendix\u00b6","text":"<p>As discussed, relying solely on the ticker list for future reference has its limitations. S&amp;P 500 membership can change over time, companies can undergo name changes (e.g., Facebook to Meta), and ticker symbols can also change.</p> <p>A more robust approach involves using the <code>permanent company number (permno)</code>, which uniquely identifies a listed company. This method allows us to programmatically track the S&amp;P 500 constituents over time. To illustrate, we will retrieve the index members as of the most recent date for which our academic license provides data, which is 29 December 2023 at the time of writing. The following section will detail how to programmatically obtain the members for that day.</p> <p>We employ SQL commands to access WRDS's database, which documents the S&amp;P 500 index's membership changes in time. This approach produces a DataFrame detailing the index's constituents from its inception. By concentrating on the most recent data available, we opt for using permnos over tickers for retrieving price information. This strategy increases the accuracy of identifying index members, offering a more reliable alternative to open-source data. Our preference for commercial data sources like WRDS over open-source options is driven not by a dismissal of the latter, but by the higher quality control standards and accountability typically associated with commercial datasets.</p> <p>As an aside, this dataset opens up several avenues for analysis regarding the index's evolution over time that you can investigate in your own time. Consider, for example, ranking the members by their tenure in the index to explore their trajectories through various economic boom and bust cycles. It could also be enlightening to identify which periods were the most dynamic in terms of membership changes. Additionally, examining the newest members of the index may offer insights into shifting trends and sectors within the broader economy.</p>"},{"location":"1_obtaining_financial_data/#end","title":"END\u00b6","text":""},{"location":"2_single_stock_case/","title":"Candlestick patterns for one stock","text":"In\u00a0[1]: Copied! <pre>import multiprocessing\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nimport talib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.feature_selection import RFECV\nfrom IPython.display import Image\nimport inspect\nfrom pathlib import Path\n\nfrom BSquant import load_data\nfrom BSquant import process_data\nfrom BSquant import cs_pattern_recognition\nfrom BSquant import cs_performance\nfrom BSquant import plot_cs_performance\nfrom BSquant import compute_trading_strategy_performance\n\npd.set_option(\"display.max_columns\", None)\n%load_ext autoreload\n%autoreload 2\n</pre> import multiprocessing import matplotlib.pyplot as plt import numpy as np import pandas as pd import plotly.graph_objects as go import talib from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression, LogisticRegressionCV from sklearn.feature_selection import RFECV from IPython.display import Image import inspect from pathlib import Path  from BSquant import load_data from BSquant import process_data from BSquant import cs_pattern_recognition from BSquant import cs_performance from BSquant import plot_cs_performance from BSquant import compute_trading_strategy_performance  pd.set_option(\"display.max_columns\", None) %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre>Image(filename=\"./../figures/candlestick_anatomy.png\")\n</pre> Image(filename=\"./../figures/candlestick_anatomy.png\") Out[2]: <p>[Source: https://tradingcryptocourse.com/free/technical-analysis-candlesticks/]</p> In\u00a0[3]: Copied! <pre>Image(filename=\"./../figures/cs_patterns.png\")\n</pre> Image(filename=\"./../figures/cs_patterns.png\") Out[3]: <p>[Source: https://www.prorealtime.com/en/help-manual/candlesticks-patterns]</p> <p>Let us now load in data for a single stock and run through a complete analysis before we embark on analysing the whole S&amp;P 500 universe. The remainder of this notebook is aimed at introducing you to how we can translate our ideas to code making use of a computer to analyse a vasta amount of data that would be infeasible cumbersome to do by hand, or even in a spreadsheet. We will make use of CRSP data that we downloaded for the  S&amp;P 500 via WRDS (see detailed instruction in the first notebook).</p> In\u00a0[4]: Copied! <pre>help(load_data)  # displays the doc string of a function\n</pre> help(load_data)  # displays the doc string of a function <pre>Help on function load_data in module BSquant:\n\nload_data(file_path: pathlib.Path, compression: Optional[str] = 'gzip', ticker: Optional[str] = None, selected_start_date: Optional[pandas._libs.tslibs.timestamps.Timestamp] = None, selected_end_date: Optional[pandas._libs.tslibs.timestamps.Timestamp] = None) -&gt; pandas.core.frame.DataFrame\n    Loads data from a compressed CSV file, filters it based on optional criteria,\n    and returns a cleaned pandas DataFrame.\n    \n    The function reads a CSV file, optionally compressed, renames columns for consistency,\n    filters the data by a specified ticker and date range, and returns the processed DataFrame.\n    \n    Args:\n        file_path (Path): Path to the compressed CSV file.\n        compression (Optional[str], optional): Compression type of the CSV file. Defaults to 'gzip'.\n        ticker (Optional[str], optional): Specific ticker to filter on. If None, no ticker filtering is applied.\n        selected_start_date (Optional[pd.Timestamp], optional): The start date for filtering the data.\n            If None, no start date filtering is applied.\n        selected_end_date (Optional[pd.Timestamp], optional): The end date for filtering the data.\n            If None, no end date filtering is applied.\n    \n    Returns:\n        pd.DataFrame: A DataFrame containing the processed and optionally filtered data.\n    \n    Examples:\n        &gt;&gt;&gt; load_data(Path('/path/to/data.csv'), ticker='AAPL', selected_start_date=pd.Timestamp('2020-01-01'),\n        ... selected_end_date=pd.Timestamp('2020-12-31'))\n        # This will return a DataFrame with data for AAPL between Jan 1, 2020, and Dec 31, 2020.\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\n    inspect.getsource(load_data)\n)  # displays the source code of a class/function. Assumes `load_data` is already defined or imported\n</pre> print(     inspect.getsource(load_data) )  # displays the source code of a class/function. Assumes `load_data` is already defined or imported <p>Note: An interactive alternative to statically printing the source code in Jupyter, is to invoke the function name followed by two question marks in a code cell.</p> In\u00a0[6]: Copied! <pre>load_data??\n</pre> load_data?? <p>You see, <code>load_data</code> demands the path to the location where the data is stored. Optionally, you can input the compression mechanim of the file, pass a ticker, as well as define a start and and end date, if you wish to restrict the analysis to a particular stock within a specific time frame. In the present case, we want to obtain data for <code>Microsoft</code> only. Its ticker is <code>MSFT</code>.</p> <p>Looking at the function definition, you see we rename the column names. Strictly speaking this is not a necessity, however, a column called <code>date</code> is probably more intuitive to understand than <code>DlyCalDt</code>. Also, by convention, column names are usually considered to be named in lower-case letters. You then see that we choose the columns we wish to import. An important step is to convert the dates to a proper datetime format. This is a crucial step as without a conversion, they would be considered strings and as such lose important properties for example, setting them as an index, or filtering between two dates. For clarity, we pass the formate in which CRPS dates are stored. In out case the CRSP format for dates in daily freqeuncy is <code>format=\"%Y-%m-%d</code>. For an exhaustive overview on that abbreviations are available, please consult : https://docs.python.org/3/library/datetime.html</p> <p>We then carry out the optional filtering steps by ticker and timeinterval, and return the data frame.</p> <p>Let us now define the path, as well as passing the necessary input parameters to <code>load_data</code> in the cells below to see the function being applied in practice.</p> In\u00a0[7]: Copied! <pre># define the filename of the stock market data (dependent on how you named the file on your system).\n# set up the path to the data (independently on your system thanks to the pathlib library).\ndata_filename = \"SP500_daily_data_1980_to_2023.csv.gz\"\n\nnotebooks_dir = Path(\"./../notebooks\")\ndata_file_path = notebooks_dir.parent / \"data\" / data_filename\nprint(data_file_path)\n</pre> # define the filename of the stock market data (dependent on how you named the file on your system). # set up the path to the data (independently on your system thanks to the pathlib library). data_filename = \"SP500_daily_data_1980_to_2023.csv.gz\"  notebooks_dir = Path(\"./../notebooks\") data_file_path = notebooks_dir.parent / \"data\" / data_filename print(data_file_path) <pre>../data/SP500_daily_data_1980_to_2023.csv.gz\n</pre> In\u00a0[8]: Copied! <pre>df = load_data(\n    file_path=data_file_path,\n    compression=\"gzip\",\n    ticker=\"MSFT\",\n    selected_start_date=pd.Timestamp(1992, 1, 1),\n    selected_end_date=pd.Timestamp(2022, 12, 31),\n)\n</pre> df = load_data(     file_path=data_file_path,     compression=\"gzip\",     ticker=\"MSFT\",     selected_start_date=pd.Timestamp(1992, 1, 1),     selected_end_date=pd.Timestamp(2022, 12, 31), ) <p>The function <code>process_data</code> (displayed in the next cell) is tasked with processing data retrieved by <code>load_data</code>, with one of its key operations being the removal of rows containing <code>NaN</code> values. This method is notably aggressive and results in the loss of approximately 20% of the data across the entire dataset. To potentially reduce this loss, there are a few considerations:</p> <ul> <li><p>Data provider inquiry: Inquiring with the data provider about the missing data may provide clues into the nature of these gaps. In scenarios like exchange outages or other reasons for missing records, accurately reconstructing the exact historical data is highly unlikely, if not impossible.</p> </li> <li><p>Data interpolation: Another strategy is to interpolate missing values. However, this comes with caveats. Any form of interpolation, will alter the final analysis outcomes. The difficulties of error propagation through interpolated data is considerable, making it a difficult process to assess how these changes impact overall results.</p> </li> </ul> <p>Also, in <code>process_data</code>, we employ <code>method chaining</code> with <code>Pandas</code>, which offers several advantages for managing and transforming dataframes. Method chaining allows for operations to be applied sequentially without modifying the original dataset, preserving the raw data unless explicitly overwritten. This approach is made possible by passing the dataframe from one operation to the next, with each method call returning a new dataframe upon which subsequent operations can act. One of the benefits of method chaining is enhanced readability and maintainability of the code. It presents the data processing workflow in a clear, step-by-step manner, making it easier to understand and modify the processing sequence at later stages. It is important to note that method chaining relies on each operation returning a modified copy of the dataframe. Therefore, operations that return <code>None</code> disrupt the chain, leading to undesired behavior.</p> <p>We proceed to calculate <code>intraday returns</code> as well as their sign. An intraday return is defined as follows:</p> <p>$$ intraday\\_return_t = \\frac{close_t - open_t}{open_t} = \\frac{close_t}{open_t} - 1 $$</p> <p>It is important to note that intraday returns are defined differently from <code>daily returns</code>. While intraday returns are calculated by comparing the closing and opening prices within the same trading day, daily returns typically consider only the closing prices across consecutive days:</p> <p>$$ daily\\_return_t = \\frac{close_t - close_{t-1}}{close_{t-1}} = \\frac{close_t}{close_{t-1}} - 1 $$</p> <p>The <code>shift</code> operator is utilised to align intraday returns at times $t$ and $t+1$. This alignment allows us to transparently evaluate the accuracy of predictions made by our candlestick patterns against the actual intraday returns observed in the market on the following day. By examining whether the signed returns at $t+1$ conform to the predictions, we can assess the effectiveness of our model.</p> <p>Please note that the DataFrame retains certain information which, while not immediately necessary for this project, is beneficial for future analyses. This includes the precise numeric values of intraday returns, the volume of stocks traded, and the price volume for the day. Such details are important for conducting regression-type econometric studies on return series and for calculating the <code>Volume Weighted Average Price (VWAP)</code>. VWAP, defined as the scalar product of transactional price and stock volume divided by the L-2 norm of the volume vector. It is widely used to assess the performance of execution algorithms and proprietary trading strategies, making it an important metric for future investigations.</p> <p>Additionally, although the computation of log-returns based on daily closing prices is currently commented out, it has been preserved to facilitate a direct comparison with intraday returns in potential future studies.</p> <p>Let us now display the source code as make the function act on the loaded data.</p> In\u00a0[9]: Copied! <pre>print(inspect.getsource(process_data))  # displays the source code of `process_data`\n</pre> print(inspect.getsource(process_data))  # displays the source code of `process_data` <pre>def process_data(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Processes the given DataFrame by cleaning data, calculating intraday returns,\n    and preparing for subsequent analysis.\n\n    This function performs several steps:\n    - Drops any rows with missing values.\n    - Converts the volume column to integers.\n    - Calculates the intraday return as (close - open) / open.\n    - Determines the sign of the intraday return (positive, negative, or zero) as an integer.\n    - Shifts the intraday returns and their signs to prepare for next-day return analysis.\n\n    Parameters:\n        df (pd.DataFrame): The DataFrame to process. Expects columns 'vol', 'open',\n                           and 'close' to be present.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the original data plus additional columns for\n                      'intraday_return', 'sign_intraday_return', 'next_intraday_return',\n                      and 'sign_next_day_return'. Rows with missing values after processing\n                      are dropped to ensure completeness of the dataset.\n\n    Note:\n        The 'next_intraday_return' and 'sign_next_day_return' columns are intended for\n        use in predictive models where the goal is to forecast the next day's market movement.\n    \"\"\"\n    df = (\n        df.dropna()\n        .assign(\n            vol=lambda x: x[\"vol\"].astype(int),\n            intraday_return=lambda x: (x[\"close\"] - x[\"open\"]) / x[\"open\"],\n            sign_intraday_return=lambda x: np.sign(x[\"intraday_return\"]).astype(int),\n        )\n        .assign(\n            # log_return=lambda x: np.log(x[\"close\"] / x[\"close\"].shift(1)),\n            next_intraday_return=lambda x: x[\"intraday_return\"].shift(-1),\n            sign_next_day_return=lambda x: x[\"sign_intraday_return\"]\n            .shift(-1)\n            .astype(\"Int64\"),\n        )\n        .dropna()\n        .reset_index(drop=True)\n    )\n    return df\n\n</pre> In\u00a0[10]: Copied! <pre>df = process_data(df)  # let us now obtain the processed data\ndf.head(5)\n</pre> df = process_data(df)  # let us now obtain the processed data df.head(5) Out[10]: ticker date prc vol close low high open price_vol intraday_return sign_intraday_return next_intraday_return sign_next_day_return 0 MSFT 1992-06-15 75.750 1707931 75.750 75.00 78.00 75.0 129375773.3 0.010000 1 -0.036424 -1 1 MSFT 1992-06-16 72.750 1800280 72.750 72.50 76.50 75.5 130970370.0 -0.036424 -1 -0.001724 -1 2 MSFT 1992-06-17 72.375 2196679 72.375 71.50 73.25 72.5 158984642.6 -0.001724 -1 -0.010274 -1 3 MSFT 1992-06-18 72.250 1706769 72.250 71.00 74.75 73.0 123314060.3 -0.010274 -1 0.000000 0 4 MSFT 1992-06-19 73.500 1350828 73.500 72.75 74.25 73.5 99285858.0 0.000000 0 0.013652 1 In\u00a0[11]: Copied! <pre>def ohlc_plot_candles_mpl(data: np.array, window: int) -&gt; plt.Figure:\n    \"\"\"\n    Plotting OHLC data using an np.array (modified after https://github.com/sofienkaabar/mastering-financial-pattern-recognition/blob/main/Master_Function_MFPR.py)\n    \"\"\"\n    plt.figure(figsize=(10, 10))\n    sample = data[-window:,]\n\n    for i in range(len(sample)):\n        plt.vlines(\n            x=i, ymin=sample[i, 2], ymax=sample[i, 1], color=\"black\", linewidth=1\n        )\n        if sample[i, 3] &gt; sample[i, 0]:\n            plt.vlines(\n                x=i, ymin=sample[i, 0], ymax=sample[i, 3], color=\"green\", linewidth=1\n            )\n        if sample[i, 3] &lt; sample[i, 0]:\n            plt.vlines(\n                x=i, ymin=sample[i, 3], ymax=sample[i, 0], color=\"red\", linewidth=1\n            )\n        if sample[i, 3] == sample[i, 0]:\n            plt.vlines(\n                x=i,\n                ymin=sample[i, 3],\n                ymax=sample[i, 0] + 0.00005,\n                color=\"black\",\n                linewidth=1.00,\n            )\n    plt.title(\"Price of seleced asset diplayed as candlestick chart\")\n    plt.xlabel(\"data points [daily frequency]\")\n    plt.ylabel(\"price [currency the stock is priced in]\")\n    plt.grid()\n</pre> def ohlc_plot_candles_mpl(data: np.array, window: int) -&gt; plt.Figure:     \"\"\"     Plotting OHLC data using an np.array (modified after https://github.com/sofienkaabar/mastering-financial-pattern-recognition/blob/main/Master_Function_MFPR.py)     \"\"\"     plt.figure(figsize=(10, 10))     sample = data[-window:,]      for i in range(len(sample)):         plt.vlines(             x=i, ymin=sample[i, 2], ymax=sample[i, 1], color=\"black\", linewidth=1         )         if sample[i, 3] &gt; sample[i, 0]:             plt.vlines(                 x=i, ymin=sample[i, 0], ymax=sample[i, 3], color=\"green\", linewidth=1             )         if sample[i, 3] &lt; sample[i, 0]:             plt.vlines(                 x=i, ymin=sample[i, 3], ymax=sample[i, 0], color=\"red\", linewidth=1             )         if sample[i, 3] == sample[i, 0]:             plt.vlines(                 x=i,                 ymin=sample[i, 3],                 ymax=sample[i, 0] + 0.00005,                 color=\"black\",                 linewidth=1.00,             )     plt.title(\"Price of seleced asset diplayed as candlestick chart\")     plt.xlabel(\"data points [daily frequency]\")     plt.ylabel(\"price [currency the stock is priced in]\")     plt.grid() In\u00a0[12]: Copied! <pre>ohlc_plot_candles_mpl(data=df[[\"open\", \"high\", \"low\", \"close\"]].values, window=100)\n</pre> ohlc_plot_candles_mpl(data=df[[\"open\", \"high\", \"low\", \"close\"]].values, window=100) In\u00a0[13]: Copied! <pre>def ohlc_plot_candles_plotly(df: pd.DataFrame) -&gt; go.Figure:\n    fig = go.Figure(\n        data=[\n            go.Candlestick(\n                x=df.index,\n                open=df[\"open\"],\n                high=df[\"high\"],\n                low=df[\"low\"],\n                close=df[\"close\"],\n            )\n        ]\n    )\n\n    fig.update_layout(\n        font={\"size\": 18},\n        legend={\n            \"yanchor\": \"top\",\n            \"y\": 0.99,\n            \"xanchor\": \"right\",\n            \"x\": 0.99,\n            \"font\": {\"size\": 20},\n        },\n        title=\"Price of seleced asset diplayed as candlestick chart\",\n        xaxis_title=\"date [daily frequency]\",\n        yaxis_title=\"price [currency the stock is priced in]\",\n        autosize=False,\n        width=800,\n        height=800,\n    )\n\n    fig.show()\n</pre> def ohlc_plot_candles_plotly(df: pd.DataFrame) -&gt; go.Figure:     fig = go.Figure(         data=[             go.Candlestick(                 x=df.index,                 open=df[\"open\"],                 high=df[\"high\"],                 low=df[\"low\"],                 close=df[\"close\"],             )         ]     )      fig.update_layout(         font={\"size\": 18},         legend={             \"yanchor\": \"top\",             \"y\": 0.99,             \"xanchor\": \"right\",             \"x\": 0.99,             \"font\": {\"size\": 20},         },         title=\"Price of seleced asset diplayed as candlestick chart\",         xaxis_title=\"date [daily frequency]\",         yaxis_title=\"price [currency the stock is priced in]\",         autosize=False,         width=800,         height=800,     )      fig.show() In\u00a0[14]: Copied! <pre>ohlc_plot_candles_plotly(\n    df=df[[\"date\", \"open\", \"high\", \"low\", \"close\"]].set_index(\"date\")[-100:]\n)\n</pre> ohlc_plot_candles_plotly(     df=df[[\"date\", \"open\", \"high\", \"low\", \"close\"]].set_index(\"date\")[-100:] ) In\u00a0[15]: Copied! <pre>candle_names = talib.get_function_groups()[\"Pattern Recognition\"]\ncandle_names[:5]\n</pre> candle_names = talib.get_function_groups()[\"Pattern Recognition\"] candle_names[:5] Out[15]: <pre>['CDL2CROWS', 'CDL3BLACKCROWS', 'CDL3INSIDE', 'CDL3LINESTRIKE', 'CDL3OUTSIDE']</pre> In\u00a0[16]: Copied! <pre>len(candle_names)\n</pre> len(candle_names) Out[16]: <pre>61</pre> In\u00a0[17]: Copied! <pre># the result of the pattern recognition will be a multi-indexed DataFrame\ncs_signals_df = cs_pattern_recognition(df=df)\ncs_signals_df\n</pre> # the result of the pattern recognition will be a multi-indexed DataFrame cs_signals_df = cs_pattern_recognition(df=df) cs_signals_df Out[17]: ticker prc vol close low high open price_vol intraday_return sign_intraday_return next_intraday_return sign_next_day_return cs_pattern candle date CDL2CROWS 1993-06-16 MSFT 91.25 2097618 91.25 89.250 93.2500 92.750 1.914076e+08 -0.016173 -1 -0.006831 -1 -100 2003-07-15 MSFT 27.27 54656026 27.27 27.100 27.5300 27.470 1.490470e+09 -0.007281 -1 -0.001451 -1 -100 2012-01-18 MSFT 28.23 65022787 28.23 27.970 28.4000 28.310 1.835593e+09 -0.002826 -1 -0.001420 -1 -100 2016-09-19 MSFT 56.93 20961382 56.93 56.850 57.7500 57.270 1.193331e+09 -0.005937 -1 -0.009416 -1 -100 2017-10-03 MSFT 74.26 12190403 74.26 74.195 74.8800 74.670 9.052593e+08 -0.005491 -1 0.009324 1 -100 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... CDLXSIDEGAP3METHODS 2020-12-23 MSFT 221.02 18662773 221.02 220.800 223.5589 223.110 4.124846e+09 -0.009368 -1 0.006007 1 100 2021-03-31 MSFT 235.77 43529147 235.77 232.390 239.1000 232.910 1.026287e+10 0.012279 1 0.016270 1 -100 2021-04-14 MSFT 255.59 23028434 255.59 255.160 258.8300 257.475 5.885837e+09 -0.007321 -1 0.006083 1 100 2022-05-13 MSFT 261.12 34807946 261.12 255.350 263.0400 257.350 9.089051e+09 0.014649 1 0.005943 1 -100 2022-11-04 MSFT 221.39 36712344 221.39 213.431 221.5900 217.550 8.127746e+09 0.017651 1 0.026488 1 -100 <p>14935 rows \u00d7 13 columns</p> In\u00a0[18]: Copied! <pre># we can query a specific candlestick pattern from a multiindex data frame like so. The multi-index DataFrame\n# will then be a simpel DataFrame. You can imagine multi-index DataFrames to be a multidimensional data structure\n# projected onto 2D space.\ncs_signals_df.loc[\"CDL2CROWS\"]\n</pre> # we can query a specific candlestick pattern from a multiindex data frame like so. The multi-index DataFrame # will then be a simpel DataFrame. You can imagine multi-index DataFrames to be a multidimensional data structure # projected onto 2D space. cs_signals_df.loc[\"CDL2CROWS\"] Out[18]: ticker prc vol close low high open price_vol intraday_return sign_intraday_return next_intraday_return sign_next_day_return cs_pattern date 1993-06-16 MSFT 91.25 2097618 91.25 89.250 93.25 92.750 1.914076e+08 -0.016173 -1 -0.006831 -1 -100 2003-07-15 MSFT 27.27 54656026 27.27 27.100 27.53 27.470 1.490470e+09 -0.007281 -1 -0.001451 -1 -100 2012-01-18 MSFT 28.23 65022787 28.23 27.970 28.40 28.310 1.835593e+09 -0.002826 -1 -0.001420 -1 -100 2016-09-19 MSFT 56.93 20961382 56.93 56.850 57.75 57.270 1.193331e+09 -0.005937 -1 -0.009416 -1 -100 2017-10-03 MSFT 74.26 12190403 74.26 74.195 74.88 74.670 9.052593e+08 -0.005491 -1 0.009324 1 -100 2019-12-30 MSFT 157.59 16329244 157.59 156.730 159.02 158.987 2.573326e+09 -0.008787 -1 0.005932 1 -100 2022-04-21 MSFT 280.81 29413448 280.81 280.060 293.30 288.580 8.259590e+09 -0.026925 -1 -0.027158 -1 -100 <p>Given the varying frequencies of candlestick pattern occurrences, we further refine our analysis by applying the <code>Wilson score</code> to adjust the precision metric according to the frequency of each pattern's occurrence, along with calculating their 95% confidence interval. This adjustment is relevant because it accounts for the variability in pattern frequencies, providing a more reliable measure of precision.</p> <p>For a candlestick signal to be considered statistically significant, its lower confidence boundary must exceed the 50% threshold. The further this lower bound is above 50%, the higher our confidence in the potential profitability of the signal. This method ensures that we prioritize signals not just on their precision, but also on the statistical confidence in their predictive power.</p> <p>More precisely, the Wilson score interval for a proportion is given by:</p> <p>$$ \\text{Center} = \\frac{\\hat{p} + \\frac{z^2}{2n}}{1 + \\frac{z^2}{n}} $$</p> <p>$$ \\text{Margin} = \\frac{z \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n} + \\frac{z^2}{4n^2}}}{1 + \\frac{z^2}{n}} $$</p> <p>$$ \\text{CI Lower Bound} = \\text{Center} - \\text{Margin} $$</p> <p>$$ \\text{CI Upper Bound} = \\text{Center} + \\text{Margin} $$</p> <p>where:</p> <ul> <li>( $\\hat{p}$ ) is the observed proportion (e.g., precision)</li> <li>( z ) is the z-score corresponding to the desired confidence level (1.96 for 95% confidence)</li> <li>( n ) is the total number of instances (e.g., TP + FP)</li> </ul> <p>The Wilson score interval is useful when the sample size is small or when the proportion is near 0 or 1. Unlike the normal approximation interval, the Wilson interval does not produce probabilities outside the [0, 1] range and adjusts the observed proportion by a factor related to the confidence level and sample size, providing a more accurate confidence interval for proportions.</p> <p>See also:</p> <ul> <li>https://www.statisticshowto.com/wilson-ci/</li> </ul> In\u00a0[19]: Copied! <pre>performance_metrics = cs_performance(cs_signals_df)\nperformance_metrics\n</pre> performance_metrics = cs_performance(cs_signals_df) performance_metrics Out[19]: TP FP total_instances precision center margin ci_upper ci_lower TP_wilson candle CDLLONGLINE 739 878 1617 0.457019 0.457121 0.024252 0.481373 0.432869 0.457121 CDLSPINNINGTOP 756 816 1572 0.480916 0.480963 0.024669 0.505631 0.456294 0.480963 CDLBELTHOLD 573 621 1194 0.479899 0.479964 0.028292 0.508256 0.451672 0.479964 CDLCLOSINGMARUBOZU 520 649 1169 0.444825 0.445005 0.028441 0.473446 0.416564 0.445005 CDLDOJI 487 533 1020 0.477451 0.477536 0.030596 0.508131 0.446940 0.477536 CDLLONGLEGGEDDOJI 487 532 1019 0.477920 0.478002 0.030612 0.508614 0.447390 0.478002 CDLHIGHWAVE 456 509 965 0.472539 0.472648 0.031437 0.504085 0.441211 0.472648 CDLSHORTLINE 425 474 899 0.472747 0.472863 0.032566 0.505430 0.440297 0.472863 CDLHIKKAKE 396 422 818 0.484108 0.484182 0.034167 0.518349 0.450015 0.484182 CDLRICKSHAWMAN 352 391 743 0.473755 0.473890 0.035810 0.509700 0.438080 0.473890 CDLENGULFING 277 308 585 0.473504 0.473677 0.040329 0.514006 0.433349 0.473677 CDLHARAMI 289 292 581 0.497418 0.497435 0.040522 0.537957 0.456913 0.497435 CDLMARUBOZU 164 215 379 0.432718 0.433393 0.049634 0.483027 0.383759 0.433393 CDL3OUTSIDE 133 137 270 0.492593 0.492697 0.059214 0.551910 0.433483 0.492697 CDLDOJISTAR 94 81 175 0.537143 0.536345 0.073082 0.609427 0.463263 0.536345 CDLHARAMICROSS 89 83 172 0.517442 0.517061 0.073858 0.590919 0.443203 0.517061 CDLHANGINGMAN 84 87 171 0.491228 0.491421 0.074102 0.565523 0.417319 0.491421 CDLHAMMER 63 90 153 0.411765 0.413926 0.077053 0.490979 0.336873 0.413926 CDL3INSIDE 76 64 140 0.542857 0.541713 0.081417 0.623130 0.460295 0.541713 CDLMATCHINGLOW 74 61 135 0.548148 0.546816 0.082793 0.629609 0.464023 0.546816 CDLDRAGONFLYDOJI 63 58 121 0.520661 0.520025 0.087635 0.607661 0.432390 0.520025 CDLTAKURI 62 56 118 0.525424 0.524622 0.088670 0.613292 0.435952 0.524622 CDLINVERTEDHAMMER 49 60 109 0.449541 0.451259 0.091799 0.543058 0.359460 0.451259 CDLGRAVESTONEDOJI 51 56 107 0.476636 0.477445 0.092984 0.570429 0.384461 0.477445 CDLHOMINGPIGEON 43 50 93 0.462366 0.463858 0.099312 0.563171 0.364546 0.463858 CDLSHOOTINGSTAR 32 46 78 0.410256 0.414469 0.106650 0.521119 0.307819 0.414469 CDLTHRUSTING 28 30 58 0.482759 0.483830 0.124548 0.608377 0.359282 0.483830 CDLXSIDEGAP3METHODS 25 23 48 0.520833 0.519290 0.135997 0.655287 0.383292 0.519290 CDLTASUKIGAP 22 19 41 0.536585 0.533451 0.145986 0.679438 0.387465 0.533451 CDLEVENINGSTAR 18 22 40 0.450000 0.454381 0.147328 0.601709 0.307053 0.454381 CDLPIERCING 15 24 39 0.384615 0.394962 0.146048 0.541009 0.248914 0.394962 CDLADVANCEBLOCK 23 15 38 0.605263 0.595599 0.148420 0.744019 0.447179 0.595599 CDLMORNINGSTAR 20 16 36 0.555556 0.550199 0.154388 0.704587 0.395811 0.550199 CDLSEPARATINGLINES 15 20 35 0.428571 0.435636 0.155790 0.591426 0.279846 0.435636 CDLDARKCLOUDCOVER 11 16 27 0.407407 0.418940 0.173793 0.592733 0.245148 0.418940 CDLGAPSIDESIDEWHITE 12 9 21 0.571429 0.560383 0.194917 0.755300 0.365466 0.560383 CDLONNECK 10 8 18 0.555556 0.545784 0.208620 0.754405 0.337164 0.545784 CDLSTALLEDPATTERN 9 7 16 0.562500 0.550400 0.218614 0.769013 0.331786 0.550400 CDLSTICKSANDWICH 7 7 14 0.500000 0.500000 0.232008 0.732008 0.267992 0.500000 CDLEVENINGDOJISTAR 4 9 13 0.307692 0.351557 0.224750 0.576307 0.126807 0.351557 CDLUNIQUE3RIVER 7 5 12 0.583333 0.563125 0.243614 0.806740 0.319511 0.563125 CDLLADDERBOTTOM 6 6 12 0.500000 0.500000 0.246218 0.746218 0.253782 0.500000 CDLCOUNTERATTACK 5 3 8 0.625000 0.584449 0.278707 0.863156 0.305742 0.584449 CDLMORNINGDOJISTAR 5 3 8 0.625000 0.584449 0.278707 0.863156 0.305742 0.584449 CDL2CROWS 5 2 7 0.714286 0.638358 0.279423 0.917781 0.358934 0.638358 CDLINNECK 3 4 7 0.428571 0.453881 0.295661 0.749542 0.158220 0.453881 CDL3WHITESOLDIERS 4 2 6 0.666667 0.601611 0.301618 0.903229 0.299993 0.601611 CDLRISEFALL3METHODS 4 2 6 0.666667 0.601611 0.301618 0.903229 0.299993 0.601611 CDLTRISTAR 2 4 6 0.333333 0.398389 0.301618 0.700007 0.096771 0.398389 CDL3LINESTRIKE 1 3 4 0.250000 0.372473 0.326885 0.699358 0.045587 0.372473 CDLHIKKAKEMOD 1 3 4 0.250000 0.372473 0.326885 0.699358 0.045587 0.372473 CDL3BLACKCROWS 1 1 2 0.500000 0.500000 0.405469 0.905469 0.094531 0.500000 CDLBREAKAWAY 0 2 2 0.000000 0.328810 0.328810 0.657620 0.000000 0.328810 CDLUPSIDEGAP2CROWS 0 2 2 0.000000 0.328810 0.328810 0.657620 0.000000 0.328810 CDLMATHOLD 1 0 1 1.000000 0.603275 0.396725 1.000000 0.206549 0.603275 CDLIDENTICAL3CROWS 0 1 1 0.000000 0.396725 0.396725 0.793451 0.000000 0.396725 <p>We aim to delve into the performance metrics by examining candlestick signals from two perspectives: the total number of instances and those instances where the Wilson-corrected true positive score exceeds 50%. Moreover, for every candlestick pattern, we plan to display the Wilson-corrected true positive score along with their 95% uncertainty interval in a bar chart. This chart will also demarcate trading zones in red, yellow, and green, as follows:</p> <ul> <li><p>Red Zone: Signals with a true positive score significantly below the 50% threshold are deemed to lack informational content. In fact, such signals are considered contraindicators, suggesting a course of action opposite to what they indicate.</p> </li> <li><p>Yellow Zone: True positive scores ranging between 50% and 55% fall into this category. These signals are potential candidates for further exploration, as they might possess some predictive value.</p> </li> <li><p>Green Zone: Signals with performance metrics exceeding 55% are regarded as informative, suggesting that they could provide a reliable basis for trading decisions.</p> </li> </ul> <p>This approach allows us to systematically assess the reliability of candlestick patterns in forecasting market movements. By categorizing signals into these zones based on their Wilson-corrected true positive scores, we can more effectively identify patterns that are likely to trigger trading action.</p> <p>Our examination reveals that certain candlestick patterns manifest frequently, occurring between 200 to 1600 times for MSFT from 15 June 1992 to the end of December 2023. However, many patterns are rare and almost never appear. Unfortunately, refining our selection to include only those signals with a precision greater than 50% dramatically reduces our pool from 61 candlestick patterns to just 20 viable candidates. Moreover, when we apply 95% confidence intervals to these candidates, we find that none offer genuinely informative signals based on our criteria. Intriguingly, our analysis identifies the <code>Long Line</code> pattern as a counter-signal, suggesting that its appearance may indicate an opportunity to take a position contrary to the one it would typically suggest.</p> In\u00a0[20]: Copied! <pre># plot all patterns, ranked by number of instances\nplot_cs_performance(\n    df=performance_metrics,\n    criterion=\"total_instances\",\n    title_suffix=\"across the whole data set.\",\n)\n\n# plot the patterns, ranked by number of instances, with a true-positive rate &gt;50%.\nplot_cs_performance(\n    df=performance_metrics.query(\"precision &gt; 0.5\").sort_values(\n        by=\"total_instances\", ascending=False\n    ),\n    criterion=\"total_instances\",\n    title_suffix=\"with precision &gt; 50%.\",\n)\n</pre> # plot all patterns, ranked by number of instances plot_cs_performance(     df=performance_metrics,     criterion=\"total_instances\",     title_suffix=\"across the whole data set.\", )  # plot the patterns, ranked by number of instances, with a true-positive rate &gt;50%. plot_cs_performance(     df=performance_metrics.query(\"precision &gt; 0.5\").sort_values(         by=\"total_instances\", ascending=False     ),     criterion=\"total_instances\",     title_suffix=\"with precision &gt; 50%.\", ) In\u00a0[21]: Copied! <pre>plot_cs_performance(\n    df=performance_metrics, criterion=\"TP_wilson\", plot_performance=True\n)\n</pre> plot_cs_performance(     df=performance_metrics, criterion=\"TP_wilson\", plot_performance=True ) In\u00a0[22]: Copied! <pre>performance_metrics.query(\"ci_lower &gt; 0.45 and TP_wilson &gt; 0.5\")\n</pre> performance_metrics.query(\"ci_lower &gt; 0.45 and TP_wilson &gt; 0.5\") Out[22]: TP FP total_instances precision center margin ci_upper ci_lower TP_wilson candle CDLDOJISTAR 94 81 175 0.537143 0.536345 0.073082 0.609427 0.463263 0.536345 CDL3INSIDE 76 64 140 0.542857 0.541713 0.081417 0.623130 0.460295 0.541713 CDLMATCHINGLOW 74 61 135 0.548148 0.546816 0.082793 0.629609 0.464023 0.546816 <p>Our investigation into candlestick patterns reveals a nuanced landscape. While certain patterns exhibit a true positive metric exceeding 50%, suggesting potential trading viability, the rigorous step of evaluating their lower confidence intervals introduces a cautionary note. Specifically, for the cases examined, all lower confidence intervals fall below the 50% threshold in the single stock scenario, tempering initial optimism.</p> <p>Among the patterns analyzed, <code>Dojistar</code>, <code>3 Inside</code>, and <code>Matchingglow</code> emerge as the most promising based on their performance metrics. However, it's premature to draw definitive conclusions. Given the inherently noisy nature of financial data, further exploration is warranted. To this end, we propose two avenues for deepening our analysis:</p> <ul> <li><p>Aggregation of signals: Investigating the potential benefits of aggregating signals from multiple patterns or across a broader dataset may reveal underlying trends or strengthen predictive signals that were not apparent in isolated analysis.</p> </li> <li><p>Application of machine learning lechniques: Leveraging machine learning can offer an avenue for pattern recognition and prediction, potentially uncovering non-linear patterns that are obscured to us and difficult to detect otherwise.</p> </li> </ul> In\u00a0[23]: Copied! <pre>cs_signals_df[\n    \"cs_pattern\"\n].value_counts()  # how many individual predictions do we have?\n</pre> cs_signals_df[     \"cs_pattern\" ].value_counts()  # how many individual predictions do we have? Out[23]: <pre>cs_pattern\n 100    9333\n-100    5433\n-200      89\n 200      80\nName: count, dtype: int64</pre> In\u00a0[24]: Copied! <pre># We pivot our multi-indexed data frame and sum across all patterns applying the \"principle of superposition\"\ntrading_signal = (\n    cs_signals_df[cs_signals_df[\"cs_pattern\"] != 0][[\"cs_pattern\"]]\n    .pivot_table(index=\"date\", columns=\"candle\", values=\"cs_pattern\", aggfunc=\"sum\")\n    .sum(axis=1)\n    .loc[lambda x: x != 0]\n)\ntrading_signal\n</pre> # We pivot our multi-indexed data frame and sum across all patterns applying the \"principle of superposition\" trading_signal = (     cs_signals_df[cs_signals_df[\"cs_pattern\"] != 0][[\"cs_pattern\"]]     .pivot_table(index=\"date\", columns=\"candle\", values=\"cs_pattern\", aggfunc=\"sum\")     .sum(axis=1)     .loc[lambda x: x != 0] ) trading_signal Out[24]: <pre>date\n1992-06-29    200.0\n1992-06-30   -200.0\n1992-07-01    100.0\n1992-07-02   -200.0\n1992-07-07   -100.0\n              ...  \n2022-12-20    100.0\n2022-12-21    100.0\n2022-12-27   -200.0\n2022-12-28    100.0\n2022-12-29    200.0\nLength: 5692, dtype: float64</pre> <p>On 1992-06-29 we receive a bullish signal that we can act upon at the earliest upon market opeing on the next available trading day, which was 1992-06-30. After having some nostalgic moments of seemingly better times \"back in the good old days\", we note to account the delay between signal and earliest possible action, by offsetting the signal by one. Otherwise we unwantedly introduce look-ahead bias which is often associated with deceivingly high returns, that almost surely result in a deadly blow applying the \"strategy\" in practice. For convenience, we plot the signal as well.</p> In\u00a0[25]: Copied! <pre>trading_signal.plot(\n    figsize=(10, 10),\n    title=\"Trading signal based on candlesticks\\n(signal &gt; 0 =&gt; long; signal &lt; 0 =&gt; short)\",\n)\nplt.ylabel(\"candlestick prediction\")\nplt.show()\n</pre> trading_signal.plot(     figsize=(10, 10),     title=\"Trading signal based on candlesticks\\n(signal &gt; 0 =&gt; long; signal &lt; 0 =&gt; short)\", ) plt.ylabel(\"candlestick prediction\") plt.show() In\u00a0[26]: Copied! <pre>performance_trading_signals = (\n    df[\n        df[\"date\"].isin([date + pd.DateOffset(days=1) for date in trading_signal.index])\n    ][[\"date\", \"intraday_return\"]]\n    .assign(account_curve=lambda x: (1 + x[\"intraday_return\"]).cumprod())\n    .assign(cumsumret=lambda x: x[\"intraday_return\"].cumsum())\n    .assign(time_between_signals=lambda x: x[\"date\"].diff().dt.days)\n)\n\nperformance_trading_signals.head(5)\n</pre> performance_trading_signals = (     df[         df[\"date\"].isin([date + pd.DateOffset(days=1) for date in trading_signal.index])     ][[\"date\", \"intraday_return\"]]     .assign(account_curve=lambda x: (1 + x[\"intraday_return\"]).cumprod())     .assign(cumsumret=lambda x: x[\"intraday_return\"].cumsum())     .assign(time_between_signals=lambda x: x[\"date\"].diff().dt.days) )  performance_trading_signals.head(5) Out[26]: date intraday_return account_curve cumsumret time_between_signals 11 1992-06-30 -0.037801 0.962199 -0.037801 NaN 12 1992-07-01 0.026690 0.987881 -0.011110 1.0 13 1992-07-02 -0.039519 0.948841 -0.050629 1.0 16 1992-07-08 0.009191 0.957562 -0.041438 6.0 18 1992-07-10 0.001805 0.959290 -0.039633 2.0 In\u00a0[27]: Copied! <pre># The reference strategy will be a buy-and hold approach.\n# For consistency, we employ intraday returns for the reference strategy as well which is chosen not entirely selflessly\n# and avoid us to adjust the prices for stock splits and dividends, as this is an entire topic on its own.\ndf_reference_strategy = df[[\"date\", \"intraday_return\"]].copy()\ndf_reference_strategy[\"account_curve\"] = (\n    1 + df_reference_strategy[\"intraday_return\"]\n).cumprod()\ndf_reference_strategy[\"cumsumret\"] = df_reference_strategy[\"intraday_return\"].cumsum()\n</pre> # The reference strategy will be a buy-and hold approach. # For consistency, we employ intraday returns for the reference strategy as well which is chosen not entirely selflessly # and avoid us to adjust the prices for stock splits and dividends, as this is an entire topic on its own. df_reference_strategy = df[[\"date\", \"intraday_return\"]].copy() df_reference_strategy[\"account_curve\"] = (     1 + df_reference_strategy[\"intraday_return\"] ).cumprod() df_reference_strategy[\"cumsumret\"] = df_reference_strategy[\"intraday_return\"].cumsum() <p>Finally, we are in a position to compare the the buy-and-hold reference trading strategy against the one obtained from the candlestick patterns. The function <code>compute_trading_strategy_performance</code> thereby assesses the key performance characteristics of the strategies, namely <code>annualised return</code>, <code>annualised volatility</code>, and <code>annualised Sharpe Ratio</code>.</p> In\u00a0[28]: Copied! <pre># Calculate the cumulative sum of intraday returns to plot the account curve\n\nperformance_trading_signals.hist(column=\"time_between_signals\", bins=20, figsize=(8, 8))\nplt.xlabel(\"Time between signals [days]\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of time differences between signals\")\nplt.show()\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.plot(\n    performance_trading_signals[\"date\"],\n    performance_trading_signals[\"cumsumret\"],\n    label=\"cumulative return using candlesticks\",\n    color=\"b\",\n)\nax.plot(\n    df_reference_strategy[\"date\"],\n    df_reference_strategy[\"cumsumret\"],\n    label=\"cumulative return assuming buy-and-hold\",\n    color=\"r\",\n)\nax.set_xlabel(\"date\")\nax.set_ylabel(\"cumulative return\")\nax.legend(loc=\"upper left\")\nax.set_title(\"long-short strategy using candlesticks vs. buy-and-hold\")\nplt.show()\n\n# Compute trading strategy statistics\nprint(\"Performance of the naive aggregated candlestick strategy\")\ncompute_trading_strategy_performance(df=performance_trading_signals, verbose=True)\nprint()\nprint(\"Performance of the reference strategy\")\ncompute_trading_strategy_performance(df=df_reference_strategy, verbose=True);\n</pre> # Calculate the cumulative sum of intraday returns to plot the account curve  performance_trading_signals.hist(column=\"time_between_signals\", bins=20, figsize=(8, 8)) plt.xlabel(\"Time between signals [days]\") plt.ylabel(\"Frequency\") plt.title(\"Histogram of time differences between signals\") plt.show()  fig, ax = plt.subplots(figsize=(8, 8)) ax.plot(     performance_trading_signals[\"date\"],     performance_trading_signals[\"cumsumret\"],     label=\"cumulative return using candlesticks\",     color=\"b\", ) ax.plot(     df_reference_strategy[\"date\"],     df_reference_strategy[\"cumsumret\"],     label=\"cumulative return assuming buy-and-hold\",     color=\"r\", ) ax.set_xlabel(\"date\") ax.set_ylabel(\"cumulative return\") ax.legend(loc=\"upper left\") ax.set_title(\"long-short strategy using candlesticks vs. buy-and-hold\") plt.show()  # Compute trading strategy statistics print(\"Performance of the naive aggregated candlestick strategy\") compute_trading_strategy_performance(df=performance_trading_signals, verbose=True) print() print(\"Performance of the reference strategy\") compute_trading_strategy_performance(df=df_reference_strategy, verbose=True); <pre>Performance of the naive aggregated candlestick strategy\nAnnualised strategy return [%]: 0.0745\nAnnualised strategy standard deviation of returns [%]: 0.2586\nSharpe ratio of strategy: 0.2879\n\nPerformance of the reference strategy\nAnnualised strategy return [%]: 0.1082\nAnnualised strategy standard deviation of returns [%]: 0.2625\nSharpe ratio of strategy: 0.4120\n</pre> <p>Upon aggregating signals from all candlestick patterns, we observe a signal nearly every trading day, alongside a marginally lower annualized volatility compared to the benchmark buy-and-hold strategy. However, these advantages are overshadowed by the superior performance of the buy-and-hold strategy, which not only delivers higher returns but also boasts a 38% higher Sharpe Ratio. Furthermore, when considering transaction costs\u2014which are significantly lower for the buy-and-hold strategy due to its singular trading requirement\u2014the choice between these two strategies becomes straightforward. The buy-and-hold strategy's efficiency and cost-effectiveness make it the preferred option over the active trading approach suggested by candlestick patterns. There is only one caveat regarding the buy-and-hold approach: We no not know a priori whether a stock goes up-or down. Let us now proceed to our final strategy in this notebook, namely applying Machine learnign to candlestick patterns.</p> <p>The following code pivots the dataframe such that we obtain the date as an index and the candlestick signals as columns. Joinging two or more data frames is a very important and frequently occuring task in data science in general and computing in particular, and again a topic on its own. The author encourages you to study merge, joins and concatonations of dataframes in your own time, as we need to focus here at the task at hand, however, I cannot stress their importance enough. Again, method chaining shows the step-by-step approach which keeps this cruicial step transparent, yet concise.</p> In\u00a0[29]: Copied! <pre>df_signal_and_return = (\n    cs_signals_df.reset_index()\n    .pivot_table(index=\"date\", columns=\"candle\", values=\"cs_pattern\", aggfunc=\"first\")\n    .fillna(0)\n    .join(df.set_index(\"date\")[[\"sign_next_day_return\"]], how=\"left\")\n)\n\ndf_signal_and_return.head()\n</pre> df_signal_and_return = (     cs_signals_df.reset_index()     .pivot_table(index=\"date\", columns=\"candle\", values=\"cs_pattern\", aggfunc=\"first\")     .fillna(0)     .join(df.set_index(\"date\")[[\"sign_next_day_return\"]], how=\"left\") )  df_signal_and_return.head() Out[29]: CDL2CROWS CDL3BLACKCROWS CDL3INSIDE CDL3LINESTRIKE CDL3OUTSIDE CDL3WHITESOLDIERS CDLADVANCEBLOCK CDLBELTHOLD CDLBREAKAWAY CDLCLOSINGMARUBOZU CDLCOUNTERATTACK CDLDARKCLOUDCOVER CDLDOJI CDLDOJISTAR CDLDRAGONFLYDOJI CDLENGULFING CDLEVENINGDOJISTAR CDLEVENINGSTAR CDLGAPSIDESIDEWHITE CDLGRAVESTONEDOJI CDLHAMMER CDLHANGINGMAN CDLHARAMI CDLHARAMICROSS CDLHIGHWAVE CDLHIKKAKE CDLHIKKAKEMOD CDLHOMINGPIGEON CDLIDENTICAL3CROWS CDLINNECK CDLINVERTEDHAMMER CDLLADDERBOTTOM CDLLONGLEGGEDDOJI CDLLONGLINE CDLMARUBOZU CDLMATCHINGLOW CDLMATHOLD CDLMORNINGDOJISTAR CDLMORNINGSTAR CDLONNECK CDLPIERCING CDLRICKSHAWMAN CDLRISEFALL3METHODS CDLSEPARATINGLINES CDLSHOOTINGSTAR CDLSHORTLINE CDLSPINNINGTOP CDLSTALLEDPATTERN CDLSTICKSANDWICH CDLTAKURI CDLTASUKIGAP CDLTHRUSTING CDLTRISTAR CDLUNIQUE3RIVER CDLUPSIDEGAP2CROWS CDLXSIDEGAP3METHODS sign_next_day_return date 1992-06-29 0.0 0.0 0.0 0.0 0.0 0.0 0.0 100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 -1 1992-06-30 0.0 0.0 0.0 0.0 0.0 0.0 0.0 -100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 -100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 1992-07-01 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 -1 1992-07-02 0.0 0.0 0.0 0.0 0.0 0.0 0.0 -100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 -100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 -1 1992-07-06 0.0 0.0 0.0 0.0 -100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 -1 <p>In the next cell, it is demonstrated explicitely that the column named <code>sign_next_day_return</code> is a categorical variable. Either the return can be positive or negative. Let us neglect the few cases where they are zero. We will filter these cases as to limit ourselves witha binary classification problem. Technically, we could, in logistic regression, accommodate for the occurance of three cathegorical variables, namely positive returns, negative returns and zero returns. However, zero returns rarely occur, and machine learning is data hungy. Hence, we filter them out. Also, the next cell shows that days of zero returns are a clear minority anyway. Hence, we feel comfortable our slight simplification is indeed empirically justified.</p> In\u00a0[30]: Copied! <pre>df_signal_and_return[\"sign_next_day_return\"].value_counts(normalize=True)\n</pre> df_signal_and_return[\"sign_next_day_return\"].value_counts(normalize=True) Out[30]: <pre>sign_next_day_return\n1     0.496058\n-1    0.486695\n0     0.017247\nName: proportion, dtype: Float64</pre> In\u00a0[31]: Copied! <pre>N_CORES = multiprocessing.cpu_count()\nTEST_SIZE = 0.1\n\ndf_filtered = df_signal_and_return[df_signal_and_return[\"sign_next_day_return\"] != 0]\n\nX = df_filtered.iloc[:, :-1]\ny = df_filtered.iloc[:, -1].astype(int)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=TEST_SIZE, shuffle=False\n)\n\n# let us quickly confirm whether the percentage of our train-test split is correct\nprint(X_train.__len__() / X.__len__())\nprint(X_test.__len__() / X.__len__())\n</pre> N_CORES = multiprocessing.cpu_count() TEST_SIZE = 0.1  df_filtered = df_signal_and_return[df_signal_and_return[\"sign_next_day_return\"] != 0]  X = df_filtered.iloc[:, :-1] y = df_filtered.iloc[:, -1].astype(int) X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=TEST_SIZE, shuffle=False )  # let us quickly confirm whether the percentage of our train-test split is correct print(X_train.__len__() / X.__len__()) print(X_test.__len__() / X.__len__()) <pre>0.8998830018385425\n0.10011699816145746\n</pre> <p>Note: You could have also slided the data set into training and test splits yourself, for example like so:</p> <pre><code>split_index = int(0.8 * len(df_filtered))  # 80% for training, 20% for testing\ndf_train = df_filtered.iloc[:split_index]\ndf_test = df_filtered.iloc[split_index:]\n</code></pre> <p>In <code>Python</code> and <code>Pandas</code>, slicing follows a consistent rule: the start index is included, and the stop index is excluded. This is known as half-open interval: <code>[start, stop)</code>. Hence, slicing is end-exclusive and <code>df_train</code> and <code>df_test</code> are non-overlapping. Hence, slicing achieves the same result we achieved by using <code>sklearn's train_test_split</code>.</p> <p>Let us now fit the logistic regressor to the training data.</p> In\u00a0[32]: Copied! <pre>%%time\n\nmodel = LogisticRegression(max_iter=5000, tol=0.01, n_jobs=N_CORES)\nmodel.fit(X_train, y_train)\n\n# Prediction on training set\npred_train = model.predict(X_train)\nassert len(pred_train) == len(y_train)\n\n# Prediction on test set\npred_test = model.predict(X_test)\nassert len(pred_test) == len(y_test)\n\nprint(\n    \"Compute hit-rate of strategy obtained by the logistic regressor on the TRAINING set\"\n)\nprint((pred_train == y_train).sum() / len(y_train), end=\"\\n\\n\")\n\nprint(\"Compute hit-rate of strategy obtained by the logistic regressor on the TEST set\")\nprint((pred_test == y_test).sum() / len(y_test), end=\"\\n\\n\")\n</pre> %%time  model = LogisticRegression(max_iter=5000, tol=0.01, n_jobs=N_CORES) model.fit(X_train, y_train)  # Prediction on training set pred_train = model.predict(X_train) assert len(pred_train) == len(y_train)  # Prediction on test set pred_test = model.predict(X_test) assert len(pred_test) == len(y_test)  print(     \"Compute hit-rate of strategy obtained by the logistic regressor on the TRAINING set\" ) print((pred_train == y_train).sum() / len(y_train), end=\"\\n\\n\")  print(\"Compute hit-rate of strategy obtained by the logistic regressor on the TEST set\") print((pred_test == y_test).sum() / len(y_test), end=\"\\n\\n\") <pre>Compute hit-rate of strategy obtained by the logistic regressor on the TRAINING set\n0.5338038632986627\n\nCompute hit-rate of strategy obtained by the logistic regressor on the TEST set\n0.5208681135225376\n\nCPU times: user 56.5 ms, sys: 132 ms, total: 188 ms\nWall time: 694 ms\n</pre> In\u00a0[33]: Copied! <pre># quick remark: we could have achieved the same result more tersly, opon invoking:\nnp.mean(pred_test == y_test)\n</pre> # quick remark: we could have achieved the same result more tersly, opon invoking: np.mean(pred_test == y_test) Out[33]: <pre>0.5208681135225376</pre> <p>Why can we use <code>np.mean(pred == y_test)</code> to compute the hit-rate efficiently?</p> <p>This is because of how boolean values are treated in numerical operations in Python. When you compare two arrays (or lists) element-wise for equality (pred == y_test), you get an array of boolean values (True or False). In this boolean array, True represents a hit (where the corresponding elements in pred and y_test are equal), and False represents a miss.</p> <p>In Python, boolean values are a subtype of integers. True is treated as 1, and False is treated as 0. Therefore, when you calculate the mean of an array of boolean values, you are effectively calculating the proportion of True values in the array. This is exactly the hit rate - the proportion of times pred and y_test are equal. The sum of this boolean array gives you the number of hits (since True is 1 and False is 0, the sum is the count of True values).</p> <p>Dividing this sum by the length of the array gives you the average value, which is the proportion of hits. So, np.mean(pred == y_test) is a neat way to calculate the hit rate directly from the arrays without explicitly counting the hits and dividing by the total number of elements.</p> In\u00a0[34]: Copied! <pre>y_train.value_counts(\n    normalize=True\n)  # the training set caontains an approximately equal proportion of postitive and negative returns\n</pre> y_train.value_counts(     normalize=True )  # the training set caontains an approximately equal proportion of postitive and negative returns Out[34]: <pre>sign_next_day_return\n 1    0.502043\n-1    0.497957\nName: proportion, dtype: float64</pre> In\u00a0[35]: Copied! <pre>y_test.value_counts(\n    normalize=True\n)  # the test set contains more positive than negative returns\n</pre> y_test.value_counts(     normalize=True )  # the test set contains more positive than negative returns Out[35]: <pre>sign_next_day_return\n 1    0.529215\n-1    0.470785\nName: proportion, dtype: float64</pre> In\u00a0[36]: Copied! <pre>performance_ml_strategy = (\n    df[df[\"date\"].isin(y_test.index)][\n        [\"date\", \"intraday_return\", \"sign_next_day_return\"]\n    ]\n    .dropna()\n    .reset_index(drop=True)\n    .assign(pred=pred_test)  # directly assigning pred without reindexing\n    .assign(\n        daily_return_strategy=lambda x: np.where(\n            np.sign(x[\"sign_next_day_return\"]) == np.sign(x[\"pred\"]),\n            abs(x[\"intraday_return\"]),\n            -abs(x[\"intraday_return\"]),\n        )\n    )\n    .assign(account_curve=lambda x: x[\"daily_return_strategy\"].cumsum())\n    .assign(contrarian_account_curve=lambda x: -1 * x[\"daily_return_strategy\"])\n)\nperformance_ml_strategy\n</pre> performance_ml_strategy = (     df[df[\"date\"].isin(y_test.index)][         [\"date\", \"intraday_return\", \"sign_next_day_return\"]     ]     .dropna()     .reset_index(drop=True)     .assign(pred=pred_test)  # directly assigning pred without reindexing     .assign(         daily_return_strategy=lambda x: np.where(             np.sign(x[\"sign_next_day_return\"]) == np.sign(x[\"pred\"]),             abs(x[\"intraday_return\"]),             -abs(x[\"intraday_return\"]),         )     )     .assign(account_curve=lambda x: x[\"daily_return_strategy\"].cumsum())     .assign(contrarian_account_curve=lambda x: -1 * x[\"daily_return_strategy\"]) ) performance_ml_strategy Out[36]: date intraday_return sign_next_day_return pred daily_return_strategy account_curve contrarian_account_curve 0 2020-01-30 -0.007297 -1 -1 0.007297 0.007297 -0.007297 1 2020-01-31 -0.011498 1 1 0.011498 0.018794 -0.011498 2 2020-02-03 0.023177 1 -1 -0.023177 -0.004382 0.023177 3 2020-02-05 -0.022442 1 -1 -0.022442 -0.026824 0.022442 4 2020-02-06 0.014699 1 -1 -0.014699 -0.041523 0.014699 ... ... ... ... ... ... ... ... 594 2022-12-20 0.010025 1 -1 -0.010025 0.601202 0.010025 595 2022-12-21 0.011337 -1 -1 0.011337 0.612538 -0.011337 596 2022-12-27 -0.007289 -1 1 -0.007289 0.605249 0.007289 597 2022-12-28 -0.009962 1 -1 -0.009962 0.595286 0.009962 598 2022-12-29 0.022746 1 1 0.022746 0.618032 -0.022746 <p>599 rows \u00d7 7 columns</p> In\u00a0[37]: Copied! <pre>plt.figure(figsize=(10, 10))\n\n# Define common bins\nmin_value = min(\n    performance_ml_strategy[\"intraday_return\"].min(),\n    performance_ml_strategy[\"intraday_return\"].min(),\n)\nmax_value = max(\n    performance_ml_strategy[\"intraday_return\"].max(),\n    performance_ml_strategy[\"intraday_return\"].max(),\n)\nbins = np.linspace(min_value, max_value, 40)\n\n# Calculate histogram for 'hits'\nhits_data = performance_ml_strategy[\n    performance_ml_strategy[\"sign_next_day_return\"] == performance_ml_strategy[\"pred\"]\n][\"intraday_return\"]\ncounts_hits, _ = np.histogram(hits_data, bins=bins)\npercent_hits = counts_hits / counts_hits.sum()\n\n# Calculate histogram for 'misses'\nmisses_data = performance_ml_strategy[\n    performance_ml_strategy[\"sign_next_day_return\"] != performance_ml_strategy[\"pred\"]\n][\"intraday_return\"]\ncounts_misses, _ = np.histogram(misses_data, bins=bins)\npercent_misses = counts_misses / counts_misses.sum()\n\n# Verify that the sum of the percentages is equal to 1\nprint(\"Sum of percentages for hits histogram:\", np.sum(percent_hits))\nprint(\"Sum of percentages for misses histogram:\", np.sum(percent_misses))\n\n# Plot the histograms\nplt.hist(bins[:-1], bins, weights=percent_hits, alpha=0.5, label=\"hits\")\nplt.hist(bins[:-1], bins, weights=percent_misses, alpha=0.5, label=\"misses\")\n\nplt.legend()\nplt.title(\"Intraday return histogram [percentage]\")\nplt.xlabel(\"intraday return\")\nplt.ylabel(\"percentage\")\nplt.show()\n</pre> plt.figure(figsize=(10, 10))  # Define common bins min_value = min(     performance_ml_strategy[\"intraday_return\"].min(),     performance_ml_strategy[\"intraday_return\"].min(), ) max_value = max(     performance_ml_strategy[\"intraday_return\"].max(),     performance_ml_strategy[\"intraday_return\"].max(), ) bins = np.linspace(min_value, max_value, 40)  # Calculate histogram for 'hits' hits_data = performance_ml_strategy[     performance_ml_strategy[\"sign_next_day_return\"] == performance_ml_strategy[\"pred\"] ][\"intraday_return\"] counts_hits, _ = np.histogram(hits_data, bins=bins) percent_hits = counts_hits / counts_hits.sum()  # Calculate histogram for 'misses' misses_data = performance_ml_strategy[     performance_ml_strategy[\"sign_next_day_return\"] != performance_ml_strategy[\"pred\"] ][\"intraday_return\"] counts_misses, _ = np.histogram(misses_data, bins=bins) percent_misses = counts_misses / counts_misses.sum()  # Verify that the sum of the percentages is equal to 1 print(\"Sum of percentages for hits histogram:\", np.sum(percent_hits)) print(\"Sum of percentages for misses histogram:\", np.sum(percent_misses))  # Plot the histograms plt.hist(bins[:-1], bins, weights=percent_hits, alpha=0.5, label=\"hits\") plt.hist(bins[:-1], bins, weights=percent_misses, alpha=0.5, label=\"misses\")  plt.legend() plt.title(\"Intraday return histogram [percentage]\") plt.xlabel(\"intraday return\") plt.ylabel(\"percentage\") plt.show() <pre>Sum of percentages for hits histogram: 1.0\nSum of percentages for misses histogram: 1.0\n</pre> In\u00a0[38]: Copied! <pre>performance_ml_strategy.plot(\n    x=\"date\",\n    y=\"account_curve\",\n    title=\"Comparing account curve of Machine Learning strategy\",\n    figsize=(10, 10),\n)\nplt.show()\n</pre> performance_ml_strategy.plot(     x=\"date\",     y=\"account_curve\",     title=\"Comparing account curve of Machine Learning strategy\",     figsize=(10, 10), ) plt.show() In\u00a0[39]: Copied! <pre>fig, ax = plt.subplots(figsize=(8, 8))\n\n# Plotting the first curve\nax.plot(\n    performance_trading_signals[\"date\"],\n    performance_trading_signals[\"cumsumret\"],\n    label=\"cumulative return using candlesticks\",\n    color=\"b\",\n)\n\n# Plotting the second curve\nax.plot(\n    df_reference_strategy[\"date\"],\n    df_reference_strategy[\"cumsumret\"],\n    label=\"cumulative return assuming buy-and-hold\",\n    color=\"r\",\n)\n\n# Adding the third curve from 'tmp'\nax.plot(\n    performance_ml_strategy[\"date\"],\n    performance_ml_strategy[\"account_curve\"],\n    label=\"cumulative return using ML strategy\",\n    color=\"g\",\n)  # Assuming color green\n\nax.set_xlabel(\"date\")\nax.set_ylabel(\"cumulative return\")\nax.legend(loc=\"upper left\")\nax.set_title(\"long-short Strategy using candlesticks vs. buy-and-bold vs. ML strategy\")\n\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(8, 8))  # Plotting the first curve ax.plot(     performance_trading_signals[\"date\"],     performance_trading_signals[\"cumsumret\"],     label=\"cumulative return using candlesticks\",     color=\"b\", )  # Plotting the second curve ax.plot(     df_reference_strategy[\"date\"],     df_reference_strategy[\"cumsumret\"],     label=\"cumulative return assuming buy-and-hold\",     color=\"r\", )  # Adding the third curve from 'tmp' ax.plot(     performance_ml_strategy[\"date\"],     performance_ml_strategy[\"account_curve\"],     label=\"cumulative return using ML strategy\",     color=\"g\", )  # Assuming color green  ax.set_xlabel(\"date\") ax.set_ylabel(\"cumulative return\") ax.legend(loc=\"upper left\") ax.set_title(\"long-short Strategy using candlesticks vs. buy-and-bold vs. ML strategy\")  plt.show() In\u00a0[40]: Copied! <pre># investigate the account curve at the end of the test period\nperformance_ml_strategy.tail(2)\n</pre> # investigate the account curve at the end of the test period performance_ml_strategy.tail(2) Out[40]: date intraday_return sign_next_day_return pred daily_return_strategy account_curve contrarian_account_curve 597 2022-12-28 -0.009962 1 -1 -0.009962 0.595286 0.009962 598 2022-12-29 0.022746 1 1 0.022746 0.618032 -0.022746 In\u00a0[41]: Copied! <pre># annual geometric return during test period (the past three years)\n(1 + performance_ml_strategy[\"account_curve\"].iloc[-1]) ** (1 / 3) - 1\n</pre> # annual geometric return during test period (the past three years) (1 + performance_ml_strategy[\"account_curve\"].iloc[-1]) ** (1 / 3) - 1 Out[41]: <pre>0.17398453769450772</pre> In\u00a0[42]: Copied! <pre>compute_trading_strategy_performance(df=performance_ml_strategy, verbose=True);\n</pre> compute_trading_strategy_performance(df=performance_ml_strategy, verbose=True); <pre>Annualised strategy return [%]: 0.0997\nAnnualised strategy standard deviation of returns [%]: 0.2673\nSharpe ratio of strategy: 0.3730\n</pre> In\u00a0[43]: Copied! <pre>(1 + 0.2173742597803736) ** 3 - 1  # 3 years of history for the test set\n</pre> (1 + 0.2173742597803736) ** 3 - 1  # 3 years of history for the test set Out[43]: <pre>0.8041487605809532</pre> # Strategy: Naive candlesticks Annualised strategy return [%]: 0.07853 Annualised strategy standard deviation of returns [%]: 0.2586 Sharpe ratio of strategy: 0.3037  # Strategy: Machine learning applied to candlesticks Annualised strategy return [%]: 0.141 Annualised strategy standard deviation of returns [%]: 0.2653 Sharpe ratio of strategy: 0.5313      # strategy: Buy and hold    Annualised strategy return [%]: 0.1097 Annualised strategy standard deviation of returns [%]: 0.2625 Sharpe ratio of strategy: 0.4181  <p><code>sklearn.feature_selection.RFECV</code> is a feature selection method from scikit-learn that stands for <code>Recursive Feature Elimination with Cross-Validation</code>. It is used to select the most significant features by recursively considering smaller and smaller sets of features.</p> <ul> <li><p>Recursive Feature Elimination (RFE): It fits a model and removes the weakest feature(s) (i.e., the feature(s) with the least importance according to the model), then fits a new model on the reduced dataset. This process is repeated until all features have been ranked by their importance.</p> </li> <li><p>Cross-Validation (CV): The RFE procedure is embedded within a cross-validation loop to determine the optimal number of features. For each iteration, a different subset of the data is used for training and validation, and the performance of the model is assessed. This helps to ensure that the selection of features is robust and performs well on unseen data.</p> </li> </ul> <p><code>selector.ranking_</code>: This array shows the ranking of features. A rank of 1 indicates that the feature was selected as important. Higher ranks indicate the order in which features were removed, where a rank of 2 means the feature was the second least important, a rank of 3 means the feature was the third least important, and so on.</p> <p>Looking at <code>selector.ranking_</code> array, it seems that most features were selected (rank 1). Some features have higher rank values, like 4 and 3 indicating their relative importance was considered lower by the <code>RFECV</code> process. These features were removed in the corresponding iterations of the feature elimination process.</p> <p>The feature with a rank of 4 was removed first, the one with 3 was removed second, and this pattern continues until all features with a rank greater than 1 are removed. The features with a rank of 1 are considered important and are selected by the <code>RFECV</code>. These are the features you might consider keeping in your model for the best balance between model complexity and performance as determined by cross-validation.</p> In\u00a0[44]: Copied! <pre>%%time\n# takes a about 80 seconds to run for one stock, but surely is dependent on the number of CPU cores on your machine.\n\nN_CORES = multiprocessing.cpu_count()\nmodel = LogisticRegressionCV(\n    cv=5, multi_class=\"ovr\", max_iter=1000, n_jobs=N_CORES\n)  # If we choose multi_class=\u2018ovr\u2019, then a binary problem is fit for each label.\nmodel.fit(X_train, y_train)\n\nselector = RFECV(model, cv=5, n_jobs=N_CORES)\nselector.fit(X_train, y_train)\nprint(selector.ranking_)\nselected_features = selector.get_support()\n</pre> %%time # takes a about 80 seconds to run for one stock, but surely is dependent on the number of CPU cores on your machine.  N_CORES = multiprocessing.cpu_count() model = LogisticRegressionCV(     cv=5, multi_class=\"ovr\", max_iter=1000, n_jobs=N_CORES )  # If we choose multi_class=\u2018ovr\u2019, then a binary problem is fit for each label. model.fit(X_train, y_train)  selector = RFECV(model, cv=5, n_jobs=N_CORES) selector.fit(X_train, y_train) print(selector.ranking_) selected_features = selector.get_support() <pre>[ 1  1  1  1 15  1  1 13  1  1  1  1  1  3  1 21  1  5  1 17  1  7 22  1\n  8 10  2  1  1  1  4  1  1  9 12  1  1  1  6  1  1 16  1  1  1 14 11  1\n  1  1 19 20  1  1  1 18]\nCPU times: user 25 s, sys: 25.1 s, total: 50.1 s\nWall time: 1min 36s\n</pre> In\u00a0[45]: Copied! <pre>selector.ranking_.__len__()  # this corresponds to the number of candlesticks we investigate\n</pre> selector.ranking_.__len__()  # this corresponds to the number of candlesticks we investigate Out[45]: <pre>56</pre> In\u00a0[46]: Copied! <pre>pred = model.predict(X_test)\nassert len(pred) == len(y_test)\nprint(\"Compute hit-rate of strategy obtained from the logistic regressor\")\nprint((pred == y_test).sum() / len(y_test))\nmodel.score(X_test, y_test)  # gives same result\n</pre> pred = model.predict(X_test) assert len(pred) == len(y_test) print(\"Compute hit-rate of strategy obtained from the logistic regressor\") print((pred == y_test).sum() / len(y_test)) model.score(X_test, y_test)  # gives same result <pre>Compute hit-rate of strategy obtained from the logistic regressor\n0.5242070116861436\n</pre> Out[46]: <pre>0.5242070116861436</pre> In\u00a0[47]: Copied! <pre># Apply this mask to X_train and X_test to keep only important features\nX_train_selected = X_train.loc[:, selected_features]\nX_test_selected = X_test.loc[:, selected_features]\n\n# Refit the model on the selected features\nmodel_selected = LogisticRegressionCV(\n    cv=5, multi_class=\"ovr\", max_iter=1000, n_jobs=N_CORES\n)\nmodel_selected.fit(X_train_selected, y_train)\n\ny_pred_selected = model_selected.predict(X_test_selected)\nmodel_selected.score(X_test_selected, y_test)\n</pre> # Apply this mask to X_train and X_test to keep only important features X_train_selected = X_train.loc[:, selected_features] X_test_selected = X_test.loc[:, selected_features]  # Refit the model on the selected features model_selected = LogisticRegressionCV(     cv=5, multi_class=\"ovr\", max_iter=1000, n_jobs=N_CORES ) model_selected.fit(X_train_selected, y_train)  y_pred_selected = model_selected.predict(X_test_selected) model_selected.score(X_test_selected, y_test) Out[47]: <pre>0.5008347245409015</pre> <p>Unfortunately, recursive feature elimination did not imporove the Machine Learning strategy.</p> In\u00a0[48]: Copied! <pre># your improvements go here\n</pre> # your improvements go here"},{"location":"2_single_stock_case/#candlestick-patterns-for-one-stock","title":"Candlestick patterns for one stock\u00b6","text":""},{"location":"2_single_stock_case/#load-libraries","title":"Load libraries\u00b6","text":"<p>At first, we will load up some dependencies that we will make use of later on. We follow the convention of naming them a standard alias for easier handling.</p>"},{"location":"2_single_stock_case/#what-is-a-candle-stick-and-what-are-candlestick-patterns","title":"What is a candle stick and what are candlestick patterns?\u00b6","text":"<p>This section aims at briefly describing what candlesticks are, so we bring everyone up to speed regarding this graphical representation of price data.</p>"},{"location":"2_single_stock_case/#anatomy-of-a-candlestick-aka-japanese-candlesticks","title":"Anatomy of a candlestick (aka Japanese candlesticks)\u00b6","text":"<p>A candlestick summarizes the four key price levels of a security within a specific time interval, such as 1 minute, 1 hour, or more commonly, 1 day. Candlesticks can also represent price data over various different intervals, such as 5 minutes, 4 hours, or any other specified duration.</p> <p>These price levels include the opening price, highest price, lowest price, and closing price within the specific time interval. Candlesticks are typically colored green when the <code>close price &gt; open price</code>, indicating a price increase during the interval, which defines a <code>bullish candle</code>. Conversely, they are colored red when the <code>close price &lt; open price</code>, indicating a price decrease, which defines a <code>bearish candle</code>. The area between the opening and closing prices is known as the body of the candlestick.</p> <p>A candlestick can also feature wicks above and below the body, known as the <code>upper wick</code> and <code>lower wick</code>, respectively. The lower wick of a bullish candle represents the distance between the lowest price and the opening price. In case of a bearish candle, it is the distance between the lowest price and the closing price. Conversely, the upper wick of a bullish candle illustrates the range from the closing price to the highest price, or, or a bearish candle, the distance from the opening price to the highest price.</p> <p>Price summaries that adhere to the described structure are known as <code>Japanese candlesticks</code>. This naming traces back to their origins in Japan, where they were first developed. In contrast, practitioners in Europe and the United States commonly use a concept known as a price bar. Unlike candlesticks, price bars do not visually differentiate the market's mood (i.e., bullish or bearish) through a colored body. Instead, they indicate the opening and closing prices with markers positioned to the left and right, respectively. This makes price bars less graphically intuitive for interpreting market movements.</p> <p>The image below illustrates the concepts explained.</p>"},{"location":"2_single_stock_case/#candlestick-patterns","title":"Candlestick patterns\u00b6","text":"<p>Two or more candlesticks may form what is known as a candlestick pattern. The question arises: Why are these patterns significant? According to the efficient market hypothesis, they should not be. This theory suggests that all information affecting the price of an equity in a frictionless market is already accounted for in the price, based on the consensus between buyers and sellers at any moment the market is open. Crucially, this would mean prices follow a <code>Markov Process</code>, with the best predictor for the price at <code>t+1</code> being the price at <code>t</code>. Candlestick analysis, however, challenges this notion. It posits that certain patterns can predict price movements in the subsequent time interval, indicating that prices may not solely rely on the most recent information, but that, in fact, history repeats it self following specific patterns.</p> <p>This project does not seek to challenge the efficient market hypothesis directly. Rather, our aim is to analyze a substantial volume of financial data from liquid stocks within the <code>S&amp;P 500</code>, sourced from a reliable provider, to determine if there is statistical evidence supporting the hypothesis of those following candle stick analysis. Our null hypothesis posits that the likelihood of a positive intraday return at <code>t+1</code> is equal to the likelihood of a negative intraday return at <code>t+1</code>, both set at 50%, or expressed formulaically as:</p> <p>$$ P(\\text{positive intraday return})_{t+1} = P(\\text{negative intraday return})_{t+1} = 0.5 $$</p> <p>Let us now examine some \"special\" candlesticks and patterns, as illustrated in the image below. \"Special\" candlesticks, such as the <code>Hammer</code> and the <code>Black Marubozu</code>, can independently signal potential future price movements without forming patterns with other candlesticks. Take the <code>Black Marubozu</code> as an example. This candlestick indicates a trading period dominated by declining prices. While we do not have tick-by-tick price details, which might have indeed shown temporary periods of rising prices, the <code>Black Marubozu</code> reveals that the period opened at its highest price (<code>open price = high price</code>) and closed at its lowest (<code>close price = low price</code>). This scenario spells trouble for traders in long positions but is ideal for those holding short positions. The presence of a <code>Black Marubozu</code> suggests an expectation for prices to continue falling in the subsequent period.</p> <p>Examining the <code>Black Morning Star</code>, we observe it consists of three candles. The pattern begins with two bearish candles, indicating a price decline over these periods. Notably, the second candle opens just below the close of the first, creating what is known as a <code>body gap</code>. This detail underscores the momentum of the downward trend, more so than if the second candle's opening were within the body of the first. Despite a slight recovery marking the day's high, the price falls again, closing slightly higher than its low. Up to this point, the narrative has been decidedly bearish. However, the third period brings a turn of events. It opens above the previous candle's opening price, signaling a potential shift. The third time interval is marked by predominately rising prices, closing significantly above its opening price, and crucially, higher than the opening price of day one. This formation, known as a <code>Black Morning Star</code>, is interpreted by candlestick analysts as signaling the end of a downtrend and suggesting a potential rise in prices in the following period. Whether this holds true will be statisticdally evaluated in this project.</p>"},{"location":"2_single_stock_case/#loading-and-pre-processing-the-data","title":"Loading and pre-processing the data\u00b6","text":"<p>We can now proceed loading and analysing data. We limit ourselves to a one stock case in this notebook to demonstrate the concepts and progress towardes the whole investment universe in the next one. Given the structure of the project, you need to pass the filename of the data that you previously downloaded from <code>WRDS</code>. In our case that filename is <code>SP500_daily_data_1980_to_2023.csv.gz</code>. I set up some helper functons that assist you doing this. The relevant functions can be imported like so:</p> <pre><code> from BSquant import load_data\n from BSquant import process_data</code></pre> <p>In software engineering there is the principle of <code>high cohesion</code> within functions, to enhancee code maintainability. High cohesion means each function is dedicated to a single task, thereby reducing dependencies among different parts of the code. For example, in our case, <code>load_data</code> is solely focused on loading data, and <code>process_data</code> is specifically designed for preprocessing steps. This separation of concerns not only clarifies the code's structure but also simplifies future modifications and debugging.</p> <p>Furthermore, it is crucial to document each function clearly. A well-crafted docstring should accompany every function, explaining its purpose, parameters, and return values. This practice not only aids your understanding but also assists your fellow researchers in grasping the functionality and intent of the code.</p> <p>You can display a function's docstring by using the <code>help(</code>) function and passing the function's name as an argument. Alternatively, to view a function's definition, you can utilize the <code>inspect</code> module. Let us explore each method to ensure you can effectively apply them in the future.</p>"},{"location":"2_single_stock_case/#displaying-price-data-in-candlestick-form","title":"Displaying price data in candlestick form\u00b6","text":"<p>In this section, we explore how to plot candlestick charts in Python. While plotting data for extensive analysis is cumbersome (imagine plotting several years of data for 500 stocks), we will primarily rely on computation to assess the effectiveness of candlestick patterns. Nonetheless, for a small subset of data and for illustrative purposes, visualizing candlestick charts remains a worthwhile exercise.</p> <p>We will examine two methods:</p> <pre><code>1) An efficient (yet less aesthetic) method using an `np.array` and `matplotlib`.\n2) A more aesthetic (but less efficient) method using a `pd.DataFrame` and `plotly`.</code></pre>"},{"location":"2_single_stock_case/#efficient-and-less-aestetic-method-using-an-nparray-and-matplotlib","title":"Efficient (and less aestetic) method using an <code>np.array</code> and <code>matplotlib</code>\u00b6","text":"<p>The function <code>ohlc_plot_candles</code> is a modification from https://github.com/sofienkaabar/mastering-financial-pattern-recognition/blob/main/Master_Function_MFPR.py and utilizes only OHLC (Open, High, Low, Close) data. This data is selected from our processed DataFrame, and a lookback period is specified. As discussed in the section \"Anatomy of a Candlestick (aka Japanese Candlesticks),\" we denote bullish days in green and bearish days in red, with upper and lower wicks marked in black.</p>"},{"location":"2_single_stock_case/#more-aestetic-but-less-efficient-method-using-a-pddataframe-and-plotly","title":"More aestetic (but less efficient) method using a <code>pd.DataFrame</code> and <code>plotly</code>\u00b6","text":"<p>The second method employs <code>plotly</code>, a library renowned for producing publication-ready, interactive plots that are especially useful within a <code>Jupyter notebook</code> environment. While <code>plotly</code> is more complex and less memory-efficient than <code>matplotlib</code>, it excels in creating visually appealing charts suitable for a limited dataset. The general recommendation is to prototype with <code>matplotlib</code> due to its simplicity and efficiency, and then switch to <code>plotly</code> for presenting final results.</p> <p>To utilize this method, we supply the OHLC data in a <code>DataFrame</code>, with an additional requirement for the <code>date</code> column. This inclusion is crucial for accurately formatting the date on the x-axis. Furthermore, it necessitates setting the <code>DataFrame's</code> index to the date column, which should contain datetime objects. Additionally, for improved visibility, we constrain the <code>DataFrame</code> to display only the most recent 100 days of data.</p>"},{"location":"2_single_stock_case/#what-candlestick-patterns-are-available","title":"What candlestick patterns are available?\u00b6","text":"<p>Having understood the visual aspects of candlestick patterns, our next step is to identify these patterns within financial data. Fortunately, we have access to <code>talib</code>, a pattern recognition library written in <code>C++</code>. Its efficiency not only provides convenience but also spares us the considerable effort of manually coding these patterns, a task that could easily constitute a Bachelor's or even Master's thesis.</p> <p><code>talib</code> is proficient in detecting candlestick patterns, indicating not only their occurrence but also classifying them as bearish or bullish. The library is capable of recognizing 63 distinct candlestick patterns. For example, the first five patterns include <code>2 Crows</code>, <code>3 Black Crows</code>, <code>3 Inside</code>, <code>3 Line Strike</code>, and <code>3 Outside</code>. While the names of these patterns are a topic on their own for enthusiasts, our focus is on the signals they provide and whether the subsequent intraday returns align with their predictions.</p> <p>The function <code>cs_pattern_recognition</code> living within <code>BSquant.py</code> is instrumental in performing candlestick pattern recognition, utilizing our previously processed data as its input. This function generates a multi-index DataFrame, where the primary index corresponds to the identified candlestick pattern, and the secondary index represents the date on which the pattern occurred. The use of multi-index DataFrames is highly recommended for those interested in delving deeper into advanced data science techniques. While this notebook will guide you through the necessary steps to achieve our specific analytical goals, it is worth noting that our discussion on the functionalities offered by multi-indexed DataFrames is not comprehensive.</p> <p>Recall you can access the docstring as well as the sourcecode yourself by invoking:</p> <pre><code>help(cs_pattern_recognition)\nprint(inspect.getsource(cs_pattern_recognition))\n</code></pre>"},{"location":"2_single_stock_case/#performance-evaluation-of-a-binary-classifier-the-confusion-matrix","title":"Performance evaluation of a binary classifier: The confusion matrix\u00b6","text":"<p>A confusion matrix is a table that quantifies and summarizes the performance of a classification algorithm, offering a visual representation of the algorithm's accuracy. This matrix is particularly useful in binary classification to evaluate the effectiveness of a trading algorithm. It comprises four key metrics:</p> <ul> <li><p>TP (True Positive): This metric counts the number of instances correctly identified as positive by the algorithm. In the context of trading, it represents the days correctly predicted to be profitable, where acting on the algorithm's signal results in a gain.</p> </li> <li><p>TN (True Negative): TN counts the days correctly identified as negative, where the trading algorithm correctly signals not to trade. These are the instances where inactivity based on the algorithm's advice is beneficial, avoiding losses.</p> </li> <li><p>FP (False Positive): Also known as Type I error, FP represents the instances where the algorithm incorrectly signals a positive outcome. In trading, this means entering a trade expecting a profit when the market actually moves in the opposite direction, resulting in a loss.</p> </li> <li><p>FN (False Negative): Known as Type II error, FN represents missed opportunities\u2014days the algorithm failed to signal a positive outcome when the market did move beneficially. These are instances where the algorithm's silence or negative prediction prevents participation in a profitable trade.</p> </li> </ul> <p>These four components of the confusion matrix allow traders and analysts to assess the reliability and errors of their trading algorithms, quantifying their predictive capabilities, misses and, consequently, areas for improvement.</p> <p>Performance metrics derived from a confusion matrix, such as <code>accuracy</code>, <code>precision</code>, <code>recall</code>, and the <code>F1 score</code>, provide a comprehensive evaluation of an algorithm's effectiveness. These metrics are defined as follows:</p> <ul> <li><p>Accuracy: This metric measures the overall correctness of the algorithm and is calculated as the ratio of correctly classified trading days (TP + TN) to the total number of trading days (TP + TN + FP + FN). It reflects the algorithm\u2019s ability to correctly identify both profitable and non-profitable trading days.</p> </li> <li><p>Precision: Precision assesses the algorithm's accuracy in predicting profitable trading days. It is represented as the ratio of correctly predicted profitable days (TP) to the total number of days predicted as profitable (TP + FP). This metric emphasizes the quality of the positive predictions.</p> </li> <li><p>Recall (Sensitivity): Recall measures the algorithm\u2019s ability to identify actual profitable trading days. It is defined as the ratio of correctly predicted profitable days (TP) to the total number of actual profitable days (TP + FN). Recall, therefore, focuses on the algorithm's capability to capture all potential profitable opportunities.</p> </li> <li><p>F1 Score: The F1 score balances precision and recall, providing a single metric to assess their harmonic mean. It is particularly useful when the cost of false positives and false negatives varies. The F1 score is calculated as 2 * (precision * recall) / (precision + recall), offering a measure of the algorithm\u2019s efficiency in terms of both precision and recall.</p> </li> </ul> <p>As each candle stick signal is unlikely to occur frequently on its own, its true negative value is less of interest for our consideration. Hence, we focus on <code>precision</code>, but please feel free to implement other performace measures on your own.</p> <p>For further information on confusion matrices, see:</p> <ul> <li>https://www.kdnuggets.com/2020/01/guide-precision-recall-confusion-matrix.html</li> <li>https://towardsdatascience.com/performance-metrics-confusion-matrix-precision-recall-and-f1-score-a8fe076a2262</li> </ul>"},{"location":"2_single_stock_case/#developing-and-back-testing-trading-strategies-buy-and-hold-vs-naive-candlestick-signals-vs-machine-learning-applied-to-candlestick-patterns","title":"Developing and back-testing trading strategies: Buy and hold vs. naive candlestick signals vs. Machine Learning applied to candlestick patterns\u00b6","text":""},{"location":"2_single_stock_case/#aggregating-trading-signals","title":"Aggregating trading signals\u00b6","text":"<p>In the following we aggregate the \"predictive power\" of all candlestick patterns detectable by <code>talib</code> to produce a trading signal that we can backtest.</p>"},{"location":"2_single_stock_case/#how-does-acting-on-the-trading-signals-compare-to-a-buy-and-hold-strategy","title":"How does acting on the trading signals compare to a buy-and-hold strategy?\u00b6","text":""},{"location":"2_single_stock_case/#does-machine-learning-help-to-to-reduce-the-noise-of-the-signals","title":"Does Machine learning help to to reduce the noise of the signals?\u00b6","text":""},{"location":"2_single_stock_case/#summary","title":"Summary\u00b6","text":""},{"location":"2_single_stock_case/#recursive-feature-elimination","title":"Recursive feature elimination\u00b6","text":""},{"location":"2_single_stock_case/#your-task-can-you-improve-the-strategy-further","title":"Your task: Can you improve the strategy further?\u00b6","text":""},{"location":"2_single_stock_case/#literature","title":"Literature\u00b6","text":"<p>[1] Young Ho Seo, Scientific Guide To Price Action and Pattern Trading Wisdom of Trend, Cycle, and Fractal Wave</p> <p>[2] Thomas N. Bulkowski, Encyclopedia of Chart Patterns, Wiley Trading, 2021.</p> <p>If you are interested to back-test more trading strategies, you can look at:</p> <p>[3] Finding Alphas: A Quantitative Approach to Building Trading Strategies Hardcover \u2013 27 Sept. 2019 by Igor Tulchinsky (Editor)</p> <p>[4] 151 Trading Strategies, Z. Kakushadze and J.A. Serur. 151 Trading Strategies. Cham, Switzerland: Palgrave Macmillan, an imprint of Springer Nature, 1st Edition (2018), XX, 480 pp; ISBN 978-3-030-02791-9, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3247865</p>"},{"location":"2_single_stock_case/#appendix","title":"Appendix\u00b6","text":""},{"location":"2_single_stock_case/#a-side-note-regarding-software-engineering-terminology","title":"A side note regarding software engineering terminology\u00b6","text":"<p>In this notebook, we embarked on the term <code>high cohesion</code>. Another important concept is <code>coupling</code>, which refers to the degree of direct knowledge one component has of another. It is a measure of how closely connected different modules or components are within a system, indicating the level of dependency between them. Understanding coupling is important for designing maintainable, scalable, and resilient software systems. There are generally two types of coupling: high (or tight) coupling and low (or loose) coupling.</p> <ul> <li><p>High (tight) coupling: High coupling occurs when components are heavily dependent on each other. This means that a change in one component often necessitates changes in another, leading to a domino effect of modifications across the system. High coupling can make the system more complex and harder to understand, maintain, and test for several reasons:</p> <ul> <li>Change propagation: Changes in one module can lead to required changes in dependent modules.</li> <li>Difficulty in isolation: It becomes challenging to isolate modules for testing or for use in different contexts.</li> <li>Reduced reusability: Tightly coupled modules are often less reusable since they depend heavily on specific behavior in other parts of the system.</li> </ul> </li> <li><p>Low (loose) coupling: Loose coupling, on the other hand, is characterized by a minimal level of dependency among components. In a loosely coupled system, changes in one component have minimal or no impact on others. This approach offers several advantages:</p> <ul> <li>Ease of modification: Modules can be changed, replaced, or updated with minimal impact on the rest of the system.</li> <li>Enhanced testability: Components can be tested in isolation, simplifying the testing process.</li> <li>Increased reusability: Modules designed with few dependencies can be easily reused in different parts of the system or in different projects.</li> </ul> </li> <li><p>Achieving loose coupling: Loose coupling can be achieved through various design principles and patterns, including:</p> <ul> <li>Interface-based design: Defining clear interfaces between components so that they communicate using abstract contracts rather than concrete implementations.</li> <li>Dependency injection: Passing dependencies into an object rather than having it create them internally.</li> <li>Service-Oriented Architecture (SOA) or Microservices: Designing systems as collections of loosely coupled services.</li> <li>Publish-subscribe models: Components communicate through events or messages rather than direct calls, which decouples the sender from the receiver.</li> </ul> </li> </ul>"},{"location":"3_SP500_case/","title":"Candlestick patterns for the S&amp;P 500","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom pathlib import Path\nfrom collections import namedtuple\nfrom scipy import stats\n\nfrom BSquant import load_data\nfrom BSquant import process_data\nfrom BSquant import cs_pattern_recognition\nfrom BSquant import cs_performance\nfrom BSquant import plot_cs_performance\nfrom BSquant import compute_trading_strategy_performance\n\npd.set_option(\"display.max_columns\", None)\n%load_ext autoreload\n%autoreload 2\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd from collections import Counter from pathlib import Path from collections import namedtuple from scipy import stats  from BSquant import load_data from BSquant import process_data from BSquant import cs_pattern_recognition from BSquant import cs_performance from BSquant import plot_cs_performance from BSquant import compute_trading_strategy_performance  pd.set_option(\"display.max_columns\", None) %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre># Define the path to your ticker file\nticker_file = \"./../data/SP500_tickers_one_per_line.txt\"\nnotebooks_dir = Path(\"./../notebooks\")\nticker_file_path = notebooks_dir.parent / \"data\" / ticker_file\n\ntickers = []\n\n# Open the ticker-file with a context manager and read each line adding ot to the list of tickers\nwith open(ticker_file_path, \"r\") as file:\n    for line in file:\n        ticker = line.strip()  # strip newline characters and whitespace\n        tickers.append(ticker)  # add the cleaned ticker to the list\n\nprint(\"Number of tickers (may include multiple tickers per stock) is\", len(tickers))\nprint(\"Number of unique tickers is:\", set(tickers).__len__())\n</pre> # Define the path to your ticker file ticker_file = \"./../data/SP500_tickers_one_per_line.txt\" notebooks_dir = Path(\"./../notebooks\") ticker_file_path = notebooks_dir.parent / \"data\" / ticker_file  tickers = []  # Open the ticker-file with a context manager and read each line adding ot to the list of tickers with open(ticker_file_path, \"r\") as file:     for line in file:         ticker = line.strip()  # strip newline characters and whitespace         tickers.append(ticker)  # add the cleaned ticker to the list  print(\"Number of tickers (may include multiple tickers per stock) is\", len(tickers)) print(\"Number of unique tickers is:\", set(tickers).__len__()) <pre>Number of tickers (may include multiple tickers per stock) is 503\nNumber of unique tickers is: 503\n</pre> In\u00a0[3]: Copied! <pre># enumerated tickers in the ticker_file\nfor i, ticker in enumerate(tickers):\n    print(f\"{i+1}:{ticker}\")\n</pre> # enumerated tickers in the ticker_file for i, ticker in enumerate(tickers):     print(f\"{i+1}:{ticker}\") <pre>1:MSFT\n2:AAPL\n3:AMZN\n4:NVDA\n5:GOOGL\n6:META\n7:GOOG\n8:TSLA\n9:BRK.B\n10:UNH\n11:LLY\n12:JPM\n13:XOM\n14:V\n15:AVGO\n16:JNJ\n17:PG\n18:MA\n19:HD\n20:ADBE\n21:COST\n22:MRK\n23:CVX\n24:ABBV\n25:CRM\n26:PEP\n27:KO\n28:WMT\n29:BAC\n30:ACN\n31:NFLX\n32:MCD\n33:LIN\n34:CSCO\n35:AMD\n36:TMO\n37:INTC\n38:ORCL\n39:ABT\n40:CMCSA\n41:PFE\n42:DIS\n43:WFC\n44:VZ\n45:INTU\n46:DHR\n47:PM\n48:IBM\n49:AMGN\n50:QCOM\n51:NOW\n52:TXN\n53:COP\n54:UNP\n55:SPGI\n56:NKE\n57:GE\n58:BA\n59:HON\n60:CAT\n61:AMAT\n62:RTX\n63:T\n64:NEE\n65:LOW\n66:SBUX\n67:ELV\n68:GS\n69:BKNG\n70:UPS\n71:ISRG\n72:PLD\n73:MDT\n74:BLK\n75:BMY\n76:TJX\n77:MS\n78:LMT\n79:SYK\n80:DE\n81:AXP\n82:MMC\n83:AMT\n84:MDLZ\n85:PGR\n86:GILD\n87:LRCX\n88:ADP\n89:CB\n90:ADI\n91:VRTX\n92:SCHW\n93:ETN\n94:PANW\n95:C\n96:REGN\n97:CVS\n98:MU\n99:SNPS\n100:BSX\n101:ZTS\n102:BX\n103:FI\n104:CME\n105:TMUS\n106:CI\n107:SO\n108:EQIX\n109:MO\n110:KLAC\n111:CDNS\n112:SLB\n113:EOG\n114:DUK\n115:BDX\n116:NOC\n117:ITW\n118:AON\n119:SHW\n120:ICE\n121:CL\n122:CSX\n123:MCK\n124:PYPL\n125:WM\n126:TGT\n127:CMG\n128:APD\n129:HUM\n130:FDX\n131:MPC\n132:USB\n133:ORLY\n134:MCO\n135:PSX\n136:ROP\n137:GD\n138:PH\n139:ANET\n140:MMM\n141:APH\n142:PXD\n143:MSI\n144:ABNB\n145:AJG\n146:FCX\n147:PNC\n148:TDG\n149:NXPI\n150:LULU\n151:TT\n152:CCI\n153:EMR\n154:MAR\n155:HCA\n156:NSC\n157:WELL\n158:ECL\n159:PCAR\n160:CTAS\n161:AZO\n162:AIG\n163:ADSK\n164:NEM\n165:SRE\n166:MCHP\n167:AFL\n168:WMB\n169:DXCM\n170:ROST\n171:VLO\n172:HLT\n173:CPRT\n174:CARR\n175:GM\n176:TFC\n177:COF\n178:NUE\n179:DLR\n180:KMB\n181:TRV\n182:MSCI\n183:EW\n184:TEL\n185:MNST\n186:PSA\n187:AEP\n188:SPG\n189:CHTR\n190:F\n191:MET\n192:OKE\n193:CNC\n194:ADM\n195:OXY\n196:IQV\n197:PAYX\n198:CEG\n199:DHI\n200:HES\n201:STZ\n202:IDXX\n203:EXC\n204:O\n205:D\n206:A\n207:BK\n208:GIS\n209:SYY\n210:DOW\n211:AMP\n212:LHX\n213:ALL\n214:JCI\n215:PCG\n216:AME\n217:CTSH\n218:PRU\n219:OTIS\n220:KVUE\n221:YUM\n222:VRSK\n223:GWW\n224:ODFL\n225:FIS\n226:IT\n227:FAST\n228:FTNT\n229:KMI\n230:EA\n231:COR\n232:BIIB\n233:BKR\n234:CSGP\n235:XEL\n236:PPG\n237:HAL\n238:RSG\n239:DD\n240:URI\n241:LEN\n242:CTVA\n243:KDP\n244:CMI\n245:ROK\n246:PEG\n247:ED\n248:ACGL\n249:ON\n250:GPN\n251:VICI\n252:EL\n253:KR\n254:IR\n255:DVN\n256:DG\n257:MLM\n258:VMC\n259:CDW\n260:HSY\n261:KHC\n262:FANG\n263:EXR\n264:PWR\n265:CAH\n266:FICO\n267:GEHC\n268:SBAC\n269:EFX\n270:WEC\n271:MPWR\n272:WST\n273:WTW\n274:DLTR\n275:MRNA\n276:EIX\n277:AWK\n278:ANSS\n279:HPQ\n280:RCL\n281:XYL\n282:TTWO\n283:CBRE\n284:ZBH\n285:FTV\n286:KEYS\n287:AVB\n288:LYB\n289:HIG\n290:MTD\n291:DAL\n292:CHD\n293:APTV\n294:DFS\n295:STT\n296:WBD\n297:RMD\n298:WY\n299:BR\n300:TROW\n301:TSCO\n302:EBAY\n303:HPE\n304:GLW\n305:DTE\n306:ETR\n307:MTB\n308:ULTA\n309:MOH\n310:WAB\n311:ES\n312:HWM\n313:TRGP\n314:NVR\n315:AEE\n316:CTRA\n317:STE\n318:RJF\n319:DOV\n320:FITB\n321:EQR\n322:PHM\n323:NTAP\n324:LH\n325:IFF\n326:CBOE\n327:INVH\n328:PPL\n329:FE\n330:VRSN\n331:TDY\n332:DRI\n333:NDAQ\n334:EXPE\n335:PTC\n336:GRMN\n337:IRM\n338:GPC\n339:STLD\n340:VTR\n341:BAX\n342:CNP\n343:EXPD\n344:FLT\n345:CLX\n346:EG\n347:BRO\n348:AKAM\n349:HOLX\n350:BALL\n351:FDS\n352:TYL\n353:ARE\n354:LVS\n355:VLTO\n356:ATO\n357:FSLR\n358:COO\n359:WAT\n360:CMS\n361:BG\n362:PFG\n363:NTRS\n364:MKC\n365:AXON\n366:CINF\n367:HBAN\n368:ILMN\n369:HUBB\n370:J\n371:OMC\n372:AVY\n373:RF\n374:SWKS\n375:WDC\n376:MRO\n377:DGX\n378:ALGN\n379:STX\n380:LUV\n381:TXT\n382:PKG\n383:IEX\n384:JBHT\n385:CCL\n386:EPAM\n387:WRB\n388:LDOS\n389:SNA\n390:CF\n391:EQT\n392:MAA\n393:LW\n394:WBA\n395:ALB\n396:TER\n397:SWK\n398:AMCR\n399:K\n400:DPZ\n401:CE\n402:BBY\n403:ENPH\n404:ESS\n405:MAS\n406:POOL\n407:SYF\n408:CAG\n409:TSN\n410:PODD\n411:L\n412:UAL\n413:CFG\n414:IP\n415:LNT\n416:NDSN\n417:HST\n418:GEN\n419:ZBRA\n420:MOS\n421:LYV\n422:LKQ\n423:IPG\n424:KIM\n425:EVRG\n426:KEY\n427:JKHY\n428:TRMB\n429:AES\n430:TAP\n431:ROL\n432:SJM\n433:MGM\n434:APA\n435:RVTY\n436:VTRS\n437:NRG\n438:GL\n439:TFX\n440:BF.B\n441:PNR\n442:CDAY\n443:WRK\n444:NI\n445:REG\n446:FFIV\n447:UDR\n448:KMX\n449:INCY\n450:CRL\n451:EMN\n452:TECH\n453:CPT\n454:CHRW\n455:PEAK\n456:CZR\n457:QRVO\n458:HII\n459:AOS\n460:ETSY\n461:ALLE\n462:JNPR\n463:MTCH\n464:MKTX\n465:AIZ\n466:PAYC\n467:HRL\n468:RHI\n469:HSIC\n470:UHS\n471:PNW\n472:NWSA\n473:WYNN\n474:AAL\n475:BXP\n476:BWA\n477:CPB\n478:FOXA\n479:BBWI\n480:TPR\n481:GNRC\n482:BEN\n483:CTLT\n484:FRT\n485:PARA\n486:FMC\n487:XRAY\n488:IVZ\n489:BIO\n490:NCLH\n491:CMA\n492:WHR\n493:HAS\n494:VFC\n495:DVA\n496:ZION\n497:RL\n498:SEE\n499:ALK\n500:MHK\n501:SEDG\n502:FOX\n503:NWS\n</pre> In\u00a0[4]: Copied! <pre>data_filename = \"SP500_daily_data_1980_to_2023.csv.gz\"\nnotebooks_dir = Path(\"./../notebooks\")\ndata_file_path = notebooks_dir.parent / \"data\" / data_filename\nprint(data_file_path)\n</pre> data_filename = \"SP500_daily_data_1980_to_2023.csv.gz\" notebooks_dir = Path(\"./../notebooks\") data_file_path = notebooks_dir.parent / \"data\" / data_filename print(data_file_path) <pre>../data/SP500_daily_data_1980_to_2023.csv.gz\n</pre> In\u00a0[5]: Copied! <pre>%time\n# passing a function as argument to another function; load all data we got\ndf = process_data(load_data(file_path=data_file_path, compression=\"gzip\"))\n</pre> %time # passing a function as argument to another function; load all data we got df = process_data(load_data(file_path=data_file_path, compression=\"gzip\")) <pre>CPU times: user 1 \u00b5s, sys: 1 \u00b5s, total: 2 \u00b5s\nWall time: 4.53 \u00b5s\n</pre> In\u00a0[6]: Copied! <pre>df.shape\n</pre> df.shape Out[6]: <pre>(3169549, 13)</pre> In\u00a0[7]: Copied! <pre>df.info()\n</pre> df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3169549 entries, 0 to 3169548\nData columns (total 13 columns):\n #   Column                Dtype         \n---  ------                -----         \n 0   ticker                object        \n 1   date                  datetime64[ns]\n 2   prc                   float64       \n 3   vol                   int64         \n 4   close                 float64       \n 5   low                   float64       \n 6   high                  float64       \n 7   open                  float64       \n 8   price_vol             float64       \n 9   intraday_return       float64       \n 10  sign_intraday_return  int64         \n 11  next_intraday_return  float64       \n 12  sign_next_day_return  Int64         \ndtypes: Int64(1), datetime64[ns](1), float64(8), int64(2), object(1)\nmemory usage: 317.4+ MB\n</pre> In\u00a0[8]: Copied! <pre>df.head()\n</pre> df.head() Out[8]: ticker date prc vol close low high open price_vol intraday_return sign_intraday_return next_intraday_return sign_next_day_return 0 AMT 1992-06-15 7.375 26700 7.375 7.375 7.500 7.375 196912.5 0.000000 0 0.000000 0 1 AMT 1992-06-16 7.375 6300 7.375 7.375 7.500 7.375 46462.5 0.000000 0 -0.016949 -1 2 AMT 1992-06-17 7.250 16400 7.250 7.250 7.375 7.375 118900.0 -0.016949 -1 0.000000 0 3 AMT 1992-06-18 7.250 6900 7.250 7.250 7.375 7.250 50025.0 0.000000 0 0.016949 1 4 AMT 1992-06-19 7.500 13700 7.500 7.250 7.500 7.375 102750.0 0.016949 1 0.000000 0 In\u00a0[9]: Copied! <pre>df[\"ticker\"].unique().__len__()\n</pre> df[\"ticker\"].unique().__len__() Out[9]: <pre>501</pre> <p>We have data fo 501 stocks though in our ticker file we got 503 tickers. We obtained the tickers from a public source and their ticker formatting might be different from <code>CRSP</code>. This is not significant for our constex, however, it is worthwhile making sure the metadata of tradable securities is updated regularly.</p> In\u00a0[10]: Copied! <pre>%%time \n\ndays_per_ticker = {}\n\nfor ticker in tickers:\n    days_per_ticker[ticker] = df.query(\"ticker == @ticker\").shape[\n        0\n    ]  # takes about 16.8 seconds\n#     days_per_ticker[ticker] = df[df['ticker'] == ticker].shape[0]  # takes about 59.6 seconds and is three times slower, still.\n</pre> %%time   days_per_ticker = {}  for ticker in tickers:     days_per_ticker[ticker] = df.query(\"ticker == @ticker\").shape[         0     ]  # takes about 16.8 seconds #     days_per_ticker[ticker] = df[df['ticker'] == ticker].shape[0]  # takes about 59.6 seconds and is three times slower, still. <pre>CPU times: user 23 s, sys: 0 ns, total: 23 s\nWall time: 23 s\n</pre> <p>As you see this step took approximately 16.8 seconds on the machine this code was executed on. Making use of the <code>pandas</code> native <code>.goupby()</code> method, which is written in <code>C</code>, and storing the results in a dictionary, achieves the same task in just about 112 ms, i.e, the computation is 150 times, i.e. an order of magnitude faster.</p> In\u00a0[11]: Copied! <pre>%%time \ndays_per_ticker = df.groupby(\"ticker\").size().to_dict();\n</pre> %%time  days_per_ticker = df.groupby(\"ticker\").size().to_dict(); <pre>CPU times: user 115 ms, sys: 0 ns, total: 115 ms\nWall time: 115 ms\n</pre> In\u00a0[12]: Copied! <pre>plt.figure(figsize=(7, 7))\nplt.hist(list(days_per_ticker.values()), bins=30)\nplt.show()\n</pre> plt.figure(figsize=(7, 7)) plt.hist(list(days_per_ticker.values()), bins=30) plt.show() In\u00a0[13]: Copied! <pre># Counter objects are a part of the collections module in Python's standard library.\n# They are specialized dictionary subclasses designed to count hashable objects.\n# A Counter is a collection where elements are stored as dictionary keys and their counts are stored\n# as dictionary values.\n\nCounter(list(days_per_ticker.values())).most_common(3)[\n    0\n]  # Counter(list(days_per_ticker.values())).most_common(3)[0][0] then extracts the number that occurs most often.\n\n# 81 stocks contain 7881 days of data\n</pre> # Counter objects are a part of the collections module in Python's standard library. # They are specialized dictionary subclasses designed to count hashable objects. # A Counter is a collection where elements are stored as dictionary keys and their counts are stored # as dictionary values.  Counter(list(days_per_ticker.values())).most_common(3)[     0 ]  # Counter(list(days_per_ticker.values())).most_common(3)[0][0] then extracts the number that occurs most often.  # 81 stocks contain 7881 days of data Out[13]: <pre>(7944, 80)</pre> In\u00a0[14]: Copied! <pre>max_days_ticker = max(\n    days_per_ticker, key=days_per_ticker.get\n)  # find the ticker with the maximum number of days\nmax_days = days_per_ticker[\n    max_days_ticker\n]  # retrieve the value (number of days) for this ticker\nprint(\n    f\"The ticker with the maximum number of days is: {max_days_ticker}, with {max_days} days.\"\n)\n\nmax_days = max(days_per_ticker.values())  # find the maximum number of days\nweights_per_ticker = {\n    ticker: days / max_days for ticker, days in days_per_ticker.items()\n}  # Calculate the weight for each ticker, z-transform the weights should you wish to use them in ML applications\nweights_per_ticker\n</pre> max_days_ticker = max(     days_per_ticker, key=days_per_ticker.get )  # find the ticker with the maximum number of days max_days = days_per_ticker[     max_days_ticker ]  # retrieve the value (number of days) for this ticker print(     f\"The ticker with the maximum number of days is: {max_days_ticker}, with {max_days} days.\" )  max_days = max(days_per_ticker.values())  # find the maximum number of days weights_per_ticker = {     ticker: days / max_days for ticker, days in days_per_ticker.items() }  # Calculate the weight for each ticker, z-transform the weights should you wish to use them in ML applications weights_per_ticker <pre>The ticker with the maximum number of days is: LEN, with 13157 days.\n</pre> Out[14]: <pre>{'A': 0.5693547161206962,\n 'AAL': 0.2824352055939804,\n 'AAPL': 0.603253021205442,\n 'ABBV': 0.2103823059968078,\n 'ABNB': 0.05837196929391199,\n 'ABT': 0.6037850573839021,\n 'ACGL': 0.4512426845025462,\n 'ACN': 0.5907121684274531,\n 'ADBE': 0.6034810367104964,\n 'ADI': 0.6037850573839021,\n 'ADM': 0.6037850573839021,\n 'ADP': 0.5999087937979782,\n 'ADSK': 0.5292999923994831,\n 'AEE': 0.555749790985787,\n 'AEP': 0.6037850573839021,\n 'AES': 0.5204073877023637,\n 'AFL': 0.6037850573839021,\n 'AIG': 0.6037850573839021,\n 'AIZ': 0.5872159306832865,\n 'AJG': 0.6027969901953333,\n 'AKAM': 0.46218742874515467,\n 'ALB': 0.5702667781409135,\n 'ALGN': 0.4382458007144486,\n 'ALK': 0.6037850573839021,\n 'ALL': 0.5988447214410579,\n 'ALLE': 0.34057915938283806,\n 'AMAT': 0.6037090522155507,\n 'AMCR': 0.2427605077145246,\n 'AMD': 0.6036330470471992,\n 'AME': 0.6033290263737934,\n 'AMGN': 0.6036330470471992,\n 'AMP': 0.4795166071292848,\n 'AMT': 0.5665425248916927,\n 'AMZN': 0.5093106331230524,\n 'ANET': 0.29429201185680626,\n 'ANSS': 0.5261837804970738,\n 'AON': 0.26936231663753135,\n 'AOS': 0.5493653568442655,\n 'APA': 0.6037090522155507,\n 'APD': 0.6037850573839021,\n 'APH': 0.6034810367104964,\n 'APTV': 0.1912290035722429,\n 'ARE': 0.5086265866078893,\n 'ATO': 0.6036330470471992,\n 'AVB': 0.4891692635099187,\n 'AVGO': 0.3213498517899217,\n 'AVY': 0.6037090522155507,\n 'AWK': 0.5027741886448278,\n 'AXON': 0.1264726001368093,\n 'AXP': 0.6037090522155507,\n 'AZO': 0.6037850573839021,\n 'BA': 0.6037850573839021,\n 'BAC': 0.6037850573839021,\n 'BALL': 0.03139013452914798,\n 'BAX': 0.6037850573839021,\n 'BBWI': 0.04613513718932887,\n 'BBY': 0.6038610625522536,\n 'BDX': 0.6037850573839021,\n 'BEN': 0.6040890780573079,\n 'BG': 0.5619822147906057,\n 'BIIB': 0.38458615185832634,\n 'BIO': 0.7769248308885004,\n 'BK': 0.6038610625522536,\n 'BKNG': 0.30584479744622634,\n 'BKR': 0.487193129132781,\n 'BLK': 0.5967165767272175,\n 'BMY': 0.6037850573839021,\n 'BR': 0.5843277342859314,\n 'BRO': 0.4841529223987231,\n 'BSX': 0.6037850573839021,\n 'BWA': 0.5812875275518735,\n 'BX': 0.31678954168883483,\n 'BXP': 0.5078665349243748,\n 'C': 0.6027209850269818,\n 'CAG': 0.6037850573839021,\n 'CAH': 0.560842137265334,\n 'CARR': 0.07159686858706392,\n 'CAT': 0.6037850573839021,\n 'CB': 0.6036330470471992,\n 'CBOE': 0.2591776240784373,\n 'CBRE': 0.11066352511970814,\n 'CCI': 0.5576499201945733,\n 'CCL': 0.6037090522155507,\n 'CDAY': 0.1086873907425705,\n 'CDNS': 0.3474956297028198,\n 'CDW': 0.20110967545793115,\n 'CE': 0.5166071292847914,\n 'CEG': 0.2825112107623318,\n 'CF': 0.5511894808847002,\n 'CFG': 0.17732005776392795,\n 'CHD': 0.6037090522155507,\n 'CHRW': 0.501102074941096,\n 'CHTR': 0.4561830204453903,\n 'CI': 0.6037090522155507,\n 'CINF': 0.6035570418788477,\n 'CL': 0.6037850573839021,\n 'CLX': 0.6037850573839021,\n 'CMA': 0.6037090522155507,\n 'CMCSA': 0.6016569126700616,\n 'CME': 0.5359124420460591,\n 'CMG': 0.40404347495629706,\n 'CMI': 0.5437409743862582,\n 'CMS': 0.6037850573839021,\n 'CNC': 0.5809835068784678,\n 'CNP': 0.7176407995743711,\n 'COF': 0.5570418788477617,\n 'COO': 0.6033290263737934,\n 'COP': 0.5590940183932508,\n 'COR': 0.4849129740822376,\n 'COST': 0.5403967469787946,\n 'CPB': 0.6038610625522536,\n 'CPRT': 0.568974690278939,\n 'CPT': 0.5825036102454967,\n 'CRL': 0.5372805350763852,\n 'CRM': 0.5099186744698639,\n 'CSCO': 0.6034810367104964,\n 'CSGP': 0.466823744014593,\n 'CSX': 0.6037850573839021,\n 'CTAS': 0.6037090522155507,\n 'CTLT': 0.18020825416128297,\n 'CTRA': 0.27399863190696966,\n 'CTSH': 0.4882572014897013,\n 'CTVA': 0.08770996427757087,\n 'CVS': 0.5210154290491753,\n 'CVX': 0.42517291175799954,\n 'CZR': 0.2549973398191077,\n 'D': 0.6037850573839021,\n 'DAL': 0.5741430417268374,\n 'DD': 0.5704187884776165,\n 'DE': 0.6037850573839021,\n 'DFS': 0.5542296876187581,\n 'DG': 0.5080185452610777,\n 'DGX': 0.515695067264574,\n 'DHI': 0.5365204833928707,\n 'DHR': 0.6037850573839021,\n 'DIS': 0.6037850573839021,\n 'DLR': 0.36672493729573613,\n 'DLTR': 0.5512654860530516,\n 'DOV': 0.6037850573839021,\n 'DOW': 0.5736870107167288,\n 'DPZ': 0.3725773352587976,\n 'DRI': 0.547085201793722,\n 'DTE': 0.6037850573839021,\n 'DUK': 0.6037090522155507,\n 'DVA': 0.44409819867751005,\n 'DVN': 0.6037850573839021,\n 'DXCM': 0.3579843429353196,\n 'EA': 0.3443794178004104,\n 'EBAY': 0.5080945504294292,\n 'ECL': 0.6037090522155507,\n 'ED': 0.6037850573839021,\n 'EFX': 0.6037850573839021,\n 'EG': 0.009272630538876643,\n 'EIX': 0.53386030250057,\n 'EL': 0.5378125712548454,\n 'ELV': 0.21631070912822073,\n 'EMN': 0.573915026221783,\n 'EMR': 0.6037850573839021,\n 'ENPH': 0.22474728281523143,\n 'EOG': 0.6037090522155507,\n 'EPAM': 0.22748346887588355,\n 'EQIX': 0.44706240024321653,\n 'EQR': 0.5814395378885764,\n 'EQT': 0.6037850573839021,\n 'ES': 0.2752907197689443,\n 'ESS': 0.5657064680398267,\n 'ETN': 0.6037850573839021,\n 'ETR': 0.6037090522155507,\n 'ETSY': 0.16667933419472525,\n 'EVRG': 0.13544121000228015,\n 'EW': 0.45405487573154973,\n 'EXC': 0.5735350003800258,\n 'EXPD': 0.6027969901953333,\n 'EXPE': 0.4234247928859162,\n 'EXR': 0.3709052215550657,\n 'F': 0.6041650832256593,\n 'FANG': 0.2148666109295432,\n 'FAST': 0.6036330470471992,\n 'FCX': 0.733753895264878,\n 'FDS': 0.5253477236452079,\n 'FDX': 0.6037850573839021,\n 'FE': 0.4998859922474728,\n 'FFIV': 0.4698639507486509,\n 'FI': 0.27460667325378124,\n 'FICO': 0.27491069392718703,\n 'FIS': 0.3427073040966786,\n 'FITB': 0.6034810367104964,\n 'FLT': 0.48042866914950216,\n 'FMC': 0.6037090522155507,\n 'FOX': 0.4114159762863875,\n 'FOXA': 0.20095766512122826,\n 'FRT': 0.6038610625522536,\n 'FSLR': 0.3273542600896861,\n 'FTNT': 0.26997035798434293,\n 'FTV': 0.14334574751083073,\n 'GD': 0.6038610625522536,\n 'GE': 0.6037090522155507,\n 'GEHC': 0.018925286919510526,\n 'GEN': 0.35775632743026525,\n 'GILD': 0.603253021205442,\n 'GIS': 0.6037850573839021,\n 'GL': 0.217374781485141,\n 'GLW': 0.6038610625522536,\n 'GM': 0.5752831192521092,\n 'GNRC': 0.2656380633883104,\n 'GOOG': 0.37037318537660563,\n 'GOOGL': 0.1864406779661017,\n 'GPC': 0.6037850573839021,\n 'GPN': 0.4380937903777457,\n 'GRMN': 0.48962529452002734,\n 'GS': 0.47168807478908564,\n 'GWW': 0.6037850573839021,\n 'HAL': 0.6037090522155507,\n 'HAS': 0.6037850573839021,\n 'HBAN': 0.6035570418788477,\n 'HCA': 0.4008512578855362,\n 'HD': 0.603253021205442,\n 'HES': 0.3376149578171316,\n 'HIG': 0.5362164627194649,\n 'HII': 0.3636087253933267,\n 'HLT': 0.48658508778596943,\n 'HOLX': 0.6035570418788477,\n 'HON': 0.6036330470471992,\n 'HPE': 0.15611461579387398,\n 'HPQ': 0.41438017785209397,\n 'HRL': 0.6037090522155507,\n 'HSIC': 0.5384966177700083,\n 'HST': 0.4100478832560614,\n 'HSY': 0.6037850573839021,\n 'HUBB': 0.15330242456487042,\n 'HUM': 0.6037850573839021,\n 'HWM': 0.12077221251045071,\n 'IBM': 0.6034050315421449,\n 'ICE': 0.35243596564566393,\n 'IDXX': 0.6037090522155507,\n 'IEX': 0.603253021205442,\n 'IFF': 0.6037090522155507,\n 'ILMN': 0.44782245192673104,\n 'INCY': 0.5349243748574903,\n 'INTC': 0.6034810367104964,\n 'INTU': 0.589344075397127,\n 'INVH': 0.13224899293151934,\n 'IP': 0.6036330470471992,\n 'IPG': 0.6037850573839021,\n 'IQV': 0.11704795926122977,\n 'IR': 0.6035570418788477,\n 'IRM': 0.47214410579919436,\n 'ISRG': 0.4503306224823288,\n 'IT': 0.6427757087481949,\n 'ITW': 0.6037850573839021,\n 'IVZ': 0.3177016037090522,\n 'J': 0.26700615641863645,\n 'JBHT': 0.6037090522155507,\n 'JCI': 0.6037850573839021,\n 'JKHY': 0.6037090522155507,\n 'JNJ': 0.6037850573839021,\n 'JNPR': 0.4687238732233792,\n 'JPM': 0.6037850573839021,\n 'K': 0.6037850573839021,\n 'KDP': 0.10481112715664666,\n 'KEY': 0.41460819335714827,\n 'KEYS': 0.39097058599984796,\n 'KHC': 0.1624990499353956,\n 'KIM': 0.6038610625522536,\n 'KLAC': 0.6034810367104964,\n 'KMB': 0.6037090522155507,\n 'KMI': 0.39233867903017405,\n 'KMX': 0.5146309949076537,\n 'KO': 0.6037850573839021,\n 'KR': 0.6037850573839021,\n 'KVUE': 0.012616857946340352,\n 'L': 0.5079425400927263,\n 'LDOS': 0.19624534468343846,\n 'LEN': 1.0,\n 'LH': 0.5485292999923995,\n 'LHX': 0.08618986091054191,\n 'LIN': 0.3017405183552482,\n 'LKQ': 0.2220110967545793,\n 'LLY': 0.6037090522155507,\n 'LMT': 0.5511134757163487,\n 'LNT': 0.491601428897165,\n 'LOW': 0.6035570418788477,\n 'LRCX': 0.6036330470471992,\n 'LULU': 0.31428137113323706,\n 'LUV': 0.6033290263737934,\n 'LVS': 0.3642927719084898,\n 'LW': 0.136429277190849,\n 'LYB': 0.2556053811659193,\n 'LYV': 0.3446834384738162,\n 'MA': 0.41726837424944896,\n 'MAA': 0.5723949228547541,\n 'MAR': 0.5887360340503154,\n 'MAS': 0.6037850573839021,\n 'MCD': 0.6037850573839021,\n 'MCHP': 0.5887360340503154,\n 'MCK': 0.6021889488485217,\n 'MCO': 0.4636315269438322,\n 'MDLZ': 0.2150186212662461,\n 'MDT': 0.6037090522155507,\n 'MET': 0.4840009120620202,\n 'META': 0.2255073344987459,\n 'MGM': 0.4985178992171468,\n 'MHK': 0.49798586303868664,\n 'MKC': 0.8810519115299841,\n 'MKTX': 0.36634491145397885,\n 'MLM': 0.5704947936459679,\n 'MMC': 0.6037850573839021,\n 'MMM': 0.6037090522155507,\n 'MNST': 0.334802766588128,\n 'MO': 0.6035570418788477,\n 'MOH': 0.39218666869347113,\n 'MOS': 0.37174127840693166,\n 'MPC': 0.23903625446530363,\n 'MPWR': 0.40753971270046363,\n 'MRK': 0.6035570418788477,\n 'MRNA': 0.16667933419472525,\n 'MRO': 0.6036330470471992,\n 'MS': 0.583567682602417,\n 'MSCI': 0.25811355172151706,\n 'MSFT': 0.6034810367104964,\n 'MSI': 0.5175951964733602,\n 'MTB': 0.5007220490993387,\n 'MTCH': 0.2575055103747055,\n 'MTD': 0.49958197157406703,\n 'MU': 0.6037090522155507,\n 'NCLH': 0.20947024397659042,\n 'NDAQ': 0.3613285703427833,\n 'NDSN': 0.6037090522155507,\n 'NEE': 0.25872159306832865,\n 'NEM': 0.6037850573839021,\n 'NFLX': 0.41324010032682224,\n 'NI': 0.6037850573839021,\n 'NKE': 0.6037850573839021,\n 'NOC': 0.6037850573839021,\n 'NOW': 0.36345671505662386,\n 'NRG': 0.4765524055635783,\n 'NSC': 0.6037850573839021,\n 'NTAP': 0.5372805350763852,\n 'NTRS': 0.6036330470471992,\n 'NUE': 0.6049251349091739,\n 'NVDA': 0.4768564262369841,\n 'NVR': 0.6002128144713841,\n 'NWS': 0.681994375617542,\n 'NWSA': 0.2870715208634187,\n 'NXPI': 0.25636543284943375,\n 'O': 0.5998327886296269,\n 'ODFL': 0.5733829900433229,\n 'OKE': 0.6037850573839021,\n 'OMC': 0.6037850573839021,\n 'ON': 0.16728737554153683,\n 'ORCL': 0.6036330470471992,\n 'ORLY': 0.5869119100098806,\n 'OTIS': 0.07159686858706392,\n 'OXY': 0.6037850573839021,\n 'PANW': 0.21889488485216996,\n 'PARA': 0.10899141141597629,\n 'PAYC': 0.26746218742874517,\n 'PAYX': 0.6035570418788477,\n 'PCAR': 0.6038610625522536,\n 'PCG': 0.6037850573839021,\n 'PEAK': 0.3123052367560994,\n 'PEG': 0.6037850573839021,\n 'PEP': 0.6037850573839021,\n 'PFE': 0.6034050315421449,\n 'PFG': 0.5600820855818196,\n 'PG': 0.6037850573839021,\n 'PGR': 0.603937067720605,\n 'PH': 0.6037090522155507,\n 'PHM': 0.6037850573839021,\n 'PKG': 0.4574751083073649,\n 'PLD': 0.48772516531124116,\n 'PM': 0.4174963897545033,\n 'PNC': 0.603253021205442,\n 'PNR': 0.532492209470244,\n 'PNW': 0.6037850573839021,\n 'PODD': 0.31823363988751235,\n 'POOL': 0.5369765144029794,\n 'PPG': 0.6037850573839021,\n 'PPL': 0.6037850573839021,\n 'PRU': 0.44554229687618757,\n 'PSA': 0.599528767956221,\n 'PSX': 0.3340427149046135,\n 'PTC': 0.5418408451774721,\n 'PWR': 0.5365204833928707,\n 'PXD': 0.5050543436953713,\n 'PYPL': 0.17389982518811278,\n 'QCOM': 0.6035570418788477,\n 'QRVO': 0.17207570114767803,\n 'RCL': 0.5869879151782321,\n 'REG': 0.5765752071140837,\n 'REGN': 0.6037090522155507,\n 'RF': 0.47450026601808926,\n 'RHI': 0.6021129436801702,\n 'RJF': 0.6037850573839021,\n 'RL': 0.5327202249752984,\n 'RMD': 0.46378353728053506,\n 'ROK': 0.6034050315421449,\n 'ROL': 0.6044691038990652,\n 'ROP': 0.5194953256821464,\n 'ROST': 0.6037850573839021,\n 'RSG': 0.48772516531124116,\n 'RTX': 0.12898077069240707,\n 'RVTY': 0.012008816599528767,\n 'SBAC': 0.4693319145701908,\n 'SBUX': 0.6030250057003876,\n 'SCHW': 0.3449114539788706,\n 'SEDG': 0.16774340655164552,\n 'SEE': 0.6034050315421449,\n 'SHW': 0.6036330470471992,\n 'SJM': 0.7606597248612905,\n 'SLB': 0.6037850573839021,\n 'SNA': 0.6037850573839021,\n 'SNPS': 0.6036330470471992,\n 'SO': 0.6037850573839021,\n 'SPG': 0.5836436877707684,\n 'SPGI': 0.14684198525499734,\n 'SRE': 0.49828988371209243,\n 'STE': 0.5289959717260774,\n 'STLD': 0.5189632895036862,\n 'STT': 0.5790833776696815,\n 'STX': 0.4832408603785057,\n 'STZ': 0.8055027741886448,\n 'SWK': 0.6037850573839021,\n 'SWKS': 0.4114919814547389,\n 'SYF': 0.18020825416128297,\n 'SYK': 0.5056623850421829,\n 'SYY': 0.6034810367104964,\n 'T': 0.6031770160370905,\n 'TAP': 0.6661092954320894,\n 'TDG': 0.34050315421448657,\n 'TDY': 0.5410047883256062,\n 'TECH': 0.6037090522155507,\n 'TEL': 0.43376149578171314,\n 'TER': 0.6035570418788477,\n 'TFC': 0.3738694231207722,\n 'TFX': 0.6037090522155507,\n 'TGT': 0.5111347571634871,\n 'TJX': 0.6037090522155507,\n 'TMO': 0.6037090522155507,\n 'TMUS': 0.2042258873603405,\n 'TPR': 0.2955080945504294,\n 'TRGP': 0.3090370145169872,\n 'TRMB': 0.6036330470471992,\n 'TROW': 0.6034810367104964,\n 'TRV': 0.4701679714220567,\n 'TSCO': 0.5679106179220187,\n 'TSLA': 0.2583415672265714,\n 'TSN': 0.5011780801094474,\n 'TT': 0.3194497225811355,\n 'TTWO': 0.5093106331230524,\n 'TXN': 0.6036330470471992,\n 'TXT': 0.6037850573839021,\n 'TYL': 0.6024929695219275,\n 'UAL': 0.460211294368017,\n 'UDR': 0.6038610625522536,\n 'UHS': 0.6037090522155507,\n 'ULTA': 0.30956905069544727,\n 'UNH': 0.603253021205442,\n 'UNP': 0.6037850573839021,\n 'UPS': 0.4615793873983431,\n 'URI': 0.4978338527019837,\n 'USB': 0.4499505966405716,\n 'V': 0.510602720985027,\n 'VFC': 0.6037850573839021,\n 'VICI': 0.11309569050695448,\n 'VLO': 0.603253021205442,\n 'VLTO': 0.0047883256061412175,\n 'VMC': 0.6037850573839021,\n 'VRSK': 0.2722505130348864,\n 'VRSN': 0.4954016873147374,\n 'VRTX': 0.6034050315421449,\n 'VTR': 0.49106939271870487,\n 'VTRS': 0.059588051987535154,\n 'VZ': 0.4492665501254085,\n 'WAB': 0.5809075017101163,\n 'WAT': 0.5378125712548454,\n 'WBA': 0.1721517063160295,\n 'WBD': 0.21099034734361938,\n 'WDC': 0.6035570418788477,\n 'WEC': 0.6031010108687391,\n 'WELL': 0.09721061032150186,\n 'WFC': 0.6037850573839021,\n 'WHR': 0.6037850573839021,\n 'WM': 0.4629474804286691,\n 'WMB': 0.6037850573839021,\n 'WMT': 0.6036330470471992,\n 'WRB': 0.30158850801854525,\n 'WRK': 0.16257505510374706,\n 'WST': 0.6022649540168732,\n 'WTW': 0.4052595576499202,\n 'WY': 0.6038610625522536,\n 'WYNN': 0.40518355248156873,\n 'XEL': 0.5454130880899901,\n 'XOM': 0.4605153150414228,\n 'XRAY': 0.6036330470471992,\n 'XYL': 0.23257581515543058,\n 'YUM': 0.5017101162879076,\n 'ZBH': 0.1628030706088014,\n 'ZBRA': 0.6037090522155507,\n 'ZION': 0.603253021205442,\n 'ZTS': 0.20878619746142738}</pre> In\u00a0[15]: Copied! <pre>selected_start_date = pd.Timestamp(2012, 1, 1)\nselected_end_date = pd.Timestamp(2022, 12, 31)\ndf_filtered = df[\n    (df[\"date\"] &gt;= selected_start_date) &amp; (df[\"date\"] &lt;= selected_end_date)\n]\ndf_filtered.groupby(\n    \"ticker\"\n).size().value_counts()  # 374 out of the 500 stocks are of the desired duration\n</pre> selected_start_date = pd.Timestamp(2012, 1, 1) selected_end_date = pd.Timestamp(2022, 12, 31) df_filtered = df[     (df[\"date\"] &gt;= selected_start_date) &amp; (df[\"date\"] &lt;= selected_end_date) ] df_filtered.groupby(     \"ticker\" ).size().value_counts()  # 374 out of the 500 stocks are of the desired duration Out[15]: <pre>2768    374\n2769     14\n2767      4\n2770      3\n2771      3\n       ... \n2707      1\n130       1\n2772      1\n2372      1\n2497      1\nName: count, Length: 96, dtype: int64</pre> <p>If we wanted to just go for the mode directly, we could have achieved this by</p> In\u00a0[16]: Copied! <pre>mode_size = df_filtered.groupby(\"ticker\").size().mode()[0]\nmode_size\n</pre> mode_size = df_filtered.groupby(\"ticker\").size().mode()[0] mode_size Out[16]: <pre>2768</pre> <p>And counted the number of stocks of that length using</p> In\u00a0[17]: Copied! <pre>df_common_size = (\n    df_filtered.groupby(\"ticker\")\n    .filter(lambda x: len(x) == mode_size)\n    .reset_index(drop=True)\n)\n\ndf_common_size[\"ticker\"].unique().__len__()\n</pre> df_common_size = (     df_filtered.groupby(\"ticker\")     .filter(lambda x: len(x) == mode_size)     .reset_index(drop=True) )  df_common_size[\"ticker\"].unique().__len__() Out[17]: <pre>374</pre> <p>If we selected only those stocks that have an equal amount of days between our start and end day, we have to reduce our universe from 500 stocks to 374. This is a significant reduction that one should be sure to afford.</p> <p>As an alternative way, we appreaciate the different length of the data and conduct the pattern analysis for each of them separately.</p> <p>We limit ourselves to onem year of data to see how each of the stocks in the portfolio performed relative to their starting price</p> In\u00a0[18]: Copied! <pre>def add_normalized_price(df: pd.DataFrame) -&gt; pd.DataFrame:\n    df[\"first_price_indicator\"] = np.where(df.index == 0, 1, 0)\n    df[\"first_price_value\"] = df[\"first_price_indicator\"] * df[\"close\"]\n    df[\"first_price_value\"].replace(to_replace=0, method=\"ffill\", inplace=True)\n    df[\"normalized_price\"] = df[\"close\"] / df[\"first_price_value\"]\n    df.drop(columns=[\"first_price_indicator\", \"first_price_value\"], inplace=True)\n    return df\n</pre> def add_normalized_price(df: pd.DataFrame) -&gt; pd.DataFrame:     df[\"first_price_indicator\"] = np.where(df.index == 0, 1, 0)     df[\"first_price_value\"] = df[\"first_price_indicator\"] * df[\"close\"]     df[\"first_price_value\"].replace(to_replace=0, method=\"ffill\", inplace=True)     df[\"normalized_price\"] = df[\"close\"] / df[\"first_price_value\"]     df.drop(columns=[\"first_price_indicator\", \"first_price_value\"], inplace=True)     return df In\u00a0[19]: Copied! <pre># implementation using a multi-indexed data frame\nresult = df_common_size.set_index([\"ticker\", \"date\"]).join(\n    df_common_size.groupby(\"ticker\").first().add_prefix(\"first_\")\n)  # dg.set_index(['Date','ListingId']) will be equivalent to the vectorized version\nresult[\"normalized_price\"] = result[\"close\"] / result[\"first_close\"]\n</pre> # implementation using a multi-indexed data frame result = df_common_size.set_index([\"ticker\", \"date\"]).join(     df_common_size.groupby(\"ticker\").first().add_prefix(\"first_\") )  # dg.set_index(['Date','ListingId']) will be equivalent to the vectorized version result[\"normalized_price\"] = result[\"close\"] / result[\"first_close\"] In\u00a0[20]: Copied! <pre># plotting the data. Note you can select a nunber of stocks via the variable \"counter\" as well.\n\nselected_start_date = pd.Timestamp(2022, 1, 1)\nselected_end_date = pd.Timestamp(2022, 12, 31)\n\ndf_filtered = df[\n    (df[\"date\"] &gt;= selected_start_date) &amp; (df[\"date\"] &lt;= selected_end_date)\n]\ndf_filtered\n\nmode_size = df_filtered.groupby(\"ticker\").size().mode()[0]\n\ndf_common_size = (\n    df_filtered.groupby(\"ticker\")\n    .filter(lambda x: len(x) == mode_size)\n    .reset_index(drop=True)\n)\n\nresult = df_common_size.set_index([\"ticker\", \"date\"]).join(\n    df_common_size.groupby(\"ticker\").first().add_prefix(\"first_\")\n)  # dg.set_index(['Date','ListingId']) will be equivalent to the vectorized version\nresult[\"normalized_price\"] = result[\"close\"] / result[\"first_close\"]\n\nplt.figure(figsize=(10, 6))\n\ncounter = 0\n\nfor ticker, data in result.groupby(level=\"ticker\"):\n    plt.plot(\n        data.index.get_level_values(\"date\"), data[\"normalized_price\"], label=ticker\n    )\n#     counter += 1\n#     if counter == 20:\n#         break\n\n# plt.legend(title='Ticker', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.title(\"Normalised price by date for the selected stocks\")\nplt.xlabel(\"date\")\nplt.ylabel(\"normalized price\")\nplt.tight_layout()\nplt.show()\n</pre> # plotting the data. Note you can select a nunber of stocks via the variable \"counter\" as well.  selected_start_date = pd.Timestamp(2022, 1, 1) selected_end_date = pd.Timestamp(2022, 12, 31)  df_filtered = df[     (df[\"date\"] &gt;= selected_start_date) &amp; (df[\"date\"] &lt;= selected_end_date) ] df_filtered  mode_size = df_filtered.groupby(\"ticker\").size().mode()[0]  df_common_size = (     df_filtered.groupby(\"ticker\")     .filter(lambda x: len(x) == mode_size)     .reset_index(drop=True) )  result = df_common_size.set_index([\"ticker\", \"date\"]).join(     df_common_size.groupby(\"ticker\").first().add_prefix(\"first_\") )  # dg.set_index(['Date','ListingId']) will be equivalent to the vectorized version result[\"normalized_price\"] = result[\"close\"] / result[\"first_close\"]  plt.figure(figsize=(10, 6))  counter = 0  for ticker, data in result.groupby(level=\"ticker\"):     plt.plot(         data.index.get_level_values(\"date\"), data[\"normalized_price\"], label=ticker     ) #     counter += 1 #     if counter == 20: #         break  # plt.legend(title='Ticker', bbox_to_anchor=(1.05, 1), loc='upper left') plt.title(\"Normalised price by date for the selected stocks\") plt.xlabel(\"date\") plt.ylabel(\"normalized price\") plt.tight_layout() plt.show() <p>We see that the stock prices are not adapted to stock splits. As we work with intraday returns, we get away dealing with price-adjustments which, strictly speaking, is a topic in its own right. However, we do see the investment universe evolved, seemingly randomly, and there were constituents that performed positively, neutral, and negatively.</p> <p>Hence, selecting one stock in hindsight, and evaluating its buy-and-hold performance, is subject to bias. We will test in the following, how the candlestick patterns perform for the investment universe, to see whether they allow a a portfolion to be actively managed in a long-short fashion.</p> In\u00a0[21]: Copied! <pre># the path to the data file is the same as for the notbook discussing the single-stock case.\ndata_filename = \"SP500_daily_data_1980_to_2023.csv.gz\"\nnotebooks_dir = Path(\"./../notebooks\")\ndata_file_path = notebooks_dir.parent / \"data\" / data_filename\n\ndf = process_data(\n    load_data(\n        data_file_path,\n        selected_start_date=pd.Timestamp(2019, 1, 1),\n        selected_end_date=pd.Timestamp(2022, 12, 31),\n    )\n)\n</pre> # the path to the data file is the same as for the notbook discussing the single-stock case. data_filename = \"SP500_daily_data_1980_to_2023.csv.gz\" notebooks_dir = Path(\"./../notebooks\") data_file_path = notebooks_dir.parent / \"data\" / data_filename  df = process_data(     load_data(         data_file_path,         selected_start_date=pd.Timestamp(2019, 1, 1),         selected_end_date=pd.Timestamp(2022, 12, 31),     ) ) In\u00a0[22]: Copied! <pre>%%time\ncs_signals_df = cs_pattern_recognition(df=df)\n</pre> %%time cs_signals_df = cs_pattern_recognition(df=df) <pre>CPU times: user 4.65 s, sys: 5.84 s, total: 10.5 s\nWall time: 10.5 s\n</pre> In\u00a0[23]: Copied! <pre>%%time\nperformance_metrics = cs_performance(cs_signals_df)\n</pre> %%time performance_metrics = cs_performance(cs_signals_df) <pre>CPU times: user 87.1 ms, sys: 0 ns, total: 87.1 ms\nWall time: 86.9 ms\n</pre> In\u00a0[24]: Copied! <pre># plot all patterns, ranked by number of instances\nplot_cs_performance(\n    df=performance_metrics,\n    criterion=\"total_instances\",\n    title_suffix=\"across the whole data set.\",\n)\n\n# plot the patterns, ranked by number of instances, with a true-positive rate &gt;50%.\nplot_cs_performance(\n    df=performance_metrics.query(\"ci_lower &gt; 0.5\").sort_values(\n        by=\"total_instances\", ascending=False\n    ),\n    criterion=\"total_instances\",\n    title_suffix=\"with ci_lower &gt; 50%.\",\n)\n</pre> # plot all patterns, ranked by number of instances plot_cs_performance(     df=performance_metrics,     criterion=\"total_instances\",     title_suffix=\"across the whole data set.\", )  # plot the patterns, ranked by number of instances, with a true-positive rate &gt;50%. plot_cs_performance(     df=performance_metrics.query(\"ci_lower &gt; 0.5\").sort_values(         by=\"total_instances\", ascending=False     ),     criterion=\"total_instances\",     title_suffix=\"with ci_lower &gt; 50%.\", ) <p>Notable this time, appreciating more data, we indeed isntances where the lower part of the confidence interval is greater than 50%. This tells us we are 95% sure that these patterns indeed correctly predict the next day's intraday return. Conversely, we are now also in a position to identify counter-signals. These are instances where the upper part of the confidence interval is below the 50% threshold. Hence, it is indicated to take action for these signals in opposite direction of what they suggest, i.e. to take them as contrarian signals.</p> <p>Let us visualise these reults in the following.</p> In\u00a0[25]: Copied! <pre>performance_metrics.query(\"ci_lower &gt; 0.5\").sort_values(\n    by=[\"ci_lower\"], ascending=False\n)\n</pre> performance_metrics.query(\"ci_lower &gt; 0.5\").sort_values(     by=[\"ci_lower\"], ascending=False ) Out[25]: TP FP total_instances precision center margin ci_upper ci_lower TP_wilson candle CDL3LINESTRIKE 254 190 444 0.572072 0.571454 0.045829 0.617282 0.525625 0.571454 CDLINVERTEDHAMMER 3030 2742 5772 0.524948 0.524931 0.012879 0.537810 0.512053 0.524931 CDLGRAVESTONEDOJI 2958 2733 5691 0.519768 0.519755 0.012976 0.532731 0.506779 0.519755 CDLUNIQUE3RIVER 409 348 757 0.540291 0.540087 0.035413 0.575500 0.504674 0.540087 CDLCOUNTERATTACK 342 287 629 0.543720 0.543455 0.038807 0.582262 0.504647 0.543455 CDLLONGLEGGEDDOJI 33980 32928 66908 0.507862 0.507861 0.003788 0.511649 0.504073 0.507861 CDLSEPARATINGLINES 1184 1076 2260 0.523894 0.523853 0.020573 0.544426 0.503280 0.523853 CDLHAMMER 5576 5303 10879 0.512547 0.512543 0.009391 0.521934 0.503152 0.512543 CDLRICKSHAWMAN 25188 24453 49641 0.507403 0.507403 0.004398 0.511800 0.503005 0.507403 CDLMATCHINGLOW 3900 3691 7591 0.513766 0.513759 0.011241 0.525000 0.502519 0.513759 CDLDOJI 34238 33455 67693 0.505783 0.505783 0.003766 0.509549 0.502017 0.505783 CDLSTICKSANDWICH 408 352 760 0.536842 0.536657 0.035362 0.572019 0.501295 0.536657 In\u00a0[26]: Copied! <pre>performance_metrics.query(\"ci_upper &lt; 0.5\").sort_values(\n    by=[\"ci_upper\"], ascending=False\n)\n</pre> performance_metrics.query(\"ci_upper &lt; 0.5\").sort_values(     by=[\"ci_upper\"], ascending=False ) Out[26]: TP FP total_instances precision center margin ci_upper ci_lower TP_wilson candle CDLSPINNINGTOP 53080 53849 106929 0.496404 0.496404 0.002997 0.499401 0.493408 0.496404 CDLMARUBOZU 11173 11518 22691 0.492398 0.492399 0.006504 0.498904 0.485895 0.492399 CDLBELTHOLD 36943 37701 74644 0.494923 0.494923 0.003587 0.498509 0.491336 0.494923 CDLSHOOTINGSTAR 3697 3929 7626 0.484789 0.484797 0.011214 0.496011 0.473583 0.484797 CDLLONGLINE 48672 50428 99100 0.491140 0.491141 0.003112 0.494253 0.488028 0.491141 CDLSHORTLINE 28193 29360 57553 0.489862 0.489862 0.004084 0.493946 0.485778 0.489862 CDLTRISTAR 320 383 703 0.455192 0.455436 0.036713 0.492148 0.418723 0.455436 CDLCLOSINGMARUBOZU 35159 37383 72542 0.484671 0.484672 0.003637 0.488308 0.481035 0.484672 CDLDARKCLOUDCOVER 1093 1259 2352 0.464711 0.464768 0.020140 0.484909 0.444628 0.464768 CDLPIERCING 862 1008 1870 0.460963 0.461043 0.022570 0.483612 0.438473 0.461043 CDL2CROWS 234 299 533 0.439024 0.439461 0.041982 0.481443 0.397479 0.439461 CDLENGULFING 17525 19420 36945 0.474354 0.474356 0.005091 0.479448 0.469265 0.474356 CDLEVENINGDOJISTAR 444 559 1003 0.442672 0.442891 0.030681 0.473572 0.412209 0.442891 CDL3OUTSIDE 8330 9687 18017 0.462341 0.462349 0.007279 0.469629 0.455070 0.462349 CDLEVENINGSTAR 1317 1601 2918 0.451337 0.451401 0.018044 0.469444 0.433357 0.451401 In\u00a0[27]: Copied! <pre>plot_cs_performance(\n    df=performance_metrics, criterion=\"TP_wilson\", plot_performance=True\n)\n</pre> plot_cs_performance(     df=performance_metrics, criterion=\"TP_wilson\", plot_performance=True ) <p>Based on this analysis, we now name signals and contrarian signals:</p> In\u00a0[28]: Copied! <pre>performance_metrics.query(\"ci_lower &gt; 0.5\").index  # signals\n</pre> performance_metrics.query(\"ci_lower &gt; 0.5\").index  # signals Out[28]: <pre>Index(['CDLDOJI', 'CDLLONGLEGGEDDOJI', 'CDLRICKSHAWMAN', 'CDLHAMMER',\n       'CDLMATCHINGLOW', 'CDLINVERTEDHAMMER', 'CDLGRAVESTONEDOJI',\n       'CDLSEPARATINGLINES', 'CDLSTICKSANDWICH', 'CDLUNIQUE3RIVER',\n       'CDLCOUNTERATTACK', 'CDL3LINESTRIKE'],\n      dtype='object', name='candle')</pre> In\u00a0[29]: Copied! <pre>performance_metrics.query(\"ci_upper &lt; 0.5\").index  # anti signals\n</pre> performance_metrics.query(\"ci_upper &lt; 0.5\").index  # anti signals Out[29]: <pre>Index(['CDLSPINNINGTOP', 'CDLLONGLINE', 'CDLBELTHOLD', 'CDLCLOSINGMARUBOZU',\n       'CDLSHORTLINE', 'CDLENGULFING', 'CDLMARUBOZU', 'CDL3OUTSIDE',\n       'CDLSHOOTINGSTAR', 'CDLEVENINGSTAR', 'CDLDARKCLOUDCOVER', 'CDLPIERCING',\n       'CDLEVENINGDOJISTAR', 'CDLTRISTAR', 'CDL2CROWS'],\n      dtype='object', name='candle')</pre> <p>If you want, you can test strategies that contain only those candlestick patterns that have proven to be profitable and/or those which manifsted themselves as anti-signals.</p> <p>Also you now can implement your own Machine-Learning logics to see whether you can come up with your own logic. Also, you can run the logic on a more potent machine, to see how the precision and confidence intervals change per candlestick pattern.</p> In\u00a0[30]: Copied! <pre># synthetic S&amp;P 500 intraday performacne\ndf_reference_strategy = (\n    df[[\"ticker\", \"date\", \"intraday_return\"]]\n    .pivot_table(index=\"date\", columns=\"ticker\")\n    .mean(axis=1)\n)\ndf_reference_strategy = df_reference_strategy.rename(\"intraday_return\").reset_index()\ndf_reference_strategy[\"account_curve\"] = (\n    1 + df_reference_strategy[\"intraday_return\"]\n).cumprod()\ndf_reference_strategy[\"cumsumret\"] = df_reference_strategy[\"intraday_return\"].cumsum()\ndf_reference_strategy.plot(x=\"date\", y=\"account_curve\", figsize=(8, 8))\nplt.show()\n\ncompute_trading_strategy_performance(df=df_reference_strategy, verbose=True);\n</pre> # synthetic S&amp;P 500 intraday performacne df_reference_strategy = (     df[[\"ticker\", \"date\", \"intraday_return\"]]     .pivot_table(index=\"date\", columns=\"ticker\")     .mean(axis=1) ) df_reference_strategy = df_reference_strategy.rename(\"intraday_return\").reset_index() df_reference_strategy[\"account_curve\"] = (     1 + df_reference_strategy[\"intraday_return\"] ).cumprod() df_reference_strategy[\"cumsumret\"] = df_reference_strategy[\"intraday_return\"].cumsum() df_reference_strategy.plot(x=\"date\", y=\"account_curve\", figsize=(8, 8)) plt.show()  compute_trading_strategy_performance(df=df_reference_strategy, verbose=True); <pre>Annualised strategy return [%]: 0.0637\nAnnualised strategy standard deviation of returns [%]: 0.1624\nSharpe ratio of strategy: 0.3922\n</pre> <p>However, we can also opt for a method that compares the buy-and hold approach against employign the naive candlestick strategy. Recall the naive candlestick approach was to sum up the signal for each day across all the candlestick patterns. We can then compare the Sharpe Ration for both, the buy-and-hold approach, as well as the active candlestick approach and compare which of them delivers higher risk-adjusted returns, if at all. This is carrie dout in the below, whearas we slightly modified the approach of the single-stock method from notebook 2.</p> In\u00a0[31]: Copied! <pre>def analyse_Sharpe_Ratios_for_active_and_passive_strategies(\n    SR_buy_and_hold: np.array, SR_naive_cs: np.array\n) -&gt; None:\n    # determine the combined range of both Sharpe Ratios\n    all_ratios = np.concatenate((SR_buy_and_hold, SR_naive_cs))\n    min_edge = all_ratios.min()\n    max_edge = all_ratios.max()\n    bins = np.linspace(\n        min_edge, max_edge, 40\n    )  # 40 equal-width bins across the full range\n\n    # compute empirical CDF for SR_buy_and_hold\n    sorted_SR_buy_and_hold = np.sort(SR_buy_and_hold)\n    yvals_buy_and_hold = np.arange(1, len(sorted_SR_buy_and_hold) + 1) / float(\n        len(sorted_SR_buy_and_hold)\n    )\n\n    # compute empirical CDF for SR_naive_cs\n    sorted_SR_naive_cs = np.sort(SR_naive_cs)\n    yvals_naive_cs = np.arange(1, len(sorted_SR_naive_cs) + 1) / float(\n        len(sorted_SR_naive_cs)\n    )\n\n    # plotting\n    fig, axs = plt.subplots(3, 1, figsize=(10, 18))\n\n    # histograms\n    axs[0].hist(SR_buy_and_hold, bins=bins, alpha=0.5, label=\"Buy and hold\")\n    axs[0].hist(SR_naive_cs, bins=bins, alpha=0.5, label=\"Naive CS\")\n    axs[0].set_title(\"Comparison of Sharpe Ratios: Buy and hold vs. Naive CS\")\n    axs[0].set_xlabel(\"Sharpe Ratio\")\n    axs[0].set_ylabel(\"Frequency\")\n    axs[0].legend()\n\n    # empirical CDFs\n    axs[1].plot(\n        sorted_SR_buy_and_hold,\n        yvals_buy_and_hold,\n        label=\"Buy and hold\",\n        marker=\".\",\n        linestyle=\"none\",\n    )\n    axs[1].plot(\n        sorted_SR_naive_cs,\n        yvals_naive_cs,\n        label=\"Naive CS\",\n        marker=\".\",\n        linestyle=\"none\",\n    )\n    axs[1].set_title(\"Empirical CDF of Sharpe Ratios\")\n    axs[1].set_xlabel(\"Sharpe Ratio\")\n    axs[1].set_ylabel(\"CDF\")\n    axs[1].legend()\n    axs[1].grid(True)\n\n    # box Plots\n    axs[2].boxplot([SR_buy_and_hold, SR_naive_cs], labels=[\"Buy and hold\", \"Naive CS\"])\n    axs[2].set_title(\"Box Plot of Sharpe Ratios\")\n    axs[2].set_ylabel(\"Sharpe Ratio\")\n\n    plt.tight_layout()\n    plt.show()\n\n    return None\n\n\ndef analyze_sharpe_ratios(SR_buy_and_hold: np.array, SR_naive_cs: np.array) -&gt; None:\n    print(\"Buy and Hold Strategy:\")\n    print(f\"Mean Sharpe Ratio: {np.mean(SR_buy_and_hold):.4f}\")\n    print(f\"Median Sharpe Ratio: {np.median(SR_buy_and_hold):.4f}\")\n    print(f\"Kurtosis: {stats.kurtosis(SR_buy_and_hold):.4f}\")\n    print(f\"Skewness: {stats.skew(SR_buy_and_hold):.4f}\\n\")\n\n    print(\"Naive Candlestick Strategy:\")\n    print(f\"Mean Sharpe Ratio: {np.mean(SR_naive_cs):.4f}\")\n    print(f\"Median Sharpe Ratio: {np.median(SR_naive_cs):.4f}\")\n    print(f\"Kurtosis: {stats.kurtosis(SR_naive_cs):.4f}\")\n    print(f\"Skewness: {stats.skew(SR_naive_cs):.4f}\")\n\n    return None\n\n\ndef compare_sharpe_ratios(SR_buy_and_hold: np.array, SR_naive_cs: np.array) -&gt; None:\n    t_stat, p_value = stats.ttest_ind(\n        SR_naive_cs, SR_buy_and_hold, alternative=\"greater\"\n    )\n\n    print(f\"t-statistic: {t_stat}\")\n    print(f\"p-value: {p_value}\")\n\n    # Interpret the p-value\n    if p_value &lt; 0.05:\n        print(\n            \"The naive candlestick strategy has significantly greater Sharpe Ratios than the buy-and-hold strategy at the 5% significance level.\"\n        )\n    else:\n        print(\n            \"There is no significant difference in Sharpe Ratios in favour of the naive candlestick strategy over the buy-and-hold strategy at the 5% significance level.\"\n        )\n\n        return None\n</pre> def analyse_Sharpe_Ratios_for_active_and_passive_strategies(     SR_buy_and_hold: np.array, SR_naive_cs: np.array ) -&gt; None:     # determine the combined range of both Sharpe Ratios     all_ratios = np.concatenate((SR_buy_and_hold, SR_naive_cs))     min_edge = all_ratios.min()     max_edge = all_ratios.max()     bins = np.linspace(         min_edge, max_edge, 40     )  # 40 equal-width bins across the full range      # compute empirical CDF for SR_buy_and_hold     sorted_SR_buy_and_hold = np.sort(SR_buy_and_hold)     yvals_buy_and_hold = np.arange(1, len(sorted_SR_buy_and_hold) + 1) / float(         len(sorted_SR_buy_and_hold)     )      # compute empirical CDF for SR_naive_cs     sorted_SR_naive_cs = np.sort(SR_naive_cs)     yvals_naive_cs = np.arange(1, len(sorted_SR_naive_cs) + 1) / float(         len(sorted_SR_naive_cs)     )      # plotting     fig, axs = plt.subplots(3, 1, figsize=(10, 18))      # histograms     axs[0].hist(SR_buy_and_hold, bins=bins, alpha=0.5, label=\"Buy and hold\")     axs[0].hist(SR_naive_cs, bins=bins, alpha=0.5, label=\"Naive CS\")     axs[0].set_title(\"Comparison of Sharpe Ratios: Buy and hold vs. Naive CS\")     axs[0].set_xlabel(\"Sharpe Ratio\")     axs[0].set_ylabel(\"Frequency\")     axs[0].legend()      # empirical CDFs     axs[1].plot(         sorted_SR_buy_and_hold,         yvals_buy_and_hold,         label=\"Buy and hold\",         marker=\".\",         linestyle=\"none\",     )     axs[1].plot(         sorted_SR_naive_cs,         yvals_naive_cs,         label=\"Naive CS\",         marker=\".\",         linestyle=\"none\",     )     axs[1].set_title(\"Empirical CDF of Sharpe Ratios\")     axs[1].set_xlabel(\"Sharpe Ratio\")     axs[1].set_ylabel(\"CDF\")     axs[1].legend()     axs[1].grid(True)      # box Plots     axs[2].boxplot([SR_buy_and_hold, SR_naive_cs], labels=[\"Buy and hold\", \"Naive CS\"])     axs[2].set_title(\"Box Plot of Sharpe Ratios\")     axs[2].set_ylabel(\"Sharpe Ratio\")      plt.tight_layout()     plt.show()      return None   def analyze_sharpe_ratios(SR_buy_and_hold: np.array, SR_naive_cs: np.array) -&gt; None:     print(\"Buy and Hold Strategy:\")     print(f\"Mean Sharpe Ratio: {np.mean(SR_buy_and_hold):.4f}\")     print(f\"Median Sharpe Ratio: {np.median(SR_buy_and_hold):.4f}\")     print(f\"Kurtosis: {stats.kurtosis(SR_buy_and_hold):.4f}\")     print(f\"Skewness: {stats.skew(SR_buy_and_hold):.4f}\\n\")      print(\"Naive Candlestick Strategy:\")     print(f\"Mean Sharpe Ratio: {np.mean(SR_naive_cs):.4f}\")     print(f\"Median Sharpe Ratio: {np.median(SR_naive_cs):.4f}\")     print(f\"Kurtosis: {stats.kurtosis(SR_naive_cs):.4f}\")     print(f\"Skewness: {stats.skew(SR_naive_cs):.4f}\")      return None   def compare_sharpe_ratios(SR_buy_and_hold: np.array, SR_naive_cs: np.array) -&gt; None:     t_stat, p_value = stats.ttest_ind(         SR_naive_cs, SR_buy_and_hold, alternative=\"greater\"     )      print(f\"t-statistic: {t_stat}\")     print(f\"p-value: {p_value}\")      # Interpret the p-value     if p_value &lt; 0.05:         print(             \"The naive candlestick strategy has significantly greater Sharpe Ratios than the buy-and-hold strategy at the 5% significance level.\"         )     else:         print(             \"There is no significant difference in Sharpe Ratios in favour of the naive candlestick strategy over the buy-and-hold strategy at the 5% significance level.\"         )          return None In\u00a0[32]: Copied! <pre>%%time \n\n# we should loop through all of the tickers to create trading signals for each stock\n\nStrategyPerformance = namedtuple(\n    \"StrategyPerformance\", [\"SR_buy_and_hold\", \"SR_naive_cs\"]\n)\nnaive_cs_vs_buy_and_hold_performance = {}\n\nfor ticker in tickers:\n    df_single_stock = df[df[\"ticker\"] == ticker]\n    cs_single_stock_signals_df = cs_signals_df[cs_signals_df[\"ticker\"] == ticker]\n\n    trading_signal = (\n        cs_single_stock_signals_df.query(\"cs_pattern != 0\")\n        .pivot_table(index=\"date\", columns=\"candle\", values=\"cs_pattern\", aggfunc=\"sum\")\n        .sum(axis=1)\n        .loc[lambda x: x != 0]\n    )\n\n    performance_trading_signals = (\n        df_single_stock[\n            df_single_stock[\"date\"].isin(\n                [date + pd.DateOffset(days=1) for date in trading_signal.index]\n            )\n        ][[\"date\", \"intraday_return\"]]\n        .assign(account_curve=lambda x: (1 + x[\"intraday_return\"]).cumprod())\n        .assign(cumsumret=lambda x: x[\"intraday_return\"].cumsum())\n        .assign(time_between_signals=lambda x: x[\"date\"].diff().dt.days)\n    )\n\n    (_, _, SR_buy_and_hold) = compute_trading_strategy_performance(df=df_single_stock)\n    (_, _, SR_naive_cs) = compute_trading_strategy_performance(\n        df=performance_trading_signals\n    )\n\n    naive_cs_vs_buy_and_hold_performance[ticker] = StrategyPerformance(\n        SR_buy_and_hold=SR_buy_and_hold, SR_naive_cs=SR_naive_cs\n    )\n</pre> %%time   # we should loop through all of the tickers to create trading signals for each stock  StrategyPerformance = namedtuple(     \"StrategyPerformance\", [\"SR_buy_and_hold\", \"SR_naive_cs\"] ) naive_cs_vs_buy_and_hold_performance = {}  for ticker in tickers:     df_single_stock = df[df[\"ticker\"] == ticker]     cs_single_stock_signals_df = cs_signals_df[cs_signals_df[\"ticker\"] == ticker]      trading_signal = (         cs_single_stock_signals_df.query(\"cs_pattern != 0\")         .pivot_table(index=\"date\", columns=\"candle\", values=\"cs_pattern\", aggfunc=\"sum\")         .sum(axis=1)         .loc[lambda x: x != 0]     )      performance_trading_signals = (         df_single_stock[             df_single_stock[\"date\"].isin(                 [date + pd.DateOffset(days=1) for date in trading_signal.index]             )         ][[\"date\", \"intraday_return\"]]         .assign(account_curve=lambda x: (1 + x[\"intraday_return\"]).cumprod())         .assign(cumsumret=lambda x: x[\"intraday_return\"].cumsum())         .assign(time_between_signals=lambda x: x[\"date\"].diff().dt.days)     )      (_, _, SR_buy_and_hold) = compute_trading_strategy_performance(df=df_single_stock)     (_, _, SR_naive_cs) = compute_trading_strategy_performance(         df=performance_trading_signals     )      naive_cs_vs_buy_and_hold_performance[ticker] = StrategyPerformance(         SR_buy_and_hold=SR_buy_and_hold, SR_naive_cs=SR_naive_cs     ) <pre>CPU times: user 50 s, sys: 755 \u00b5s, total: 50 s\nWall time: 50 s\n</pre> In\u00a0[33]: Copied! <pre>SR_buy_and_hold = np.array(\n    [\n        performance.SR_buy_and_hold\n        for performance in naive_cs_vs_buy_and_hold_performance.values()\n        if not np.isnan(performance.SR_buy_and_hold)\n    ]\n)\nSR_naive_cs = np.array(\n    [\n        performance.SR_naive_cs\n        for performance in naive_cs_vs_buy_and_hold_performance.values()\n        if not np.isnan(performance.SR_naive_cs)\n    ]\n)\n\nanalyse_Sharpe_Ratios_for_active_and_passive_strategies(\n    SR_buy_and_hold=SR_buy_and_hold, SR_naive_cs=SR_naive_cs\n)\n</pre> SR_buy_and_hold = np.array(     [         performance.SR_buy_and_hold         for performance in naive_cs_vs_buy_and_hold_performance.values()         if not np.isnan(performance.SR_buy_and_hold)     ] ) SR_naive_cs = np.array(     [         performance.SR_naive_cs         for performance in naive_cs_vs_buy_and_hold_performance.values()         if not np.isnan(performance.SR_naive_cs)     ] )  analyse_Sharpe_Ratios_for_active_and_passive_strategies(     SR_buy_and_hold=SR_buy_and_hold, SR_naive_cs=SR_naive_cs ) In\u00a0[34]: Copied! <pre>analyze_sharpe_ratios(SR_buy_and_hold=SR_buy_and_hold, SR_naive_cs=SR_naive_cs)\n</pre> analyze_sharpe_ratios(SR_buy_and_hold=SR_buy_and_hold, SR_naive_cs=SR_naive_cs) <pre>Buy and Hold Strategy:\nMean Sharpe Ratio: 0.2293\nMedian Sharpe Ratio: 0.2298\nKurtosis: 1.6132\nSkewness: -0.3861\n\nNaive Candlestick Strategy:\nMean Sharpe Ratio: 0.2325\nMedian Sharpe Ratio: 0.2617\nKurtosis: 1.7315\nSkewness: -0.2999\n</pre> <p>Notably, both the mean and the median Sharpe Ratio for the naive candlestick approach (active trading strategy) are higher than for the passive (buy-and-hold) approach. Interestingly, also the skewness is slighly lower for the active approach. Recall that skewness values close to zero suggest a symmetrical distribution of returns around the mean. For the buy-and-hold strategy, a skewness of -0.3861 indicates a skew to the left, suggesting a distribution with a fatter left tail indicating more frequent negative extreme returns than positive ones. The candlestick approach's skewness of -0.2999 also indicates a leftward skew but to a lesser extent, suggesting a slightly more symmetric distribution of returns around the mean compared to the buy-and-hold strategy.</p> <p>Unfortunately, the extra returns obtained form the candlestick approach do not come for free: They are also more risk as indicated by the larger kurtosis. Also note we did not assume any transation costs. For a trading company, a market maker, or a large bank, which are in a position to negotiate lower transaction costs than retail traders, these are less of an issue. However, an active approach involving daily transactions for a retail trader is disadvantageous, to say the least. The active candlestick strategy hence appears more suitable for risk seeking speculatnts who are in pursuit of \"high-risk-high-return\" bets.</p> <p>However, it is noted that the outperformace of mean return, median return, and a smaller skew for the active candlestick approach are indeed interesting observations. In order to determine whether higher Sharpe Ratios for the candlestick approach are statistically significantly greater than the ones for the passive strategy, we perform a one-sided t-test.</p> In\u00a0[35]: Copied! <pre>compare_sharpe_ratios(SR_buy_and_hold, SR_naive_cs)\n</pre> compare_sharpe_ratios(SR_buy_and_hold, SR_naive_cs) <pre>t-statistic: 0.1020279443073558\np-value: 0.4593775985116076\nThere is no significant difference in Sharpe Ratios in favour of the naive candlestick strategy over the buy-and-hold strategy at the 5% significance level.\n</pre> In\u00a0[36]: Copied! <pre>positive_signals = performance_metrics.query(\"ci_lower &gt; 0.5\").index  # signals\ncounter_signals = performance_metrics.query(\n    \"ci_upper &lt; 0.5\"\n).index  # anti-signals/contrarians\n</pre> positive_signals = performance_metrics.query(\"ci_lower &gt; 0.5\").index  # signals counter_signals = performance_metrics.query(     \"ci_upper &lt; 0.5\" ).index  # anti-signals/contrarians In\u00a0[37]: Copied! <pre>%%time \n\nStrategyPerformance = namedtuple(\n    \"StrategyPerformance\", [\"SR_buy_and_hold\", \"SR_naive_cs\"]\n)\nnaive_cs_vs_buy_and_hold_performance = {}\n\nfor ticker in tickers:\n    df_single_stock = df[df[\"ticker\"] == ticker]\n    cs_single_stock_signals_df = cs_signals_df[cs_signals_df[\"ticker\"] == ticker]\n\n    # create a copy for modification\n    filtered_signals_df = cs_single_stock_signals_df.copy()\n\n    # apply the filter directly to this copy\n    filter_mask = filtered_signals_df.index.get_level_values(\"candle\").isin(\n        positive_signals.union(counter_signals)\n    )\n    filtered_signals_df = filtered_signals_df.loc[filter_mask]\n\n    # adjust 'cs_pattern' by multiplying by -1 for counter signals\n    counter_signals_mask = filtered_signals_df.index.get_level_values(\"candle\").isin(\n        counter_signals\n    )\n    filtered_signals_df.loc[counter_signals_mask, \"cs_pattern\"] *= -1\n\n    trading_signal = (\n        filtered_signals_df.query(\"cs_pattern != 0\")\n        .pivot_table(index=\"date\", columns=\"candle\", values=\"cs_pattern\", aggfunc=\"sum\")\n        .sum(axis=1)\n        .loc[lambda x: x != 0]\n    )\n\n    performance_trading_signals = (\n        df_single_stock[\n            df_single_stock[\"date\"].isin(\n                [date + pd.DateOffset(days=1) for date in trading_signal.index]\n            )\n        ][[\"date\", \"intraday_return\"]]\n        .assign(account_curve=lambda x: (1 + x[\"intraday_return\"]).cumprod())\n        .assign(cumsumret=lambda x: x[\"intraday_return\"].cumsum())\n        .assign(time_between_signals=lambda x: x[\"date\"].diff().dt.days)\n    )\n\n    (_, _, SR_buy_and_hold) = compute_trading_strategy_performance(df=df_single_stock)\n    (_, _, SR_naive_cs) = compute_trading_strategy_performance(\n        df=performance_trading_signals\n    )\n\n    naive_cs_vs_buy_and_hold_performance[ticker] = StrategyPerformance(\n        SR_buy_and_hold=SR_buy_and_hold, SR_naive_cs=SR_naive_cs\n    )\n</pre> %%time   StrategyPerformance = namedtuple(     \"StrategyPerformance\", [\"SR_buy_and_hold\", \"SR_naive_cs\"] ) naive_cs_vs_buy_and_hold_performance = {}  for ticker in tickers:     df_single_stock = df[df[\"ticker\"] == ticker]     cs_single_stock_signals_df = cs_signals_df[cs_signals_df[\"ticker\"] == ticker]      # create a copy for modification     filtered_signals_df = cs_single_stock_signals_df.copy()      # apply the filter directly to this copy     filter_mask = filtered_signals_df.index.get_level_values(\"candle\").isin(         positive_signals.union(counter_signals)     )     filtered_signals_df = filtered_signals_df.loc[filter_mask]      # adjust 'cs_pattern' by multiplying by -1 for counter signals     counter_signals_mask = filtered_signals_df.index.get_level_values(\"candle\").isin(         counter_signals     )     filtered_signals_df.loc[counter_signals_mask, \"cs_pattern\"] *= -1      trading_signal = (         filtered_signals_df.query(\"cs_pattern != 0\")         .pivot_table(index=\"date\", columns=\"candle\", values=\"cs_pattern\", aggfunc=\"sum\")         .sum(axis=1)         .loc[lambda x: x != 0]     )      performance_trading_signals = (         df_single_stock[             df_single_stock[\"date\"].isin(                 [date + pd.DateOffset(days=1) for date in trading_signal.index]             )         ][[\"date\", \"intraday_return\"]]         .assign(account_curve=lambda x: (1 + x[\"intraday_return\"]).cumprod())         .assign(cumsumret=lambda x: x[\"intraday_return\"].cumsum())         .assign(time_between_signals=lambda x: x[\"date\"].diff().dt.days)     )      (_, _, SR_buy_and_hold) = compute_trading_strategy_performance(df=df_single_stock)     (_, _, SR_naive_cs) = compute_trading_strategy_performance(         df=performance_trading_signals     )      naive_cs_vs_buy_and_hold_performance[ticker] = StrategyPerformance(         SR_buy_and_hold=SR_buy_and_hold, SR_naive_cs=SR_naive_cs     ) <pre>CPU times: user 48.8 s, sys: 9.18 ms, total: 48.8 s\nWall time: 48.9 s\n</pre> In\u00a0[38]: Copied! <pre>SR_buy_and_hold = np.array(\n    [\n        performance.SR_buy_and_hold\n        for performance in naive_cs_vs_buy_and_hold_performance.values()\n        if not np.isnan(performance.SR_buy_and_hold)\n    ]\n)\nSR_naive_cs = np.array(\n    [\n        performance.SR_naive_cs\n        for performance in naive_cs_vs_buy_and_hold_performance.values()\n        if not np.isnan(performance.SR_naive_cs)\n    ]\n)\n</pre> SR_buy_and_hold = np.array(     [         performance.SR_buy_and_hold         for performance in naive_cs_vs_buy_and_hold_performance.values()         if not np.isnan(performance.SR_buy_and_hold)     ] ) SR_naive_cs = np.array(     [         performance.SR_naive_cs         for performance in naive_cs_vs_buy_and_hold_performance.values()         if not np.isnan(performance.SR_naive_cs)     ] ) In\u00a0[39]: Copied! <pre>analyze_sharpe_ratios(SR_buy_and_hold=SR_buy_and_hold, SR_naive_cs=SR_naive_cs)\n</pre> analyze_sharpe_ratios(SR_buy_and_hold=SR_buy_and_hold, SR_naive_cs=SR_naive_cs) <pre>Buy and Hold Strategy:\nMean Sharpe Ratio: 0.2293\nMedian Sharpe Ratio: 0.2298\nKurtosis: 1.6132\nSkewness: -0.3861\n\nNaive Candlestick Strategy:\nMean Sharpe Ratio: 0.2822\nMedian Sharpe Ratio: 0.3150\nKurtosis: 0.7447\nSkewness: -0.2592\n</pre> In\u00a0[40]: Copied! <pre>compare_sharpe_ratios(SR_buy_and_hold, SR_naive_cs)\n</pre> compare_sharpe_ratios(SR_buy_and_hold, SR_naive_cs) <pre>t-statistic: 1.6637194409255192\np-value: 0.0482428136676386\nThe naive candlestick strategy has significantly greater Sharpe Ratios than the buy-and-hold strategy at the 5% significance level.\n</pre> <p>Recall from above, the naive Candlestick strategy utilizing all patterns was characterised by the following performacne metrics:</p> <pre><code>Mean Sharpe Ratio: 0.2325\nMedian Sharpe Ratio: 0.2617\nKurtosis: 1.7315\nSkewness: -0.2999\n</code></pre>"},{"location":"3_SP500_case/#candlestick-patterns-for-the-sp-500","title":"Candlestick patterns for the S&amp;P 500\u00b6","text":""},{"location":"3_SP500_case/#loading-data-for-the-sp500","title":"Loading data for the S&amp;P500\u00b6","text":""},{"location":"3_SP500_case/#remove-rows-with-missing-data","title":"Remove rows with missing data\u00b6","text":""},{"location":"3_SP500_case/#let-us-now-compute-for-how-many-days-per-stock-we-got-data-for","title":"Let us now compute for how many days per stock we got data for.\u00b6","text":"<p>Companies and be included and taken off the S&amp;P 500 index. Some startups that were not previously listed might prosper and develop into companies large enough to be included in the index, while other may be outcompeted by others, cease to exist, acquired, split up, or taken private and hence either be excluded and/or delisted. Let us investigate for how many days we got data for each stock. While doing so, we find an interesting detour related to Python performance:</p> <p>Python is primarily considered an interpreted language. Python code is executed by an interpreter, which reads the code at runtime and executes it line by line. This process is different from compiled languages, where the source code is transformed into machine code or bytecode before execution, typically resulting in an executable file. However, at a more detailed level, Python code is indeed compiled under the hood. More precisely, when Python code is executed, it is compiled into bytecode, which is a lower-level, platform-independent representation of the source code. This bytecode is then interpreted by the <code>Python Virtual Machine (PVM)</code>, however, compared to a purely compiled language such as <code>C</code> or <code>C++</code>, not turned into a standalone executable file. This process is automatic and transparent to the user, making Python feel like a purely interpreted language. Tools and third-party packages do exist that can package Python programs along with an interpreter into a standalone executable, but this is an additional step beyond Python's standard behavior.</p> <p>The important point is that parsing byte-code through the PVM imposes an overhead which costs time. Hence, <code>Python</code> is considered \"slow\". However, you can use <code>C</code> and <code>C++</code> code within <code>Python</code> to leverage performance benefits. This is a common practice for computational heavy tasks where the execution speed of <code>Python</code> is a bottleneck. Integrating <code>C</code> or <code>C++</code> code into <code>Python</code> can significantly improve the performance of certain operations, especially those that are CPU-bound, such as numerical computations, data processing, and more. This, however, required more detailed knowlege of the <code>Python compiler</code>, is not straight forward, and a topic for another repository.</p> <p>However, that does not mean we cannot speed up our code. Paricularly, we can make use of libraries that are written, at least partially, in <code>C</code> and available in Python, such as <code>numpy</code>. As <code>pandas</code> makes use of <code>numpy</code>, it is often possible to enjoy better performance, especially when we compute in-momory like we using <code>pandas</code>. Thus, it is generally good advise for the sake of performance, to \"write highl-level code thinking low-level\", and the following is meant to demonstrate this.</p> <p>To compute the number of days we got for each of the S&amp;P500 members, a streight-forward (but slow) method is to loop trough each ticker, filter the data frame according to the ticker, and to compute the number of rows. This will be executed below.</p>"},{"location":"3_SP500_case/#we-now-investigate-how-the-length-of-the-history-of-each-stock-in-days-is-distributed","title":"We now investigate how the length of the history of each stock [in days] is distributed\u00b6","text":""},{"location":"3_SP500_case/#how-are-the-stocks-weighted-with-repect-to-the-one-with-the-longst-history-in-the-portfolio","title":"How are the stocks weighted with repect to the one with the longst history in the portfolio?\u00b6","text":""},{"location":"3_SP500_case/#alternatively-we-could-define-a-start-and-end-date-ourselves-and-make-sure-to-select-only-those-stocks-with-a-densely-populated-history","title":"Alternatively, we could define a start and end date ourselves and make sure to select only those stocks with a densely populated history.\u00b6","text":"<p>Densely here means that the stocks should have the same number of data. This ensures stocks thate were recently taken in are not selected as they do not contain enough data</p>"},{"location":"3_SP500_case/#how-does-each-stock-evolve-in-time","title":"How does each stock evolve in time?\u00b6","text":""},{"location":"3_SP500_case/#candlestick-analysis","title":"Candlestick analysis\u00b6","text":"<p>We are now in a position to analyse the whole investment universe. Unfortunately, we now need to deal with a problem we so far go around with: \"Big data analysis\" comes with \"big computational resources\". Recall, we need to:</p> <ul> <li>Loading data to memory coming from 500 stocks, with a history of up to 40 years,</li> <li>Making the pattern recogniton logic act on them, where:<ul> <li>for every date, we create up to 61 new rows, where 61 is the number of candlestick patterns we are  able to identify using <code>talib</code>.</li> </ul> </li> </ul> <p>Unfortunately, this exceeds the memory resources of a standard workstation or laptop.</p> <p>In the following we hence limit ourselves to two years of data and outline how to proceed the analysis, but need to leave considering a wider time-interval to the interested reader whos has a more powerful machine available.</p>"},{"location":"3_SP500_case/#applying-candlestick-analysis-across-the-sp-500-universe","title":"Applying candlestick analysis across the S&amp;P 500 universe\u00b6","text":"<p>Unfortunately, the data provider does not have OHLC data for the S&amp;P 500 index. At the time of writing, an inquiry is still ongoing. In the following, we illustrate how we obtain a synthetic performance reference nontheless, namely by computing the mean intraday return at any day acroos all the oniverse assuming equal weights. This shoudl serve as an approximate solution that works with the data at hand.</p>"},{"location":"3_SP500_case/#analysing-sharpe-ratios-for-a-passive-and-acive-trading-strategy-for-the-sp-500-universe","title":"Analysing Sharpe Ratios for a passive and acive trading strategy for the S&amp;P 500 universe\u00b6","text":"<p>We now turn our focus to comparing the Sharpe Ratios from both, passive and active trading strategies, examining their distribution through histograms, cumulative distribution functions, and box plots. Additionally, we employ specific functions for a detailed statistical analysis.</p> <p>Precisely, <code>analyse_Sharpe_Ratios_for_active_and_passive_strategies()</code> visualizes their distributions, <code>analyze_sharpe_ratios()</code> delves into their statistical characteristics, and <code>compare_sharpe_ratios()</code> statistically determines if the active strategy's Sharpe Ratios significantly outperform those of a reference strategy.</p>"},{"location":"3_SP500_case/#first-approach-taking-into-account-all-candlestick-patterns","title":"First approach: Taking into account all candlestick patterns\u00b6","text":"<p>Now, we compute the Sharpe Ratios for the active, as well as passive (reference) strategy.</p>"},{"location":"3_SP500_case/#second-approach-filter-only-statisticaly-significant-cs-patterns","title":"Second approach: Filter only statisticaly significant cs patterns\u00b6","text":""},{"location":"3_SP500_case/#conclusion","title":"Conclusion\u00b6","text":"<p>Upon filtering the candlestick signals and contrarian signals that were found to be indicating a price move on the next day at the 5% significance level, we could improve the all performance statistics, based on the Sharpe Ratio, in all four categories examined. The full set consists of 61 candlestick patterns, whereas the filtered approach consists of 12 signals and 15 counter signals.</p> <p>The signals were identified at the 5% significance level to be:</p> <pre><code>Index(['CDLDOJI', 'CDLLONGLEGGEDDOJI', 'CDLRICKSHAWMAN', 'CDLHAMMER',\n       'CDLMATCHINGLOW', 'CDLINVERTEDHAMMER', 'CDLGRAVESTONEDOJI',\n       'CDLSEPARATINGLINES', 'CDLSTICKSANDWICH', 'CDLUNIQUE3RIVER',\n       'CDLCOUNTERATTACK', 'CDL3LINESTRIKE'],\n      dtype='object', name='candle')\n</code></pre> <p>The contrarian signals were identified at the 5% significance level to be:</p> <pre><code>Index(['CDLSPINNINGTOP', 'CDLLONGLINE', 'CDLBELTHOLD', 'CDLCLOSINGMARUBOZU',\n       'CDLSHORTLINE', 'CDLENGULFING', 'CDLMARUBOZU', 'CDL3OUTSIDE',\n       'CDLSHOOTINGSTAR', 'CDLEVENINGSTAR', 'CDLDARKCLOUDCOVER', 'CDLPIERCING',\n       'CDLEVENINGDOJISTAR', 'CDLTRISTAR', 'CDL2CROWS'],\n      dtype='object', name='candle')\n</code></pre> <p>Notably, the filtered candlestick approach outperforms the naive buy strategy in all four performance categories investigated. Moreover, a one-sided <code>t-test</code> revealed that the Sharpe Ratios obtained by the filtered candlestick approach is greated than those obtained by the naive buy approach at the 5% level.</p> <p>In further research, one could run the very same code on a more potent machine and simply select a longer data range when loading in the data, to see whether the results reported here still hold. Also, one could attempt an expanding-window approach regarding the considered time-frame to investigate how performance changes in time and whether there are stocks for which the candlestick approach works particularly good or bad. The data source considered for this analysis were the past two years for all S&amp;P 500 components, although for some stocks, there exists data dating back to the 1980ies. Assuming a densely populated data this equates to an upper boundary of 20 000 years of daily stock OHLC data.</p> <p>It remains, however, that the level of analysis carried out and presented here, required access to proprietary data and significant computing power. These could be, for example, the High Performance Computing (HPC) facilities at Imperial College, or a potent private institution. Moreover, an active trading approach is predominantly aimed at players like large hedge funds and investment banks which still have proprietary trading teams, and that are in a position to negotiate low transaction costs. They should also be potent enough to observe, and act, on data streams across the entire S&amp;P 500 universe. An extention to any other index, such as the STOXX600 or any Asian index is easily diable using the existing code.</p> <p>For fund managers, with a more passive approach, the presented analysis can be interesting to optimise their entry points opon which to accummulate or offload positions.</p> <p>For brokers, the presented analysis is useful to craft an arrival strategies to which to adopt their execution logics based on an oppinion whether a stock goes up or down. In case of no signal on a particularly stock, one then would simply fall back to a default behaviour.</p>"},{"location":"3_SP500_case/#end","title":"END\u00b6","text":""},{"location":"3_SP500_case/#appendix","title":"Appendix\u00b6","text":""},{"location":"3_SP500_case/#ml-approach","title":"ML approach\u00b6","text":"<p>The author cannot run the ML approach as we do not have enough memory available to load the required history of the stocks. ML methods are inherently data hungy, so loading just two years of data per stock will not be enough for meaningful results. Also, we cannot mix the history of one stock with the history of another, as financial data is chronological in nature.</p>"},{"location":"BSquant/","title":"BSquant","text":"In\u00a0[\u00a0]: Copied! <pre>from typing import Optional\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nimport plotly.express as px\n</pre> from typing import Optional from pathlib import Path import numpy as np import pandas as pd from scipy.stats import norm import plotly.express as px In\u00a0[\u00a0]: Copied! <pre>def load_data(\n    file_path: Path,\n    compression: Optional[str] = \"gzip\",\n    ticker: Optional[str] = None,\n    selected_start_date: Optional[pd.Timestamp] = None,\n    selected_end_date: Optional[pd.Timestamp] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads data from a compressed CSV file, filters it based on optional criteria,\n    and returns a cleaned pandas DataFrame.\n\n    The function reads a CSV file, optionally compressed, renames columns for consistency,\n    filters the data by a specified ticker and date range, and returns the processed DataFrame.\n\n    Args:\n        file_path (Path): Path to the compressed CSV file.\n        compression (Optional[str], optional): Compression type of the CSV file. Defaults to 'gzip'.\n        ticker (Optional[str], optional): Specific ticker to filter on. If None, no ticker filtering is applied.\n        selected_start_date (Optional[pd.Timestamp], optional): The start date for filtering the data.\n            If None, no start date filtering is applied.\n        selected_end_date (Optional[pd.Timestamp], optional): The end date for filtering the data.\n            If None, no end date filtering is applied.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the processed and optionally filtered data.\n\n    Examples:\n        &gt;&gt;&gt; load_data(Path('/path/to/data.csv'), ticker='AAPL', selected_start_date=pd.Timestamp('2020-01-01'),\n        ... selected_end_date=pd.Timestamp('2020-12-31'))\n        # This will return a DataFrame with data for AAPL between Jan 1, 2020, and Dec 31, 2020.\n    \"\"\"\n    rename_map = {\n        \"Ticker\": \"ticker\",\n        \"DlyCalDt\": \"date\",\n        \"DlyPrc\": \"prc\",\n        \"DlyOpen\": \"open\",\n        \"DlyHigh\": \"high\",\n        \"DlyLow\": \"low\",\n        \"DlyClose\": \"close\",\n        \"DlyVol\": \"vol\",\n        \"DlyPrcVol\": \"price_vol\",\n    }\n\n    # Load and initially process the data\n    df = pd.read_csv(file_path, usecols=rename_map.keys(), compression=compression)\n    df.rename(columns=rename_map, inplace=True)\n    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\")\n\n    if ticker is not None:\n        df = df.copy().query(\"ticker == @ticker\").reset_index(drop=True)\n\n    # Apply date filtering\n    if selected_start_date is not None:\n        df = df[df[\"date\"] &gt;= selected_start_date]\n    if selected_end_date is not None:\n        df = df[df[\"date\"] &lt;= selected_end_date]\n\n    return df.reset_index(drop=True)\n</pre> def load_data(     file_path: Path,     compression: Optional[str] = \"gzip\",     ticker: Optional[str] = None,     selected_start_date: Optional[pd.Timestamp] = None,     selected_end_date: Optional[pd.Timestamp] = None, ) -&gt; pd.DataFrame:     \"\"\"     Loads data from a compressed CSV file, filters it based on optional criteria,     and returns a cleaned pandas DataFrame.      The function reads a CSV file, optionally compressed, renames columns for consistency,     filters the data by a specified ticker and date range, and returns the processed DataFrame.      Args:         file_path (Path): Path to the compressed CSV file.         compression (Optional[str], optional): Compression type of the CSV file. Defaults to 'gzip'.         ticker (Optional[str], optional): Specific ticker to filter on. If None, no ticker filtering is applied.         selected_start_date (Optional[pd.Timestamp], optional): The start date for filtering the data.             If None, no start date filtering is applied.         selected_end_date (Optional[pd.Timestamp], optional): The end date for filtering the data.             If None, no end date filtering is applied.      Returns:         pd.DataFrame: A DataFrame containing the processed and optionally filtered data.      Examples:         &gt;&gt;&gt; load_data(Path('/path/to/data.csv'), ticker='AAPL', selected_start_date=pd.Timestamp('2020-01-01'),         ... selected_end_date=pd.Timestamp('2020-12-31'))         # This will return a DataFrame with data for AAPL between Jan 1, 2020, and Dec 31, 2020.     \"\"\"     rename_map = {         \"Ticker\": \"ticker\",         \"DlyCalDt\": \"date\",         \"DlyPrc\": \"prc\",         \"DlyOpen\": \"open\",         \"DlyHigh\": \"high\",         \"DlyLow\": \"low\",         \"DlyClose\": \"close\",         \"DlyVol\": \"vol\",         \"DlyPrcVol\": \"price_vol\",     }      # Load and initially process the data     df = pd.read_csv(file_path, usecols=rename_map.keys(), compression=compression)     df.rename(columns=rename_map, inplace=True)     df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\")      if ticker is not None:         df = df.copy().query(\"ticker == @ticker\").reset_index(drop=True)      # Apply date filtering     if selected_start_date is not None:         df = df[df[\"date\"] &gt;= selected_start_date]     if selected_end_date is not None:         df = df[df[\"date\"] &lt;= selected_end_date]      return df.reset_index(drop=True) In\u00a0[\u00a0]: Copied! <pre>def process_data(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Processes the given DataFrame by cleaning data, calculating intraday returns,\n    and preparing for subsequent analysis.\n\n    This function performs several steps:\n    - Drops any rows with missing values.\n    - Converts the volume column to integers.\n    - Calculates the intraday return as (close - open) / open.\n    - Determines the sign of the intraday return (positive, negative, or zero) as an integer.\n    - Shifts the intraday returns and their signs to prepare for next-day return analysis.\n\n    Parameters:\n        df (pd.DataFrame): The DataFrame to process. Expects columns 'vol', 'open',\n                           and 'close' to be present.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the original data plus additional columns for\n                      'intraday_return', 'sign_intraday_return', 'next_intraday_return',\n                      and 'sign_next_day_return'. Rows with missing values after processing\n                      are dropped to ensure completeness of the dataset.\n\n    Note:\n        The 'next_intraday_return' and 'sign_next_day_return' columns are intended for\n        use in predictive models where the goal is to forecast the next day's market movement.\n    \"\"\"\n    df = (\n        df.dropna()\n        .assign(\n            vol=lambda x: x[\"vol\"].astype(int),\n            intraday_return=lambda x: (x[\"close\"] - x[\"open\"]) / x[\"open\"],\n            sign_intraday_return=lambda x: np.sign(x[\"intraday_return\"]).astype(int),\n        )\n        .assign(\n            # log_return=lambda x: np.log(x[\"close\"] / x[\"close\"].shift(1)),\n            next_intraday_return=lambda x: x[\"intraday_return\"].shift(-1),\n            sign_next_day_return=lambda x: x[\"sign_intraday_return\"]\n            .shift(-1)\n            .astype(\"Int64\"),\n        )\n        .dropna()\n        .reset_index(drop=True)\n    )\n    return df\n</pre> def process_data(df: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"     Processes the given DataFrame by cleaning data, calculating intraday returns,     and preparing for subsequent analysis.      This function performs several steps:     - Drops any rows with missing values.     - Converts the volume column to integers.     - Calculates the intraday return as (close - open) / open.     - Determines the sign of the intraday return (positive, negative, or zero) as an integer.     - Shifts the intraday returns and their signs to prepare for next-day return analysis.      Parameters:         df (pd.DataFrame): The DataFrame to process. Expects columns 'vol', 'open',                            and 'close' to be present.      Returns:         pd.DataFrame: A DataFrame with the original data plus additional columns for                       'intraday_return', 'sign_intraday_return', 'next_intraday_return',                       and 'sign_next_day_return'. Rows with missing values after processing                       are dropped to ensure completeness of the dataset.      Note:         The 'next_intraday_return' and 'sign_next_day_return' columns are intended for         use in predictive models where the goal is to forecast the next day's market movement.     \"\"\"     df = (         df.dropna()         .assign(             vol=lambda x: x[\"vol\"].astype(int),             intraday_return=lambda x: (x[\"close\"] - x[\"open\"]) / x[\"open\"],             sign_intraday_return=lambda x: np.sign(x[\"intraday_return\"]).astype(int),         )         .assign(             # log_return=lambda x: np.log(x[\"close\"] / x[\"close\"].shift(1)),             next_intraday_return=lambda x: x[\"intraday_return\"].shift(-1),             sign_next_day_return=lambda x: x[\"sign_intraday_return\"]             .shift(-1)             .astype(\"Int64\"),         )         .dropna()         .reset_index(drop=True)     )     return df In\u00a0[\u00a0]: Copied! <pre>def cs_pattern_recognition(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Identifies candlestick patterns in financial data using the `talib` library.\n\n    This function iterates over a list of candlestick pattern recognition functions provided by `talib` and applies each to the input DataFrame containing Open, High, Low, and Close prices (OHLC). It generates a DataFrame for each pattern indicating where the pattern occurs. The results are then consolidated into a single DataFrame, filtering out instances where no pattern was recognized.\n\n    Parameters:\n    - df (pd.DataFrame): A DataFrame containing the financial data with columns for 'open', 'high', 'low', and 'close' prices. The DataFrame must also have a 'date' column used for indexing the results.\n\n    Returns:\n    - pd.DataFrame: A DataFrame indexed by 'candle' pattern names and 'date', containing rows where any candlestick pattern was recognized. The DataFrame includes the original OHLC data, a 'cs_pattern' column indicating the pattern strength or direction, and the 'candle' name for each recognized pattern.\n\n    Note:\n    - The 'cs_pattern' column values are provided by `talib` and represent the strength or direction of the recognized pattern, with non-zero values indicating the occurrence of a pattern.\n    - The function requires `talib` to be installed and accessible in the environment.\n    \"\"\"\n\n    import talib\n\n    candle_names = talib.get_function_groups()[\"Pattern Recognition\"]\n\n    list_of_results_df = []\n\n    for candle in candle_names:\n        tds = df.copy()\n        tds[\"cs_pattern\"] = getattr(talib, candle)(\n            df[\"open\"], df[\"high\"], df[\"low\"], df[\"close\"]\n        )\n        tds[\"candle\"] = candle\n        list_of_results_df.append(tds.set_index([\"candle\", \"date\"]))\n\n    result_df = pd.concat(list_of_results_df)\n    cs_signals_df = result_df[result_df[\"cs_pattern\"] != 0]\n\n    return cs_signals_df\n</pre> def cs_pattern_recognition(df: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"     Identifies candlestick patterns in financial data using the `talib` library.      This function iterates over a list of candlestick pattern recognition functions provided by `talib` and applies each to the input DataFrame containing Open, High, Low, and Close prices (OHLC). It generates a DataFrame for each pattern indicating where the pattern occurs. The results are then consolidated into a single DataFrame, filtering out instances where no pattern was recognized.      Parameters:     - df (pd.DataFrame): A DataFrame containing the financial data with columns for 'open', 'high', 'low', and 'close' prices. The DataFrame must also have a 'date' column used for indexing the results.      Returns:     - pd.DataFrame: A DataFrame indexed by 'candle' pattern names and 'date', containing rows where any candlestick pattern was recognized. The DataFrame includes the original OHLC data, a 'cs_pattern' column indicating the pattern strength or direction, and the 'candle' name for each recognized pattern.      Note:     - The 'cs_pattern' column values are provided by `talib` and represent the strength or direction of the recognized pattern, with non-zero values indicating the occurrence of a pattern.     - The function requires `talib` to be installed and accessible in the environment.     \"\"\"      import talib      candle_names = talib.get_function_groups()[\"Pattern Recognition\"]      list_of_results_df = []      for candle in candle_names:         tds = df.copy()         tds[\"cs_pattern\"] = getattr(talib, candle)(             df[\"open\"], df[\"high\"], df[\"low\"], df[\"close\"]         )         tds[\"candle\"] = candle         list_of_results_df.append(tds.set_index([\"candle\", \"date\"]))      result_df = pd.concat(list_of_results_df)     cs_signals_df = result_df[result_df[\"cs_pattern\"] != 0]      return cs_signals_df In\u00a0[\u00a0]: Copied! <pre>def cs_performance(cs_signals_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Analyses the performance of identified candlestick (CS) patterns by calculating\n    precision and confidence intervals.\n\n    This function takes a DataFrame containing candlestick pattern signals and next-day returns.\n    It computes True Positives (TP) and False Positives (FP) for each pattern based on the alignment\n    between the pattern signal and the actual next-day return. It then aggregates these values to\n    calculate the precision (TP / (TP + FP)) for each candlestick pattern. Additionally, it computes\n    the Wilson score confidence interval for the precision of each pattern.\n\n    Parameters:\n    - cs_signals_df (pd.DataFrame): A DataFrame indexed by candlestick pattern ('candle') and date,\n      containing the signals (cs_pattern) and the next day's return direction\n      (sign_next_day_return).\n\n    Returns:\n    - pd.DataFrame: A DataFrame indexed by candlestick pattern, containing performance metrics\n      including the number of true positives (TP), false positives (FP), total instances of the\n      pattern, precision, Wilson score confidence interval (ci_upper, ci_lower), and the\n      center of the Wilson score interval (TP_wilson). The DataFrame is sorted by total instances\n      and TP in descending order.\n\n    The DataFrame columns include:\n    - TP: Total true positives\n    - FP: Total false positives\n    - total_instances: Total instances of each candlestick pattern\n    - precision: Precision of the pattern signal (TP / (TP + FP))\n    - center: Center of the Wilson score interval\n    - margin: Margin of the Wilson score interval\n    - ci_upper: Upper bound of the Wilson score confidence interval\n    - ci_lower: Lower bound of the Wilson score confidence interval\n    - TP_wilson: Center of the Wilson score interval, representing adjusted true positives\n\n    Note:\n    - The function uses the norm.ppf function from scipy.stats to calculate the Wilson score interval,\n      assuming a confidence level of 95%.\n    \"\"\"\n\n    cs_signals_df_copy = cs_signals_df.copy()\n\n    # Calculate TP and FP for each CS pattern\n    cs_signals_df_copy[\"TP\"] = (\n        (\n            np.sign(cs_signals_df_copy[\"cs_pattern\"])\n            == np.sign(cs_signals_df_copy[\"sign_next_day_return\"])\n        )\n        &amp; (cs_signals_df_copy[\"cs_pattern\"] != 0)\n    ).astype(int)\n\n    cs_signals_df_copy[\"FP\"] = (\n        (\n            np.sign(cs_signals_df_copy[\"cs_pattern\"])\n            != np.sign(cs_signals_df_copy[\"sign_next_day_return\"])\n        )\n        &amp; (cs_signals_df_copy[\"cs_pattern\"] != 0)\n    ).astype(int)\n\n    # Group by the 'signal' level of the index (which represents the candlestick pattern) and sum TP and FP for each pattern\n    performance_metrics = cs_signals_df_copy.groupby(\"candle\").agg(\n        {\n            \"TP\": \"sum\",\n            \"FP\": \"sum\",\n        }\n    )\n    performance_metrics[\"total_instances\"] = (\n        performance_metrics[\"TP\"] + performance_metrics[\"FP\"]\n    )\n\n    # Calculate the performance metrics for each candlestick pattern\n    performance_metrics[\"precision\"] = performance_metrics[\"TP\"] / (\n        performance_metrics[\"TP\"] + performance_metrics[\"FP\"]\n    )\n\n    # Replace any NaN values with 0\n    performance_metrics.fillna(0, inplace=True)\n\n    # Calculate the Wilson score interval for each row\n    z = norm.ppf(0.975)\n\n    for index, row in performance_metrics.iterrows():\n        p_hat = row[\"precision\"]  # The observed proportion (precision)\n        n = row[\"total_instances\"]  # The total instances (TP + FP)\n        denominator = 1 + z**2 / n\n        center = (p_hat + z**2 / (2 * n)) / denominator\n        margin = (\n            z * np.sqrt((p_hat * (1 - p_hat) / n) + z**2 / (4 * n**2))\n        ) / denominator\n        performance_metrics.at[index, \"center\"] = center\n        performance_metrics.at[index, \"margin\"] = margin\n\n    performance_metrics[\"ci_upper\"] = (\n        performance_metrics[\"center\"] + performance_metrics[\"margin\"]\n    )\n    performance_metrics[\"ci_lower\"] = (\n        performance_metrics[\"center\"] - performance_metrics[\"margin\"]\n    )\n    performance_metrics[\"TP_wilson\"] = performance_metrics[\"center\"]\n\n    # Sort the performance DataFrame by 'TP' in descending order\n    performance_metrics = performance_metrics.sort_values(\n        by=[\"total_instances\", \"TP\"], ascending=False\n    )\n\n    return performance_metrics\n</pre> def cs_performance(cs_signals_df: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"     Analyses the performance of identified candlestick (CS) patterns by calculating     precision and confidence intervals.      This function takes a DataFrame containing candlestick pattern signals and next-day returns.     It computes True Positives (TP) and False Positives (FP) for each pattern based on the alignment     between the pattern signal and the actual next-day return. It then aggregates these values to     calculate the precision (TP / (TP + FP)) for each candlestick pattern. Additionally, it computes     the Wilson score confidence interval for the precision of each pattern.      Parameters:     - cs_signals_df (pd.DataFrame): A DataFrame indexed by candlestick pattern ('candle') and date,       containing the signals (cs_pattern) and the next day's return direction       (sign_next_day_return).      Returns:     - pd.DataFrame: A DataFrame indexed by candlestick pattern, containing performance metrics       including the number of true positives (TP), false positives (FP), total instances of the       pattern, precision, Wilson score confidence interval (ci_upper, ci_lower), and the       center of the Wilson score interval (TP_wilson). The DataFrame is sorted by total instances       and TP in descending order.      The DataFrame columns include:     - TP: Total true positives     - FP: Total false positives     - total_instances: Total instances of each candlestick pattern     - precision: Precision of the pattern signal (TP / (TP + FP))     - center: Center of the Wilson score interval     - margin: Margin of the Wilson score interval     - ci_upper: Upper bound of the Wilson score confidence interval     - ci_lower: Lower bound of the Wilson score confidence interval     - TP_wilson: Center of the Wilson score interval, representing adjusted true positives      Note:     - The function uses the norm.ppf function from scipy.stats to calculate the Wilson score interval,       assuming a confidence level of 95%.     \"\"\"      cs_signals_df_copy = cs_signals_df.copy()      # Calculate TP and FP for each CS pattern     cs_signals_df_copy[\"TP\"] = (         (             np.sign(cs_signals_df_copy[\"cs_pattern\"])             == np.sign(cs_signals_df_copy[\"sign_next_day_return\"])         )         &amp; (cs_signals_df_copy[\"cs_pattern\"] != 0)     ).astype(int)      cs_signals_df_copy[\"FP\"] = (         (             np.sign(cs_signals_df_copy[\"cs_pattern\"])             != np.sign(cs_signals_df_copy[\"sign_next_day_return\"])         )         &amp; (cs_signals_df_copy[\"cs_pattern\"] != 0)     ).astype(int)      # Group by the 'signal' level of the index (which represents the candlestick pattern) and sum TP and FP for each pattern     performance_metrics = cs_signals_df_copy.groupby(\"candle\").agg(         {             \"TP\": \"sum\",             \"FP\": \"sum\",         }     )     performance_metrics[\"total_instances\"] = (         performance_metrics[\"TP\"] + performance_metrics[\"FP\"]     )      # Calculate the performance metrics for each candlestick pattern     performance_metrics[\"precision\"] = performance_metrics[\"TP\"] / (         performance_metrics[\"TP\"] + performance_metrics[\"FP\"]     )      # Replace any NaN values with 0     performance_metrics.fillna(0, inplace=True)      # Calculate the Wilson score interval for each row     z = norm.ppf(0.975)      for index, row in performance_metrics.iterrows():         p_hat = row[\"precision\"]  # The observed proportion (precision)         n = row[\"total_instances\"]  # The total instances (TP + FP)         denominator = 1 + z**2 / n         center = (p_hat + z**2 / (2 * n)) / denominator         margin = (             z * np.sqrt((p_hat * (1 - p_hat) / n) + z**2 / (4 * n**2))         ) / denominator         performance_metrics.at[index, \"center\"] = center         performance_metrics.at[index, \"margin\"] = margin      performance_metrics[\"ci_upper\"] = (         performance_metrics[\"center\"] + performance_metrics[\"margin\"]     )     performance_metrics[\"ci_lower\"] = (         performance_metrics[\"center\"] - performance_metrics[\"margin\"]     )     performance_metrics[\"TP_wilson\"] = performance_metrics[\"center\"]      # Sort the performance DataFrame by 'TP' in descending order     performance_metrics = performance_metrics.sort_values(         by=[\"total_instances\", \"TP\"], ascending=False     )      return performance_metrics In\u00a0[\u00a0]: Copied! <pre>def plot_cs_performance(\n    df: pd.DataFrame,\n    criterion: str = \"total_instances\",\n    plot_performance: bool = False,\n    title_suffix: str = \"\",\n) -&gt; None:\n    \"\"\"\n    Generates a bar plot to visualize the performance of candlestick patterns based on a specified criterion.\n\n    This function creates a bar chart for candlestick pattern performance metrics, with optional error bars\n    representing 95% confidence intervals and performance zones. The performance zones are visual representations\n    that categorize the patterns into red, yellow, and green zones based on their performance.\n\n    Parameters:\n    - df (pd.DataFrame): DataFrame containing candlestick pattern performance metrics, indexed by pattern names.\n    - criterion (str): The performance metric to be visualized on the y-axis of the bar plot. Default is \"total_instances\".\n    - plot_performance (bool): If True, adds error bars for 95% confidence intervals and performance zones to the plot.\n                               Performance zones are marked in red (0.0 to 0.5), yellow (0.5 to 0.55), and green (0.55 to 1.0).\n    - title_suffix (str): Additional text to append to the plot's title. Default is an empty string.\n\n    Returns:\n    - None: The function directly displays the plot and does not return any value.\n\n    The function utilizes Plotly Express for plotting and updates the layout and traces with Plotly Graph Objects\n    for customization, including error bars and shaded performance zones.\n    \"\"\"\n\n    # Create the bar plot\n    fig = px.bar(\n        df,\n        x=df.index,\n        y=criterion,\n        title=f\"{criterion} for {len(df)} CS patterns&lt;br&gt;{title_suffix}\",\n        labels={\"x\": \"CS Pattern\", \"y\": criterion},\n        width=800,\n        height=800,\n    )\n\n    # Update layout\n    fig.update_layout(\n        font={\"size\": 18},\n        autosize=False,\n    )\n\n    # Conditionally add 95% confidence intervals and performance zones\n    if plot_performance:\n        # Calculate the error relative to TP_wilson for upper and lower bounds\n        error_y_upper = df[\"ci_upper\"] - df[criterion]  # Distance to upper CI bound\n        error_y_lower = df[criterion] - df[\"ci_lower\"]  # Distance to lower CI bound\n\n        # Adjust the bar plot to include error bars\n        fig.update_traces(\n            error_y={\n                \"type\": \"data\",\n                \"symmetric\": False,\n                \"array\": error_y_upper,\n                \"arrayminus\": error_y_lower,\n            }\n        )\n        fig.add_hrect(y0=0.0, y1=0.5, line_width=0, fillcolor=\"red\", opacity=0.4)\n        fig.add_hrect(y0=0.5, y1=0.55, line_width=0, fillcolor=\"yellow\", opacity=0.4)\n        fig.add_hrect(y0=0.55, y1=1.0, line_width=0, fillcolor=\"green\", opacity=0.4)\n\n    fig.show()\n\n    return None\n</pre> def plot_cs_performance(     df: pd.DataFrame,     criterion: str = \"total_instances\",     plot_performance: bool = False,     title_suffix: str = \"\", ) -&gt; None:     \"\"\"     Generates a bar plot to visualize the performance of candlestick patterns based on a specified criterion.      This function creates a bar chart for candlestick pattern performance metrics, with optional error bars     representing 95% confidence intervals and performance zones. The performance zones are visual representations     that categorize the patterns into red, yellow, and green zones based on their performance.      Parameters:     - df (pd.DataFrame): DataFrame containing candlestick pattern performance metrics, indexed by pattern names.     - criterion (str): The performance metric to be visualized on the y-axis of the bar plot. Default is \"total_instances\".     - plot_performance (bool): If True, adds error bars for 95% confidence intervals and performance zones to the plot.                                Performance zones are marked in red (0.0 to 0.5), yellow (0.5 to 0.55), and green (0.55 to 1.0).     - title_suffix (str): Additional text to append to the plot's title. Default is an empty string.      Returns:     - None: The function directly displays the plot and does not return any value.      The function utilizes Plotly Express for plotting and updates the layout and traces with Plotly Graph Objects     for customization, including error bars and shaded performance zones.     \"\"\"      # Create the bar plot     fig = px.bar(         df,         x=df.index,         y=criterion,         title=f\"{criterion} for {len(df)} CS patterns{title_suffix}\",         labels={\"x\": \"CS Pattern\", \"y\": criterion},         width=800,         height=800,     )      # Update layout     fig.update_layout(         font={\"size\": 18},         autosize=False,     )      # Conditionally add 95% confidence intervals and performance zones     if plot_performance:         # Calculate the error relative to TP_wilson for upper and lower bounds         error_y_upper = df[\"ci_upper\"] - df[criterion]  # Distance to upper CI bound         error_y_lower = df[criterion] - df[\"ci_lower\"]  # Distance to lower CI bound          # Adjust the bar plot to include error bars         fig.update_traces(             error_y={                 \"type\": \"data\",                 \"symmetric\": False,                 \"array\": error_y_upper,                 \"arrayminus\": error_y_lower,             }         )         fig.add_hrect(y0=0.0, y1=0.5, line_width=0, fillcolor=\"red\", opacity=0.4)         fig.add_hrect(y0=0.5, y1=0.55, line_width=0, fillcolor=\"yellow\", opacity=0.4)         fig.add_hrect(y0=0.55, y1=1.0, line_width=0, fillcolor=\"green\", opacity=0.4)      fig.show()      return None In\u00a0[\u00a0]: Copied! <pre>def compute_trading_strategy_performance(\n    df: pd.DataFrame, verbose: bool = False\n) -&gt; tuple:\n    \"\"\"\n    Computes and returns the performance metrics of a trading strategy based on intraday returns,\n    optionally printing the results.\n\n    Parameters:\n    - df (pd.DataFrame): DataFrame with 'intraday_return' for the strategy's intraday returns.\n    - verbose (bool): If True, prints the calculated metrics. Default is False.\n\n    Returns:\n    - tuple: Contains the annualised return, annualised standard deviation of returns, and the Sharpe ratio.\n    \"\"\"\n    mean_intraday_return = df[\"intraday_return\"].mean()\n    std_intraday_return = df[\"intraday_return\"].std()\n    sharpe_ratio = (260 * mean_intraday_return) / (np.sqrt(260) * std_intraday_return)\n\n    if verbose:\n        print(f\"Annualised strategy return [%]: {260 * mean_intraday_return:.4f}\")\n        print(\n            f\"Annualised strategy standard deviation of returns [%]: {np.sqrt(260) * std_intraday_return:.4f}\"\n        )\n        print(f\"Sharpe ratio of strategy: {sharpe_ratio:.4f}\")\n\n    return (\n        260 * mean_intraday_return,\n        np.sqrt(260) * std_intraday_return,\n        sharpe_ratio,\n    )\n</pre> def compute_trading_strategy_performance(     df: pd.DataFrame, verbose: bool = False ) -&gt; tuple:     \"\"\"     Computes and returns the performance metrics of a trading strategy based on intraday returns,     optionally printing the results.      Parameters:     - df (pd.DataFrame): DataFrame with 'intraday_return' for the strategy's intraday returns.     - verbose (bool): If True, prints the calculated metrics. Default is False.      Returns:     - tuple: Contains the annualised return, annualised standard deviation of returns, and the Sharpe ratio.     \"\"\"     mean_intraday_return = df[\"intraday_return\"].mean()     std_intraday_return = df[\"intraday_return\"].std()     sharpe_ratio = (260 * mean_intraday_return) / (np.sqrt(260) * std_intraday_return)      if verbose:         print(f\"Annualised strategy return [%]: {260 * mean_intraday_return:.4f}\")         print(             f\"Annualised strategy standard deviation of returns [%]: {np.sqrt(260) * std_intraday_return:.4f}\"         )         print(f\"Sharpe ratio of strategy: {sharpe_ratio:.4f}\")      return (         260 * mean_intraday_return,         np.sqrt(260) * std_intraday_return,         sharpe_ratio,     )"}]}