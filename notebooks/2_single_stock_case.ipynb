{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a89fd38",
   "metadata": {},
   "source": [
    "# Decoding Market Signals: Candlestick patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82948cdb",
   "metadata": {},
   "source": [
    "## Load libraries\n",
    "At first, we will load up some dependencies that we will make use of later on. We follow the convention of naming them a standard alias for easier handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import talib\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from IPython.display import Image\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d315f",
   "metadata": {},
   "source": [
    "# What is a candle stick and what are candlestick patterns?\n",
    "This section aims at briefly describing what candlesticks are, so we bring everyone up to speed regarding this graphical representation of price data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268ce82",
   "metadata": {},
   "source": [
    "## Anatomy of a candle stick (aka Japanese candle stick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f49d1",
   "metadata": {
    "pycharm": {
     "name": "#%%  # Image works well, but the markdown environment does not , as seen below\n"
    }
   },
   "outputs": [],
   "source": [
    "Image(\n",
    "    filename=\"/home/bjs/Desktop/vcs/ReCoDE-DecodingMarketSignals/figures/cs_anatomy.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4646df87",
   "metadata": {},
   "source": [
    "## Candle stick patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc2ecfa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Image(\n",
    "    filename=\"/home/bjs/Desktop/vcs/ReCoDE-DecodingMarketSignals/figures/cs_prediction.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3cd637",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"./../data/MSFT.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb19b00",
   "metadata": {},
   "source": [
    "# Loading and pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae900dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what columns are available from the Wharton CRSP data?\n",
    "pd.read_csv(\"./../data/MSFT.csv\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312de386",
   "metadata": {},
   "source": [
    "We can make use of a technique called `method chaining` to pipe multiple processing steps within Pandas. This offers two advantages\n",
    " - We start from the raw data as input statement initially. In case the data is small enough to be processed in-memory, we do not require to store intermediary results on disk which confuses transparency and is less efficient.\n",
    " - Increased code readability and transparency for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf32c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd.read_csv(\n",
    "        \"./../data/MSFT.csv\",\n",
    "        usecols=[\"date\", \"OPENPRC\", \"ASKHI\", \"BIDLO\", \"PRC\", \"VOL\"],\n",
    "    )\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"OPENPRC\": \"open\",\n",
    "            \"ASKHI\": \"high\",\n",
    "            \"BIDLO\": \"low\",\n",
    "            \"PRC\": \"close\",\n",
    "            \"VOL\": \"vol\",\n",
    "        }\n",
    "    )\n",
    "    .dropna()\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        date=lambda df: pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\"),\n",
    "        vol=lambda df: df[\"vol\"].astype(int),\n",
    "        intraday_return=lambda df: (df[\"close\"] - df[\"open\"]) / df[\"open\"],\n",
    "        sign_intraday_return=lambda df: np.sign(df[\"intraday_return\"]).astype(int),\n",
    "    )[\n",
    "        [\n",
    "            \"date\",\n",
    "            \"open\",\n",
    "            \"high\",\n",
    "            \"low\",\n",
    "            \"close\",\n",
    "            \"vol\",\n",
    "            \"intraday_return\",\n",
    "            \"sign_intraday_return\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "df[\"log_return\"] = np.log(df[\"close\"] / df[\"close\"].shift(1))\n",
    "df[\"next_intraday_return\"] = df[\"intraday_return\"].shift(-1)\n",
    "df[\"sign_next_day_return\"] = df[\"sign_intraday_return\"].shift(-1).astype(\"Int64\")\n",
    "\n",
    "# df['previous_day_return'] = df['sign_intraday_return'].shift(+1)#.dropna().astype(int)\n",
    "# df['5_days_mean_return'] = df['log_return'].rolling(window=5, min_periods=1).mean().shift(-5)  # Idea: Compute 5 days average ahead returns one day after the signal. using pd.rolling and pd.shift.\n",
    "df = df.dropna()\n",
    "# df['previous_day_return'] = df['previous_day_return'].astype(int)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b40119",
   "metadata": {},
   "source": [
    "# What candlestick patterns are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "candle_names = talib.get_function_groups()[\"Pattern Recognition\"]\n",
    "candle_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520966dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_results_df = []\n",
    "# for ListingId in df['ListingId'].unique():\n",
    "#     equity_df = df.loc[df['ListingId']==ListingId].reset_index()\n",
    "\n",
    "for candle in candle_names:\n",
    "    tds = df.copy()\n",
    "    tds[\"cs_pattern\"] = getattr(talib, candle)(\n",
    "        df[\"open\"], df[\"high\"], df[\"low\"], df[\"close\"]\n",
    "    )\n",
    "    tds[\"candle\"] = candle\n",
    "    list_of_results_df.append(tds.set_index([\"candle\", \"date\"]))\n",
    "\n",
    "result_df = pd.concat(list_of_results_df)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da155be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can query an CDL pattern from a multiindex data frame like so:\n",
    "result_df.loc[\"CDL2CROWS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2894a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_signals_df = result_df[result_df[\"cs_pattern\"] != 0]\n",
    "cs_signals_df.head()\n",
    "\n",
    "# James' suggestion regarding returns:\n",
    "# relative difference between today's return and next day's return\n",
    "# (next days value - current days value) /  current days value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4560e",
   "metadata": {},
   "source": [
    "## Performance for signals will tbe a confusion matrix\n",
    "\n",
    "A confusion matrix is a table that is used to define the performance of a classification algorithm. A confusion matrix quantifies and summarizes the performance of a classification algorithm.\n",
    "\n",
    "The confusion matrix consists of four characteristics that are used to define the measurement metrics of the classifier. These four numbers are:\n",
    "\n",
    "- i) TP (True Positive): TP represents the number of days who have been properly picked up by the trading algorithm, meaning they have we make a profit.\n",
    "\n",
    "- ii) TN (True Negative): TN represents the number of correctly classified days who are where the trading algorithm is not supposed to act based on the signal.\n",
    "\n",
    "- iii) FP (False Positive): FP represents the number of misclassified days, i.e. we enter a trade but the market in fact trend in opposit direction as our signal predicts. FP is also known as a Type I error.\n",
    "\n",
    "- iv) FN (False Negative): FN represents the number of trading days that were not picked up by the signal when the market, in fact turned in the direction the signal was meant to indicate. It means a missed opportunity. FN is also known as a Type II error.\n",
    "\n",
    "- Performance metrics of an algorithm are accuracy, precision, recall, and F1 score, which are calculated on the basis of TP, TN, FP, and FN.\n",
    "\n",
    "- Accuracy of an algorithm is represented as the ratio of correctly classified trading days (TP+TN) to the total number of trading days (TP+TN+FP+FN).\n",
    "\n",
    "- Precision of an algorithm is represented as the ratio of correctly classified trading days in which we make a profit (TP) to the total number of trading days predicted to move synchronously with our signal (TP+FP).\n",
    "\n",
    "- Recall metric is defined as the ratio of correctly classified trading days (TP) divided by total number of trading days who actually were in our favour given the signal. The perception behind recall is how many day were classified as trending in the direction the signal shoul dpick up. Recall is also called as sensitivity.\n",
    "\n",
    "- F1 score is also known as the F Measure. The F1 score states the equilibrium between the precision and the recall.\n",
    "\n",
    "For further information on confusion matrices, see:\n",
    "\n",
    "https://www.kdnuggets.com/2020/01/guide-precision-recall-confusion-matrix.html\n",
    "\n",
    "https://towardsdatascience.com/performance-metrics-confusion-matrix-precision-recall-and-f1-score-a8fe076a2262"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4112c4",
   "metadata": {},
   "source": [
    "The Wilson score interval for a proportion is given by:\n",
    "\n",
    "$$\n",
    "\\text{Center} = \\frac{\\hat{p} + \\frac{z^2}{2n}}{1 + \\frac{z^2}{n}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Margin} = \\frac{z \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n} + \\frac{z^2}{4n^2}}}{1 + \\frac{z^2}{n}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{CI Lower Bound} = \\text{Center} - \\text{Margin}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{CI Upper Bound} = \\text{Center} + \\text{Margin}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( $\\hat{p}$ \\) is the observed proportion (e.g., precision)\n",
    "- \\( z \\) is the z-score corresponding to the desired confidence level (1.96 for 95% confidence)\n",
    "- \\( n \\) is the total number of instances (e.g., TP + FP)\n",
    "\n",
    "The Wilson score interval is useful when the sample size is small or when the proportion is near 0 or 1. Unlike the normal approximation interval, the Wilson interval does not produce probabilities outside the [0, 1] range and adjusts the observed proportion by a factor related to the confidence level and sample size, providing a more accurate confidence interval for proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb22a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_signals_df = cs_signals_df.copy()\n",
    "\n",
    "\n",
    "# Calculate TP, FP, TN, FN for each CS pattern\n",
    "cs_signals_df[\"TP\"] = (\n",
    "    (\n",
    "        np.sign(cs_signals_df[\"cs_pattern\"])\n",
    "        == np.sign(cs_signals_df[\"sign_next_day_return\"])\n",
    "    )\n",
    "    & (cs_signals_df[\"cs_pattern\"] != 0)\n",
    ").astype(int)\n",
    "\n",
    "cs_signals_df[\"FP\"] = (\n",
    "    (\n",
    "        np.sign(cs_signals_df[\"cs_pattern\"])\n",
    "        != np.sign(cs_signals_df[\"sign_next_day_return\"])\n",
    "    )\n",
    "    & (cs_signals_df[\"cs_pattern\"] != 0)\n",
    ").astype(int)\n",
    "\n",
    "# Does not really make sense, as most of the time the cs patterns dont appear\n",
    "# cs_signals_df['TN'] = ((cs_signals_df['cs_pattern'] == 0) & (cs_signals_df['sign_next_day_return'] == 0)).astype(int)\n",
    "# cs_signals_df['FN'] = ((cs_signals_df['cs_pattern'] == 0) & (cs_signals_df['sign_next_day_return'] != 0)).astype(int)\n",
    "\n",
    "# Group by the 'signal' level of the index (which represents the candlestick pattern)\n",
    "# and sum the TP, FP, TN, FN for each pattern\n",
    "performance_metrics = cs_signals_df.groupby(\"candle\").agg(\n",
    "    {\n",
    "        \"TP\": \"sum\",\n",
    "        \"FP\": \"sum\",\n",
    "        # 'TN': 'sum',\n",
    "        # 'FN': 'sum'\n",
    "    }\n",
    ")\n",
    "performance_metrics[\"total_instances\"] = (\n",
    "    performance_metrics[\"TP\"] + performance_metrics[\"FP\"]\n",
    ")\n",
    "\n",
    "# Calculate the performance metrics for each candlestick pattern\n",
    "performance_metrics[\"precision\"] = performance_metrics[\"TP\"] / (\n",
    "    performance_metrics[\"TP\"] + performance_metrics[\"FP\"]\n",
    ")\n",
    "# performance_metrics['Recall'] = performance_metrics['TP'] / (performance_metrics['TP'] + performance_metrics['FN'])\n",
    "# performance_metrics['Accuracy'] = (performance_metrics['TP'] + performance_metrics['TN']) / (performance_metrics['TP'] + performance_metrics['FP'] + performance_metrics['TN'] + performance_metrics['FN'])\n",
    "# performance_metrics['F1 Score'] = 2 * (performance_metrics['Precision'] * performance_metrics['Recall']) / (performance_metrics['Precision'] + performance_metrics['Recall'])\n",
    "\n",
    "# Replace any NaN values with 0\n",
    "performance_metrics.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Calculate the Wilson score interval for each row\n",
    "\n",
    "z = norm.ppf(0.975)\n",
    "# performance_metrics['precision_lower'] = 0\n",
    "# performance_metrics['precision_upper'] = 0\n",
    "for index, row in performance_metrics.iterrows():\n",
    "    p_hat = row[\"precision\"]  # The observed proportion (precision)\n",
    "    n = row[\"total_instances\"]  # The total instances (TP + FP)\n",
    "    denominator = 1 + z**2 / n\n",
    "    center = (p_hat + z**2 / (2 * n)) / denominator\n",
    "    margin = (z * np.sqrt((p_hat * (1 - p_hat) / n) + z**2 / (4 * n**2))) / denominator\n",
    "    #     performance_metrics.at[index, 'precision_lower'] = center - margin  # somehow this does not work\n",
    "    #     performance_metrics.at[index, 'precision_upper'] = center + margin\n",
    "    performance_metrics.at[index, \"center\"] = center\n",
    "    performance_metrics.at[index, \"margin\"] = margin\n",
    "\n",
    "\n",
    "performance_metrics[\"ci_upper\"] = (\n",
    "    performance_metrics[\"center\"] + performance_metrics[\"margin\"]\n",
    ")\n",
    "performance_metrics[\"ci_lower\"] = (\n",
    "    performance_metrics[\"center\"] - performance_metrics[\"margin\"]\n",
    ")\n",
    "performance_metrics[\"TP_wilson\"] = performance_metrics[\"center\"]\n",
    "\n",
    "\n",
    "# Sort the performance DataFrame by 'TP' in descending order\n",
    "performance_metrics = performance_metrics.sort_values(\n",
    "    by=[\"total_instances\", \"TP\"], ascending=False\n",
    ")\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de1a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cs_performance(\n",
    "    df: pd.DataFrame,\n",
    "    criterion: str = \"total_instances\",\n",
    "    plot_performance: bool = False,\n",
    "    title_suffix: str = \"\",\n",
    ") -> None:\n",
    "    fig = px.bar(\n",
    "        df,\n",
    "        x=df.index,\n",
    "        y=criterion,\n",
    "        barmode=\"group\",\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        font={\"size\": 18},\n",
    "        legend={\n",
    "            \"yanchor\": \"top\",\n",
    "            \"y\": 0.99,\n",
    "            \"xanchor\": \"right\",\n",
    "            \"x\": 0.99,\n",
    "            \"font\": {\"size\": 20},\n",
    "        },\n",
    "        title=f\"{criterion} for {len(df)} CS patterns<br>{title_suffix}\",\n",
    "        xaxis_title=\"CS Pattern\",\n",
    "        yaxis_title=criterion,\n",
    "        autosize=False,\n",
    "        width=800,\n",
    "        height=800,\n",
    "    )\n",
    "\n",
    "    if plot_performance:\n",
    "        fig.add_hrect(y0=0.0, y1=0.5, line_width=0, fillcolor=\"red\", opacity=0.4)\n",
    "        fig.add_hrect(y0=0.5, y1=0.55, line_width=0, fillcolor=\"yellow\", opacity=0.4)\n",
    "        fig.add_hrect(y0=0.55, y1=1.0, line_width=0, fillcolor=\"green\", opacity=0.4)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5eceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all patterns, ranked by number of instances\n",
    "plot_cs_performance(\n",
    "    df=performance_metrics,\n",
    "    criterion=\"total_instances\",\n",
    "    title_suffix=\"across the whole data set.\",\n",
    ")\n",
    "\n",
    "# plot the patterns, ranked by number of instances, with a true-positive rate >50%.\n",
    "plot_cs_performance(\n",
    "    df=performance_metrics.query(\"precision > 0.5\").sort_values(\n",
    "        by=\"total_instances\", ascending=False\n",
    "    ),\n",
    "    criterion=\"total_instances\",\n",
    "    title_suffix=\"with precision > 50%.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea134ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cs_performance(\n",
    "    df: pd.DataFrame, criterion: str = \"total_instances\", plot_performance: bool = False\n",
    ") -> None:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add the main bar chart\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df.index,\n",
    "            y=df[criterion],\n",
    "            name=criterion,\n",
    "            error_y={\n",
    "                \"type\": \"data\",\n",
    "                \"symmetric\": False,\n",
    "                \"array\": df[\"ci_upper\"] - df[criterion],\n",
    "                \"arrayminus\": df[criterion] - df[\"ci_lower\"],\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        font={\"size\": 18},\n",
    "        legend={\n",
    "            \"yanchor\": \"top\",\n",
    "            \"y\": 0.99,\n",
    "            \"xanchor\": \"right\",\n",
    "            \"x\": 0.99,\n",
    "            \"font\": {\"size\": 20},\n",
    "        },\n",
    "        title=f\"{criterion} for {len(df)} CS patterns across the data set\",\n",
    "        xaxis_title=\"CS Pattern\",\n",
    "        yaxis_title=criterion,\n",
    "        autosize=False,\n",
    "        width=2000,\n",
    "        height=1000,\n",
    "    )\n",
    "\n",
    "    # Add performance zones if plot_performance is True\n",
    "    if plot_performance:\n",
    "        fig.add_hrect(y0=0.0, y1=0.5, line_width=0, fillcolor=\"red\", opacity=0.2)\n",
    "        fig.add_hrect(y0=0.5, y1=0.55, line_width=0, fillcolor=\"yellow\", opacity=0.2)\n",
    "        fig.add_hrect(y0=0.55, y1=1.0, line_width=0, fillcolor=\"green\", opacity=0.2)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "plot_cs_performance(\n",
    "    df=performance_metrics, criterion=\"TP_wilson\", plot_performance=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c36603",
   "metadata": {},
   "source": [
    "## How does acting on the trading signals do (idealised situation without transaaction costs)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5135d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_signal = (\n",
    "    cs_signals_df[cs_signals_df[\"cs_pattern\"] != 0][[\"cs_pattern\"]]\n",
    "    .pivot_table(index=\"date\", columns=\"candle\", values=\"cs_pattern\", aggfunc=\"sum\")\n",
    "    .sum(axis=1)\n",
    "    .loc[lambda x: x != 0]\n",
    ")\n",
    "\n",
    "trading_signal.plot(\n",
    "    figsize=(8, 8),\n",
    "    title=\"Signal strength based on candle sticks\\n(signal > 0 => long; signal < 0 => short)\",\n",
    ")\n",
    "plt.ylabel(\"candle\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trading_strategy_performance(df: pd.DataFrame) -> None:\n",
    "    mean_intraday_return = df[\"intraday_return\"].mean()\n",
    "    std_intraday_return = df[\"intraday_return\"].std()\n",
    "    sharpe_ratio = (260 * mean_intraday_return) / (np.sqrt(260) * std_intraday_return)\n",
    "    print(f\"Annualised strategy return [%]: {260 * mean_intraday_return:.4}\")\n",
    "    print(\n",
    "        f\"Annualised strategy standard deviation of returns [%]: {np.sqrt(260) * std_intraday_return:.4}\"\n",
    "    )\n",
    "    print(f\"Sharpe ratio of strategy: {sharpe_ratio:.4}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1687e2",
   "metadata": {},
   "source": [
    "## How does acting on the trading signals docompare to a buy-and-hold strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9d4827",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_trading_signals = (\n",
    "    df[\n",
    "        df[\"date\"].isin([date + pd.DateOffset(days=1) for date in trading_signal.index])\n",
    "    ][[\"date\", \"intraday_return\"]]\n",
    "    .assign(account_curve=lambda x: (1 + x[\"intraday_return\"]).cumprod())\n",
    "    .assign(cumsumret=lambda x: x[\"intraday_return\"].cumsum())\n",
    "    .assign(time_between_signals=lambda x: x[\"date\"].diff().dt.days)\n",
    ")\n",
    "\n",
    "performance_trading_signals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f458ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reference_strategy = df[[\"date\", \"log_return\", \"intraday_return\"]].copy()\n",
    "df_reference_strategy[\"account_curve\"] = (\n",
    "    1 + df_reference_strategy[\"intraday_return\"]\n",
    ").cumprod()\n",
    "df_reference_strategy[\"cumsumret\"] = df_reference_strategy[\"intraday_return\"].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75bd340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative sum of intraday returns to plot the account curve\n",
    "performance_trading_signals.hist(column=\"time_between_signals\", bins=20, figsize=(8, 8))\n",
    "plt.xlabel(\"Time between signals [days]\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of time differences between signals\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.plot(\n",
    "    performance_trading_signals[\"date\"],\n",
    "    performance_trading_signals[\"cumsumret\"],\n",
    "    label=\"cumulative return using candlesticks\",\n",
    "    color=\"b\",\n",
    ")\n",
    "ax.plot(\n",
    "    df_reference_strategy[\"date\"],\n",
    "    df_reference_strategy[\"cumsumret\"],\n",
    "    label=\"cumulative return assuming buy-and-hold\",\n",
    "    color=\"r\",\n",
    ")\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"cumulative return\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set_title(\"long-short strategy using candlesticks vs. buy-and-hold\")\n",
    "plt.show()\n",
    "\n",
    "# Compute trading strategy statistics\n",
    "compute_trading_strategy_performance(df=performance_trading_signals)\n",
    "print()\n",
    "compute_trading_strategy_performance(df=df_reference_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926575d4",
   "metadata": {},
   "source": [
    "# Does Machine learning help to to reduce the noise in the signals?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb3d2b",
   "metadata": {},
   "source": [
    "The following code pivots the dataframe such that we got the date as an index and the candlestick signals as columns.\n",
    "\n",
    "Joinging two or more data frames is a very important and frequently occuring task in data science in general and computing in particular, and a topic in its own right. The author encourages you to study merge, joins and concatonations of dataframes in your own time, as we need to focus here at the task at hand, but I cannot stress their importance enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f612dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signal_and_return = (\n",
    "    cs_signals_df.reset_index()\n",
    "    .pivot_table(index=\"date\", columns=\"candle\", values=\"cs_pattern\", aggfunc=\"first\")\n",
    "    .fillna(0)\n",
    "    .join(df.set_index(\"date\")[[\"sign_next_day_return\"]], how=\"left\")\n",
    ")\n",
    "\n",
    "df_signal_and_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0c6e0",
   "metadata": {},
   "source": [
    "In the next cell it is demonstrated explicitely that the column named `sign_next_day_return` is a categorical variable. Either the return can be positive or negative. Let us neglect the few cases where they are zero. We will filter these cases as to limit ourselves witha binary classification problem. Technically, we could, in logistic regression, accommodate for the occurance of three cathegorical variables, namely positive returns, negative returns and zero returns. However, as zero returns are the minority and machine learning is data hungy, we filter them out. Also the next cell shows that days of zero returns are a clear minority and hence we feel comfortable our slight simplification is indeed justified empirically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff6bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signal_and_return[\"sign_next_day_return\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2a577",
   "metadata": {},
   "source": [
    "Let us quickly review logistic regression that helps us trying to classify a category for next days' return, depending on the candlestick signal. Recall that in logistic regression we map our linear model to a probability:\n",
    "\n",
    "$$z=\\boldsymbol{w^T}\\boldsymbol{x}$$\n",
    "\n",
    "$$P(y = 1) = \\frac{1}{(1+\\exp(-z))}$$\n",
    "\n",
    "For classification purposes, we typically then assign this probability to a discrete class (-1 and +1 in our case), based on a threshold (0 by default):\n",
    "\n",
    "$$y=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "    -1, & P(y = 1)\\le0 \\\\\n",
    "    1, & P(y = 1)>0 \\\\\n",
    "\\end{array} \n",
    "\\right.$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1706355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression??\n",
    "# cost function, gradient descent function, Andrew Ng explains how that works\n",
    "# James sends me some resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aee8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CORES = multiprocessing.cpu_count()\n",
    "df_filtered = df_signal_and_return[df_signal_and_return[\"sign_next_day_return\"] != 0]\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "RANDOM_STATE = random.randint(0, 1000)\n",
    "print(RANDOM_STATE)\n",
    "\n",
    "X = df_filtered.iloc[:, :-1]\n",
    "y = df_filtered.iloc[:, -1].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, shuffle=False\n",
    ")\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# lets quickly confirm whether the length of our train-test split is correct\n",
    "print(X_train.__len__() / X.__len__())\n",
    "print(X_test.__len__() / X.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968779fd",
   "metadata": {},
   "source": [
    "Note: You could have also slided the data set into training and test splits yourself, for example like so:\n",
    "\n",
    "```\n",
    "split_index = int(0.8 * len(df_filtered))  # 80% for training, 20% for testing\n",
    "df_train = df_filtered.iloc[:split_index]\n",
    "df_test = df_filtered.iloc[split_index:]\n",
    "```\n",
    "\n",
    "In Python and Pandas, slicing follows a consistent rule: the start index is included, and the stop index \n",
    "is excluded. This is known as half-open interval: `[start, stop)`. Hence, slicing is end-exclusive and `df_train` and `df_test` are non-overlapping and essentially fulfil the same purpose we achieved using `train_test_split`. \n",
    "\n",
    "Let us now fit the logistic regressor on teh training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5958f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = LogisticRegression(max_iter=5000, tol=0.01, n_jobs=N_CORES)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction on training set\n",
    "pred_train = model.predict(X_train)\n",
    "assert len(pred_train) == len(y_train)\n",
    "\n",
    "# Prediction on test set\n",
    "pred_test = model.predict(X_test)\n",
    "assert len(pred_test) == len(y_test)\n",
    "\n",
    "print(\n",
    "    \"Compute hit-rate of strategy obtained from the logistic regressor on the TEST set\"\n",
    ")\n",
    "print((pred_test == y_test).sum() / len(y_test), end=\"\\n\\n\")\n",
    "print(\n",
    "    \"Compute hit-rate of strategy obtained from the logistic regressor on the TRAINING set\"\n",
    ")\n",
    "print((pred_train == y_train).sum() / len(y_train), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64380c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick remark: we could have achieved the same result more tersly, opon invoking:\n",
    "np.mean(pred_test == y_test)  # 0.5154553049289892"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a1143",
   "metadata": {},
   "source": [
    "Why can we use `np.mean(pred == y_test)` to compute the hit-rate efficiently?\n",
    "\n",
    "This is because of how boolean values are treated in numerical operations in Python. When you compare two arrays (or lists) element-wise for equality (pred == y_test), you get an array of boolean values (True or False). In this boolean array, True represents a hit (where the corresponding elements in pred and y_test are equal), and False represents a miss.\n",
    "\n",
    "In Python, boolean values are a subtype of integers. True is treated as 1, and False is treated as 0. Therefore, when you calculate the mean of an array of boolean values, you are effectively calculating the proportion of True values in the array. This is exactly the hit rate - the proportion of times pred and y_test are equal. The sum of this boolean array gives you the number of hits (since True is 1 and False is 0, the sum is the count of True values).\n",
    "\n",
    "Dividing this sum by the length of the array gives you the average value, which is the proportion of hits.\n",
    "So, np.mean(pred == y_test) is a neat way to calculate the hit rate directly from the arrays without explicitly counting the hits and dividing by the total number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81533bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weighting will disproportionally weight the learning from less often occuring instances as compared to more occuring ones.\n",
    "# say, we got a segmentation problem in image recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c5dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (\n",
    "    df[df[\"date\"].isin(y_test.index)][\n",
    "        [\"date\", \"intraday_return\", \"sign_next_day_return\"]\n",
    "    ]\n",
    "    .dropna()\n",
    "    .reset_index(drop=True)\n",
    "    .assign(pred=pred_test)  # directly assigning pred without reindexing\n",
    "    .assign(\n",
    "        daily_return_strategy=lambda x: np.where(\n",
    "            np.sign(x[\"sign_next_day_return\"]) == np.sign(x[\"pred\"]),\n",
    "            abs(x[\"intraday_return\"]),\n",
    "            -abs(x[\"intraday_return\"]),\n",
    "        )\n",
    "    )\n",
    "    .assign(account_curve=lambda x: x[\"daily_return_strategy\"].cumsum())\n",
    "    .assign(contrarian_account_curve=lambda x: -1 * x[\"daily_return_strategy\"])\n",
    ")\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Define common bins\n",
    "min_value = min(tmp[\"intraday_return\"].min(), tmp[\"intraday_return\"].min())\n",
    "max_value = max(tmp[\"intraday_return\"].max(), tmp[\"intraday_return\"].max())\n",
    "bins = np.linspace(min_value, max_value, 40)\n",
    "\n",
    "# Calculate histogram for 'hits'\n",
    "hits_data = tmp[tmp[\"sign_next_day_return\"] == tmp[\"pred\"]][\"intraday_return\"]\n",
    "counts_hits, _ = np.histogram(hits_data, bins=bins)\n",
    "percent_hits = counts_hits / counts_hits.sum()\n",
    "\n",
    "# Calculate histogram for 'misses'\n",
    "misses_data = tmp[tmp[\"sign_next_day_return\"] != tmp[\"pred\"]][\"intraday_return\"]\n",
    "counts_misses, _ = np.histogram(misses_data, bins=bins)\n",
    "percent_misses = counts_misses / counts_misses.sum()\n",
    "\n",
    "# Verify that the sum of the percentages is equal to 1\n",
    "print(\"Sum of percentages for hits histogram:\", np.sum(percent_hits))\n",
    "print(\"Sum of percentages for misses histogram:\", np.sum(percent_misses))\n",
    "\n",
    "# Plot the histograms\n",
    "plt.hist(bins[:-1], bins, weights=percent_hits, alpha=0.5, label=\"hits\")\n",
    "plt.hist(bins[:-1], bins, weights=percent_misses, alpha=0.5, label=\"misses\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Intraday return histogram [percentage]\")\n",
    "plt.xlabel(\"intraday return\")\n",
    "plt.ylabel(\"percentage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa0102",
   "metadata": {},
   "source": [
    "Note: in case you wish to quickly inspect the source code inline a jupyter notebook, of a function you wrote earlier and with to recall, you can invoke either of the following statements:\n",
    "\n",
    "```\n",
    "import inspect\n",
    "print(inspect.getsource(compute_trading_strategy_performance))\n",
    "```\n",
    "\n",
    "```\n",
    "compute_trading_strategy_performance??\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7dbffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_trading_strategy_performance(df=tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd19c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.plot(\n",
    "    x=\"date\",\n",
    "    y=\"account_curve\",\n",
    "    title=\"Comparing account curve of Machine Learning strategy\",\n",
    "    figsize=(10, 10),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f1b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plotting the first curve\n",
    "ax.plot(\n",
    "    performance_trading_signals[\"date\"],\n",
    "    performance_trading_signals[\"cumsumret\"],\n",
    "    label=\"cumulative return using candlesticks\",\n",
    "    color=\"b\",\n",
    ")\n",
    "\n",
    "# Plotting the second curve\n",
    "ax.plot(\n",
    "    df_reference_strategy[\"date\"],\n",
    "    df_reference_strategy[\"cumsumret\"],\n",
    "    label=\"cumulative return assuming buy-and-hold\",\n",
    "    color=\"r\",\n",
    ")\n",
    "\n",
    "# Adding the third curve from 'tmp'\n",
    "ax.plot(\n",
    "    tmp[\"date\"],\n",
    "    tmp[\"account_curve\"],\n",
    "    label=\"cumulative return using ML strategy\",\n",
    "    color=\"g\",\n",
    ")  # Assuming color green\n",
    "\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"cumulative return\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set_title(\"long-short Strategy using candlesticks vs. buy-and-bold vs. ML strategy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a62f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how often [%] do we predict the correct returns using our Machine learning framework?\n",
    "hits_data.__len__() / (hits_data.__len__() + misses_data.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab82c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Idea: Get tail ratio in for the trading strategies"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78bbc1c5",
   "metadata": {},
   "source": [
    "# Strategy: Naive candlesticks\n",
    "Annualised strategy return [%]: 0.07667\n",
    "Annualised strategy standard deviation of returns [%]: 0.2585\n",
    "Sharpe ratio of strategy: 0.2966\n",
    "\n",
    "# Strategy: Machine learning applied to candlesticks\n",
    "Annualised strategy return [%]: 0.09971\n",
    "Annualised strategy standard deviation of returns [%]: 0.2673\n",
    "Sharpe ratio of strategy: 0.373\n",
    "    \n",
    "# strategy: Buy and hold   \n",
    "Annualised strategy return [%]: 0.1078\n",
    "Annualised strategy standard deviation of returns [%]: 0.2626\n",
    "Sharpe ratio of strategy: 0.4107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3613ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1 + tmp[\"account_curve\"].iloc[-1]) ** (\n",
    "    1 / 3\n",
    ") - 1  # annual geometric return during test period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1 + 0.2073185303721532) ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / 400 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3264c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularizing the regression? (setting weights to zero for those CS signals)\n",
    "# can we integrate this in Interactive Broker's API?\n",
    "\n",
    "#\n",
    "# - github actions to run pre-commit\n",
    "# https://github.com/kynan/nbstripout/blob/master/.pre-commit-hooks.yaml\n",
    "# https://github.com/EnergySystemsModellingLab/MUSE_OS/blob/4dd1a0a42889a5551b68a29faf25384c08849a90/.pre-commit-config.yaml#L32\n",
    "\n",
    "\n",
    "# Meeting with James next: Tuesday 6 in the afternoon (send invite)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e00d97",
   "metadata": {},
   "source": [
    "## Recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d78ff7",
   "metadata": {},
   "source": [
    "`sklearn.feature_selection.RFECV` is a feature selection method from scikit-learn that stands for `Recursive Feature Elimination with Cross-Validation`. It is used to select the most significant features by recursively considering smaller and smaller sets of features.\n",
    "\n",
    "- Recursive Feature Elimination (RFE): It fits a model and removes the weakest feature(s) (i.e., the feature(s) with the least importance according to the model), then fits a new model on the reduced dataset. This process is repeated until all features have been ranked by their importance.\n",
    "\n",
    "\n",
    "- Cross-Validation (CV): The RFE procedure is embedded within a cross-validation loop to determine the optimal number of features. For each iteration, a different subset of the data is used for training and validation, and the performance of the model is assessed. This helps to ensure that the selection of features is robust and performs well on unseen data.\n",
    "\n",
    "\n",
    "`selector.ranking_`: This array shows the ranking of features. A rank of 1 indicates that the feature was selected as important. Higher ranks indicate the order in which features were removed, where a rank of 2 means the feature was the second least important, a rank of 3 means the feature was the third least important, and so on.\n",
    "\n",
    "Looking at `selector.ranking_` array, it seems that most features were selected (rank 1). Some features have higher rank values, like 4 and 3 indicating their relative importance was considered lower by the `RFECV` process. These features were removed in the corresponding iterations of the feature elimination process.\n",
    "\n",
    "The feature with a rank of 4 was removed first, the one with 3 was removed second, and this pattern continues until all features with a rank greater than 1 are removed. The features with a rank of 1 are considered important and are selected by the `RFECV`. These are the features you might consider keeping in your model for the best balance between model complexity and performance as determined by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea8465",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# takes a about 80 seconds to run for one stock, but surely is dependent on the number of CPU cores on your machine.\n",
    "\n",
    "N_CORES = multiprocessing.cpu_count()\n",
    "model = LogisticRegressionCV(\n",
    "    cv=5, multi_class=\"ovr\", max_iter=1000, n_jobs=N_CORES\n",
    ")  # If we choose multi_class=‘ovr’, then a binary problem is fit for each label.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# See whether the L2 / L3 metrics add value to out strategy\n",
    "selector = RFECV(model, cv=5, n_jobs=N_CORES)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "print(selector.ranking_)\n",
    "\n",
    "selected_features = selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.ranking_.__len__()  # this corresponds to the numbe rof candlesticks we investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4bea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "assert len(pred) == len(y_test)\n",
    "\n",
    "print(\"Compute hit-rate of strategy obtained from the logistic regressor\")\n",
    "print((pred == y_test).sum() / len(y_test))\n",
    "model.score(X_test, y_test)  # gives same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31f99e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this mask to X_train and X_test to keep only important features\n",
    "X_train_selected = X_train.loc[:, selected_features]\n",
    "X_test_selected = X_test.loc[:, selected_features]\n",
    "\n",
    "# Refit the model on the selected features\n",
    "model_selected = LogisticRegressionCV(\n",
    "    cv=5, multi_class=\"ovr\", max_iter=1000, n_jobs=N_CORES\n",
    ")\n",
    "model_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred_selected = model_selected.predict(X_test_selected)\n",
    "model_selected.score(X_test_selected, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88e664b",
   "metadata": {},
   "source": [
    "# Your task: Can you improve the strategy further?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ef9dd",
   "metadata": {},
   "source": [
    "# Literature \n",
    "\n",
    "[1] Scientific Guide To Price Action and Pattern Trading Wisdom of Trend, Cycle, and Fractal Wave (Young Ho Seo)\n",
    "\n",
    "If you are interested to back-test more trading strategies, you can look at:\n",
    "\n",
    "[2] Finding Alphas: A Quantitative Approach to Building Trading Strategies Hardcover – 27 Sept. 2019 by Igor Tulchinsky (Editor)\n",
    "\n",
    "[3] 151 Trading Strategies, Z. Kakushadze and J.A. Serur. 151 Trading Strategies. Cham, Switzerland: Palgrave Macmillan, an imprint of Springer Nature, 1st Edition (2018), XX, 480 pp; ISBN 978-3-030-02791-9, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3247865"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc520c6",
   "metadata": {},
   "source": [
    "# Notes (delete in final version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4150c0b8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1) Pull-Requests on github (Python package index)\n",
    "\n",
    "2) Modularize the code using classes/OOP/Design Patterns.\n",
    "\n",
    "3) Writing Unit-Test\n",
    "\n",
    "4) CI test suits\n",
    "\n",
    "5) Storage of data (backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO's:\n",
    "\n",
    "# Get strategy of logistic regression in as a third one to compare angainst the naive candle stic and the buy and hold one\n",
    "\n",
    "# document the notebook in more detail and get it into a nice shape (ongoing process).\n",
    "# IDEA: what to do in case of imbalanced features? I could use imblean,\n",
    "# however, that will alter the data.(Maybe James has a better idea.)?\n",
    "\n",
    "# jupytext to convert jupyter notebooks to text\n",
    "# figure out about Teams sharing screen though not being a presenter\n",
    "# TODO:Account curve\n",
    "# contact CRSP regarding corporate actions meta data, i.e. stock splits and dividends and how to best correct prices.\n",
    "# benchmark: SP500 for the overall set, but for one stock only, the buy-and-hold strategy is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d04c0",
   "metadata": {},
   "source": [
    "## Optional: Talk about split and dividend adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c2e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does contain stock splits\n",
    "# df_reference_strategy = df[['date','log_return']].copy()\n",
    "# df_reference_strategy['account_curve'] = (1 + temp_df2['log_return']).cumprod()\n",
    "# df_reference_strategy['account_curve'].plot()\n",
    "# plt.show()\n",
    "\n",
    "# does contain stock splits\n",
    "df_reference_strategy[\"cumsumret\"].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e475b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"log_return\"].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87407cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier returns should align with stock splits => CRSP data not split- and divident-adjusted\n",
    "condition = (df[\"log_return\"] > 0.2) | (df[\"log_return\"] < -0.2)\n",
    "df[condition | condition.shift(1) | condition.shift(-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f665e9",
   "metadata": {},
   "source": [
    "## Lets verify the shape of the account curve is the same for percentage changes and for closing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e300470",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1 + df[\"close\"].pct_change()).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1f5a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[\"close\"] / df[\"close\"][0]).plot(figsize=(8, 8))\n",
    "plt.show()\n",
    "print((df[\"close\"] / df[\"close\"][0]).iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e8532",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=\"date\", y=\"close\", figsize=(8, 8))\n",
    "plt.title(\"Account curve to buy and hold MSFT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5499f2f",
   "metadata": {},
   "source": [
    "## How to visually inspect the signals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976accc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cs_chart(df: pd.DataFrame, ListingId=None) -> go.Figure:\n",
    "    if ListingId is None:\n",
    "        fig = go.Figure(\n",
    "            data=[\n",
    "                go.Candlestick(\n",
    "                    x=df.index,\n",
    "                    open=df[\"open\"],\n",
    "                    high=df[\"high\"],\n",
    "                    low=df[\"low\"],\n",
    "                    close=df[\"close\"],\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            font={\"size\": 18},\n",
    "            legend={\n",
    "                \"yanchor\": \"top\",\n",
    "                \"y\": 0.99,\n",
    "                \"xanchor\": \"right\",\n",
    "                \"x\": 0.99,\n",
    "                \"font\": {\"size\": 20},\n",
    "            },\n",
    "            title=\"Evolution of price chart diplayed as candlestick chart\",\n",
    "            xaxis_title=\"time [days]\",\n",
    "            yaxis_title=\"price\",\n",
    "            autosize=False,\n",
    "            width=800,\n",
    "            height=800,\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "    elif ListingId is not None:\n",
    "        fig = go.Figure(\n",
    "            data=[\n",
    "                go.Candlestick(\n",
    "                    x=df[\"Date\"][df[\"ListingId\"] == ListingId],\n",
    "                    open=df[\"open\"][df[\"ListingId\"] == ListingId],\n",
    "                    high=df[\"high\"][df[\"ListingId\"] == ListingId],\n",
    "                    low=df[\"low\"][df[\"ListingId\"] == ListingId],\n",
    "                    close=df[\"close\"][df[\"ListingId\"] == ListingId],\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            font={\"size\": 18},\n",
    "            legend={\n",
    "                \"yanchor\": \"top\",\n",
    "                \"y\": 0.99,\n",
    "                \"xanchor\": \"right\",\n",
    "                \"x\": 0.99,\n",
    "                \"font\": {\"size\": 20},\n",
    "            },\n",
    "            title=\"Evolution of price chart diplayed as candlestick chart\",\n",
    "            xaxis_title=\"time [days]\",\n",
    "            yaxis_title=\"price\",\n",
    "            autosize=False,\n",
    "            width=800,\n",
    "            height=800,\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "    else:\n",
    "        print(\"Do not understand plotting instruction. Check data frame and ListingId\")\n",
    "\n",
    "\n",
    "plot_cs_chart(result_df.loc[\"CDLDOJI\"].reset_index().set_index(\"date\")[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7724e",
   "metadata": {},
   "source": [
    "# Appendix : These is a collection of rough work for the author of this notebook. Ignore. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f96f23f",
   "metadata": {},
   "source": [
    "Mike's idea:\n",
    "\n",
    "The broker is rewarded a fraction of over performacne\n",
    "\n",
    "50 days we trade 1% at a price of 100; then the price crashes to 90 and we trade 5 days at 90.\n",
    "\n",
    "Questions: Can a trader benefit from knowing there is a share buy-back? I.e. Buying back shares should create a surplus demand\n",
    "\n",
    "(50 * 100 + 5 * 90) / 55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e7683b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```\n",
    "import talib\n",
    "\n",
    "def compute_intraday_return(row):\n",
    "    row['positive_intraday'] = np.where((row['Close|Executed'] - row['Open|Executed']) > 0, True, False)\n",
    "    return row\n",
    "\n",
    "cs_performance_dict = dict()\n",
    "\n",
    "for ListingId in df['ListingId'].unique():\n",
    "    equity_df = pd.DataFrame(index=df[df['ListingId']==ListingId]['Date'], \n",
    "                                data=df[df['ListingId']==ListingId][['Open|Executed', 'High|Executed', 'Low|Executed', 'Close|Executed']].values, \n",
    "                                columns = df[df['ListingId']==ListingId][['Open|Executed', 'High|Executed', 'Low|Executed', 'Close|Executed']].columns\n",
    "                            )\n",
    "    equity_df = equity_df.apply(compute_intraday_return, axis='columns')\n",
    "    \n",
    "    results = []\n",
    "    cols = []\n",
    "    for attr in dir(talib):\n",
    "        if attr[:3]=='CDL':\n",
    "            res = getattr(talib, attr)(equity_df['Open|Executed'], equity_df['High|Executed'], equity_df['Low|Executed'], equity_df['Close|Executed'])\n",
    "            results.append(res)\n",
    "            cols.append(attr)\n",
    "\n",
    "    patterns = pd.DataFrame(results).T\n",
    "    patterns.columns = cols\n",
    "    \n",
    "    signal_df = patterns[(patterns.select_dtypes(include=['number']) != 0).any(1)]  # alternative: patterns.loc[(patterns.loc[:, patterns.dtypes != object] != 0).any(1)]\n",
    "    \n",
    "    signal_and_return_df = signal_df.sum(axis=1).rename('cumulative_signal').to_frame().merge(equity_df['positive_intraday'].shift(-1), left_index=True, right_index=True)[:-1]  # dont count the last as its return will be NaN\n",
    "    signal_and_return_df = signal_and_return_df.loc[signal_and_return_df['cumulative_signal'] != 0]\n",
    "    signal_and_return_df['performance'] = np.where((signal_and_return_df['cumulative_signal']>0) & (signal_and_return_df['positive_intraday']==True), True,\n",
    "                                                   np.where((signal_and_return_df['cumulative_signal']<0) & (signal_and_return_df['positive_intraday']==False), True, False))\n",
    "    \n",
    "    TP = signal_and_return_df['performance'].sum() / (signal_and_return_df['performance'].shape[0] & Where the signaal occurs )  #TODO modify the ratio of TP/FP\n",
    "    FP = 1-TP                                                                                      # \n",
    "    \n",
    "    cs_performance_dict[ListingId] = {'TP_rate':round(TP, 4), 'FP_rate':round(FP, 4)}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
