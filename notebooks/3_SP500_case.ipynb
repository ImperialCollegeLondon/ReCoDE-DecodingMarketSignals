{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data for the S&P500 and analyzing its constituents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List, Optional\n",
    "from BSquant import load_and_process_data\n",
    "from BSquant import cs_pattern_recognition\n",
    "from BSquant import cs_performance\n",
    "from BSquant import plot_cs_performance\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your file\n",
    "file_path_to_ticker_data = \"./../data/SP500_tickers_one_per_line.txt\"\n",
    "\n",
    "tickers = []\n",
    "\n",
    "# Open the ticker-file with a context manager and read each line adding ot to the list of tickers\n",
    "with open(file_path_to_ticker_data, \"r\") as file:\n",
    "    for line in file:\n",
    "        ticker = line.strip()  # Strip newline characters and whitespace\n",
    "        tickers.append(ticker)  # Add the cleaned ticker to the list\n",
    "\n",
    "print(\"Number of tickers (may include multiple tickers per stock) is\", len(tickers))\n",
    "print(\"Number of unique tickers is:\", set(tickers).__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ticker in enumerate(tickers):\n",
    "    print(f\"{i+1}:{ticker}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is the header columns provided by CRSP:\n",
    "\n",
    "['CUSIP',\n",
    " 'CUSIP9',\n",
    " 'ConditionalType',\n",
    " 'DelActionType',\n",
    " 'DelPaymentType',\n",
    " 'DelReasonType',\n",
    " 'DelStatusType',\n",
    " 'DisDeclareDt',\n",
    " 'DisDetailType',\n",
    " 'DisDivAmt',\n",
    " 'DisExDt',\n",
    " 'DisFacPr',\n",
    " 'DisFacShr',\n",
    " 'DisFreqType',\n",
    " 'DisOrdinaryFlg',\n",
    " 'DisOrigCurType',\n",
    " 'DisPERMCO',\n",
    " 'DisPERMNO',\n",
    " 'DisPayDt',\n",
    " 'DisPaymentType',\n",
    " 'DisRecordDt',\n",
    " 'DisSeqNbr',\n",
    " 'DisTaxType',\n",
    " 'DisType',\n",
    " 'DlyAsk'\n",
    " 'DlyBid',\n",
    " 'DlyCalDt',\n",
    " 'DlyCap',\n",
    " 'DlyCapFlg',\n",
    " 'DlyClose',\n",
    " 'DlyDelFlg',\n",
    " 'DlyDistRetFlg',\n",
    " 'DlyFacPrc',\n",
    " 'DlyHigh',\n",
    " 'DlyLow',\n",
    " 'DlyMMCnt',\n",
    " 'DlyNonOrdDivAmt',\n",
    " 'DlyNumTrd',\n",
    " 'DlyOpen',\n",
    " 'DlyOrdDivAmt',\n",
    " 'DlyPrc',\n",
    " 'DlyPrcFlg',\n",
    " 'DlyPrcVol',\n",
    " 'DlyPrevCap',\n",
    " 'DlyPrevCapFlg',\n",
    " 'DlyPrevDt',\n",
    " 'DlyPrevPrc',\n",
    " 'DlyPrevPrcFlg',\n",
    " 'DlyRet',\n",
    " 'DlyRetDurFlg',\n",
    " 'DlyRetI',\n",
    " 'DlyRetMissFlg',\n",
    " 'DlyRetx',\n",
    " 'DlyVol',\n",
    " 'ExchangeTier',\n",
    " 'HdrCUSIP',\n",
    " 'HdrCUSIP9',\n",
    " 'ICBIndustry',\n",
    " 'IssuerNm',\n",
    " 'IssuerType',\n",
    " 'NAICS',\n",
    " 'NASDCompno',\n",
    " 'NASDIssuno',\n",
    " 'PERMCO',\n",
    " 'PERMNO',\n",
    " 'PrimaryExch',\n",
    " 'SICCD',\n",
    " 'SecInfoEndDt',\n",
    " 'SecInfoStartDt',\n",
    " 'SecurityActiveFlg',\n",
    " 'SecurityBegDt',\n",
    " 'SecurityEndDt',\n",
    " 'SecurityHdrFlg',\n",
    " 'SecurityNm',\n",
    " 'SecuritySubType',\n",
    " 'SecurityType',\n",
    " 'ShareClass',\n",
    " 'ShareType',\n",
    " 'ShrAdrFlg',\n",
    " 'ShrEndDt',\n",
    " 'ShrFacType',\n",
    " 'ShrOut',\n",
    " 'ShrSource',\n",
    " 'ShrStartDt',\n",
    " 'Ticker',\n",
    " 'TradingStatusFlg',\n",
    " 'TradingSymbol',\n",
    " 'USIncFlg',\n",
    " 'YYYYMMDD',\n",
    " 'awratd',\n",
    " 'showed',\n",
    " 'sprightly',\n",
    " 'vwretd',\n",
    " 'vwretx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the first `.dropna()` is agressive , though it will be applied row wise only and hence treat each stock \"fairly\". MSFT will have a longer history than AMZN. I think that is a good compromise. \n",
    "\n",
    "`df[['DlyPrc', 'DlyClose']].dropna().eval('DlyPrc == DlyClose').all() # DlyPrc is equal to DlyClose`\n",
    "verifies that `CRSP's` `DlyPrc` is indeed equal to `DlyClose` whenever we got an entry. Should be be interested to work only with closing prices for your project, obraining the entries for `DlyPrc` would have been sufficient.\n",
    "\n",
    "Positive surprise: I think we can retrieve VWAP like so (confirm with CRSP): \n",
    "\n",
    "`df['VWAP'] = df['DlyPrcVol'] / df['DlyVol']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a look at missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {\n",
    "    \"Ticker\": \"ticker\",\n",
    "    \"DlyCalDt\": \"date\",\n",
    "    \"DlyPrc\": \"prc\",\n",
    "    \"DlyOpen\": \"open\",\n",
    "    \"DlyHigh\": \"high\",\n",
    "    \"DlyLow\": \"low\",\n",
    "    \"DlyClose\": \"close\",\n",
    "    \"DlyVol\": \"vol\",\n",
    "    \"DlyPrcVol\": \"price_vol\",\n",
    "}\n",
    "\n",
    "usecols = [\n",
    "    \"Ticker\",\n",
    "    \"DlyCalDt\",\n",
    "    \"DlyPrc\",\n",
    "    \"DlyOpen\",\n",
    "    \"DlyHigh\",\n",
    "    \"DlyLow\",\n",
    "    \"DlyClose\",\n",
    "    \"DlyVol\",\n",
    "    \"DlyPrcVol\",\n",
    "]\n",
    "\n",
    "df_missing_data = (\n",
    "    pd.read_csv(\n",
    "        \"./../data/SP500_daily_data_stock2.csv.gz\", usecols=usecols, compression=\"gzip\"\n",
    "    )\n",
    "    .rename(columns={k: v for k, v in rename_map.items() if k in usecols})\n",
    "    .fillna(999999)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_data = (\n",
    "    pd.read_csv(\n",
    "        \"./../data/SP500_daily_data_stock2.csv.gz\",\n",
    "        usecols=[\n",
    "            \"Ticker\",\n",
    "            \"DlyCalDt\",\n",
    "            \"DlyPrc\",\n",
    "            \"DlyOpen\",\n",
    "            \"DlyHigh\",\n",
    "            \"DlyLow\",\n",
    "            \"DlyClose\",\n",
    "            \"DlyVol\",\n",
    "            \"DlyPrcVol\",\n",
    "        ],\n",
    "        compression=\"gzip\",\n",
    "    )\n",
    "    .rename(columns={k: v for k, v in rename_map.items() if k in usecols})\n",
    "    .fillna(999999)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# dg[dg.apply(lambda row: (row == 999999).any(), axis=1)]  # takes ages\n",
    "df_missing_data[(df_missing_data == 999999).any(axis=1)]  # done in about 500 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why do we have high, low and close data, but not open data?\n",
    "df_missing_data[(df_missing_data == 999999).any(axis=1)].query(\n",
    "    \" ticker == 'AAPL' \"\n",
    ").tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = load_and_process_data(\n",
    "    file_path=\"./../data/SP500_daily_data_stock2.csv.gz\",\n",
    "    usecols=[\n",
    "        \"Ticker\",\n",
    "        \"DlyCalDt\",\n",
    "        \"DlyPrc\",\n",
    "        \"DlyOpen\",\n",
    "        \"DlyHigh\",\n",
    "        \"DlyLow\",\n",
    "        \"DlyClose\",\n",
    "        \"DlyVol\",\n",
    "        \"DlyPrcVol\",\n",
    "    ],\n",
    "    compression=\"gzip\",\n",
    "    #                           selected_start_date=pd.Timestamp(2021, 1, 1),\n",
    "    #                           selected_end_date=pd.Timestamp(2022, 12, 31)\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "file_path = \"./../data/SP500_daily_data_stock2.csv.gz\"\n",
    "usecols = [\n",
    "    \"Ticker\",\n",
    "    \"DlyCalDt\",\n",
    "    \"DlyPrc\",\n",
    "    \"DlyOpen\",\n",
    "    \"DlyHigh\",\n",
    "    \"DlyLow\",\n",
    "    \"DlyClose\",\n",
    "    \"DlyVol\",\n",
    "    \"DlyPrcVol\",\n",
    "]\n",
    "compression = \"gzip\"\n",
    "df = load_and_process_data(\n",
    "    file_path=file_path, usecols=usecols, compression=compression\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df) / len(df_missing_data)  # we drop 20% of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing 20% of the data is quite significant. Yet, it is a steight-forward approach of cleaning up missing data. A more careful approach should investigate why the data are missing in the first palce and whether they can be inferred by another source or interpolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\n",
    "    \"ticker\"\n",
    "].unique().__len__()  # we have data for exactly 500 stocks; for stocks we do not have data, but that doesnt matter for our purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us now compute for how many days per stock we got data for. \n",
    "\n",
    "Companies and be included and taken off the S&P 500 index. Some startups that were not previously listed might prosper and develop into companies large enough to be included in the index, while other may be outcompeted by others, cease to exist, acquired, split up, or taken private and hence either be excluded and/or delisted. Let us investigate for how many days we got data for each stock. While doing so, we find an interesting detour related to Python performance:\n",
    "\n",
    "Python is primarily considered an interpreted language. Python code is executed by an interpreter, which reads the code at runtime and executes it line by line. This process is different from compiled languages, where the source code is transformed into machine code or bytecode before execution, typically resulting in an executable file. However, at a more detailed level, Python code is indeed compiled under the hood. More precisely, when Python code is executed, it is compiled into bytecode, which is a lower-level, platform-independent representation of the source code. This bytecode is then interpreted by the `Python Virtual Machine (PVM)`, however, compared to a purely compiled language such as `C` or `C++`, not turned into a standalone executable file. This process is automatic and transparent to the user, making Python feel like a purely interpreted language. Tools and third-party packages do exist that can package Python programs along with an interpreter into a standalone executable, but this is an additional step beyond Python's standard behavior.\n",
    "\n",
    "The important point is that parsing byte-code through the PVM imposes an overhead which costs time. Hence, `Python` is considered \"slow\". However, you can use `C` and `C++` code within `Python` to leverage performance benefits. This is a common practice for computational heavy tasks where the execution speed of `Python` is a bottleneck. Integrating `C` or `C++` code into `Python` can significantly improve the performance of certain operations, especially those that are CPU-bound, such as numerical computations, data processing, and more.\n",
    "This, however, required more detailed knowlege of the `Python compiler`, is not straight forward, and a topic for another repository. \n",
    "\n",
    "However, that does not mean we cannot speed up our code. Paricularly, we can make use of libraries that are written, at least partially, in `C` and available in Python, such as `numpy`. As `pandas` makes use of `numpy`, it is often possible to enjoy better performance, especially when we compute in-momory like we using `pandas`. Thus, it is generally good advise for the sake of performance, to \"write highl-level code thinking low-level\", and the following is meant to demonstrate this.\n",
    "\n",
    "To compute the number of days we got for each of the S&P500 members, a streight-forward (but slow) method is to loop trough each ticker, filter the data frame according to the ticker, and to compute the number of rows. This will be executed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "days_per_ticker = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    days_per_ticker[ticker] = df.query(\"ticker == @ticker\").shape[\n",
    "        0\n",
    "    ]  # takes about 16.8 seconds\n",
    "#     days_per_ticker[ticker] = df[df['ticker'] == ticker].shape[0]  # takes about 59.6 seconds and is three times slower, still."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see this step took approximately 16.8 seconds on the machine this code was executed on. Making use of the `pandas` native `.goupby()` method, which is written in `C`, and storing the results in a dictionary, achieves the same task in just about 112 ms, i.e, the computation is 150 times, i.e. an order of magnitude \n",
    "faster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "days_per_ticker = df.groupby(\"ticker\").size().to_dict();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now investigate how the length of the history of each stock [in days] is distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.hist(list(days_per_ticker.values()), bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter objects are a part of the collections module in Python's standard library.\n",
    "# They are specialized dictionary subclasses designed to count hashable objects.\n",
    "# A Counter is a collection where elements are stored as dictionary keys and their counts are stored\n",
    "# as dictionary values.\n",
    "\n",
    "Counter(list(days_per_ticker.values())).most_common(3)[\n",
    "    0\n",
    "]  # Counter(list(days_per_ticker.values())).most_common(3)[0][0] then extracts the number that occurs most often.\n",
    "\n",
    "# 81 stocks contain 7881 days of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are the stocks weighted with repect to the one with the longst history in the portfolio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_days_ticker = max(\n",
    "    days_per_ticker, key=days_per_ticker.get\n",
    ")  # find the ticker with the maximum number of days\n",
    "max_days = days_per_ticker[\n",
    "    max_days_ticker\n",
    "]  # retrieve the value (number of days) for this ticker\n",
    "print(\n",
    "    f\"The ticker with the maximum number of days is: {max_days_ticker}, with {max_days} days.\"\n",
    ")\n",
    "\n",
    "max_days = max(days_per_ticker.values())  # find the maximum number of days\n",
    "weights_per_ticker = {\n",
    "    ticker: days / max_days for ticker, days in days_per_ticker.items()\n",
    "}  # Calculate the weight for each ticker\n",
    "weights_per_ticker\n",
    "\n",
    "# z-transform the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively, we could define a start and end date ourselves and make sure to select only those stocks with a densely populated history.\n",
    "\n",
    "Densely here means that the stocks should have the same number of data. This ensures stocks thate were recently taken in are not selected as they do not contain enough data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_start_date = pd.Timestamp(2012, 1, 1)\n",
    "selected_end_date = pd.Timestamp(2022, 12, 31)\n",
    "df_filtered = df[\n",
    "    (df[\"date\"] >= selected_start_date) & (df[\"date\"] <= selected_end_date)\n",
    "]\n",
    "df_filtered.groupby(\n",
    "    \"ticker\"\n",
    ").size().value_counts()  # 374 out of the 500 stocks are of the desired duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to just go for the mode directly, we could have achieved this by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_size = df_filtered.groupby(\"ticker\").size().mode()[0]\n",
    "mode_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And counted the number of stocks of that lenth using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_size = (\n",
    "    df_filtered.groupby(\"ticker\")\n",
    "    .filter(lambda x: len(x) == mode_size)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_common_size[\"ticker\"].unique().__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we selected only those stocks that have an equal amount of days between our start and end day, we have to reduce our universe from 500 stocks to 374. This is a significant reduction that one should be sure to afford.\n",
    "\n",
    "As an alternative way, we appreaciate the different length of the data and conduct the pattern analysis for each of them separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does each stock evolve in time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit ourselves to onem year of data to see how each of the stocks in the portfolio performed relative to their starting price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_normalized_price(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"first_price_indicator\"] = np.where(df.index == 0, 1, 0)\n",
    "    df[\"first_price_value\"] = df[\"first_price_indicator\"] * df[\"close\"]\n",
    "    df[\"first_price_value\"].replace(to_replace=0, method=\"ffill\", inplace=True)\n",
    "    df[\"normalized_price\"] = df[\"close\"] / df[\"first_price_value\"]\n",
    "    df.drop(columns=[\"first_price_indicator\", \"first_price_value\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation using a multi-indexed data frame\n",
    "result = df_common_size.set_index([\"ticker\", \"date\"]).join(\n",
    "    df_common_size.groupby(\"ticker\").first().add_prefix(\"first_\")\n",
    ")  # dg.set_index(['Date','ListingId']) will be equivalent to the vectorized version\n",
    "result[\"normalized_price\"] = result[\"close\"] / result[\"first_close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the data. Note you can select a nunber of stocks via the variable \"counter\" as well.\n",
    "\n",
    "selected_start_date = pd.Timestamp(2022, 1, 1)\n",
    "selected_end_date = pd.Timestamp(2022, 12, 31)\n",
    "\n",
    "df_filtered = df[\n",
    "    (df[\"date\"] >= selected_start_date) & (df[\"date\"] <= selected_end_date)\n",
    "]\n",
    "df_filtered\n",
    "\n",
    "mode_size = df_filtered.groupby(\"ticker\").size().mode()[0]\n",
    "\n",
    "df_common_size = (\n",
    "    df_filtered.groupby(\"ticker\")\n",
    "    .filter(lambda x: len(x) == mode_size)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "result = df_common_size.set_index([\"ticker\", \"date\"]).join(\n",
    "    df_common_size.groupby(\"ticker\").first().add_prefix(\"first_\")\n",
    ")  # dg.set_index(['Date','ListingId']) will be equivalent to the vectorized version\n",
    "result[\"normalized_price\"] = result[\"close\"] / result[\"first_close\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for ticker, data in result.groupby(level=\"ticker\"):\n",
    "    plt.plot(\n",
    "        data.index.get_level_values(\"date\"), data[\"normalized_price\"], label=ticker\n",
    "    )\n",
    "#     counter += 1\n",
    "#     if counter == 20:\n",
    "#         break\n",
    "\n",
    "# plt.legend(title='Ticker', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.title(\"Normalized price by date for the selected stocks\")\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"normalized price\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the stock prices are not adapted to stock splits. As we work with intraday returns, we get away dealing with price-adjustments which, strictly speaking, is a topic in its own right. However, we do see the investment universe evolved, seemingly randomly, and there were constituents that performed positively, neutral, and negatively. \n",
    "\n",
    "Hence, selecting one stock in hindsight, and evaluating its buy-and-hold performance, is subject to bias. We will test in the following, how the candlestick patterns perform for the investment universe, to see whether they allow a a portfolion to be actively managed in a long-short fashion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candlestick analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pipeline pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i) is there a smarter way of passing input arguments to load_and_process_data?\n",
    "\n",
    "# ii) can I somehow cache the data frame rather than reading it from disk every single time?\n",
    "\n",
    "\n",
    "def data_processing_pipeline(\n",
    "    file_path: str = \"./../data/SP500_daily_data_stock2.csv.gz\",\n",
    "    usecols: List[str] = [\n",
    "        \"Ticker\",\n",
    "        \"DlyCalDt\",\n",
    "        \"DlyPrc\",\n",
    "        \"DlyOpen\",\n",
    "        \"DlyHigh\",\n",
    "        \"DlyLow\",\n",
    "        \"DlyClose\",\n",
    "        \"DlyVol\",\n",
    "        \"DlyPrcVol\",\n",
    "    ],\n",
    "    compression: Optional[str] = \"gzip\",\n",
    "    ticker: Optional[str] = None,\n",
    "    selected_start_date: Optional[pd.Timestamp] = None,\n",
    "    selected_end_date: Optional[pd.Timestamp] = None,\n",
    ") -> pd.DataFrame:\n",
    "    df = load_and_process_data(\n",
    "        file_path, usecols, compression, ticker, selected_start_date, selected_end_date\n",
    "    )\n",
    "\n",
    "    steps = [cs_pattern_recognition, cs_performance]\n",
    "\n",
    "    for step in steps:\n",
    "        df = step(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_processing_pipeline(\n",
    "    ticker=\"AAPL\",\n",
    "    selected_start_date=pd.Timestamp(2020, 1, 1),\n",
    "    selected_end_date=pd.Timestamp(2022, 12, 31),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = tickers[0]\n",
    "df.query(\"ticker == @ticker\")\n",
    "cs_pattern_recognition(df=df.query(\"ticker == @ticker\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_start_date = pd.Timestamp(2021, 1, 1)\n",
    "selected_end_date = pd.Timestamp(2022, 12, 31)\n",
    "\n",
    "df_filtered = df[\n",
    "    (df[\"date\"] >= selected_start_date) & (df[\"date\"] <= selected_end_date)\n",
    "]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cs_pattern_recognition(df=df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_performance(cs_signals_df=cs_pattern_recognition(df=df_filtered)).query(\n",
    "    \"ci_lower > 0.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = cs_performance(cs_signals_df=cs_pattern_recognition(df=df_filtered))[\n",
    "    [\"ci_upper\", \"TP_wilson\", \"ci_lower\"]\n",
    "].plot(figsize=(10, 10), rot=90)\n",
    "plt.axhline(y=0.5, color=\"r\", linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cs_performance(\n",
    "    cs_performance(cs_pattern_recognition(df=df.query(\"ticker == @ticker\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cs_performance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for ticker in tickers[0]:\n",
    "    days_per_ticker[ticker] = df.query(\"ticker == @ticker\").shape[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we run this on HPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytables; pychunks; loading into batches? Is it possible to batch data in pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.3, 0.2, 0.15]]).T  # x is a column vector\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x @ x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(x @ x.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
