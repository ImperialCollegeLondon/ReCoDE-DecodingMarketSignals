{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding Market Signals: Candlestick patterns for the S&P 500 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "from scipy import stats\n",
    "\n",
    "from BSquant import load_data\n",
    "from BSquant import process_data\n",
    "from BSquant import cs_pattern_recognition\n",
    "from BSquant import cs_performance\n",
    "from BSquant import plot_cs_performance\n",
    "from BSquant import compute_trading_strategy_performance\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data for the S&P500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your ticker file\n",
    "ticker_file = \"./../data/SP500_tickers_one_per_line.txt\"\n",
    "notebooks_dir = Path(\"./../notebooks\")\n",
    "ticker_file_path = notebooks_dir.parent / \"data\" / ticker_file\n",
    "\n",
    "tickers = []\n",
    "\n",
    "# Open the ticker-file with a context manager and read each line adding ot to the list of tickers\n",
    "with open(ticker_file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        ticker = line.strip()  # Strip newline characters and whitespace\n",
    "        tickers.append(ticker)  # Add the cleaned ticker to the list\n",
    "\n",
    "print(\"Number of tickers (may include multiple tickers per stock) is\", len(tickers))\n",
    "print(\"Number of unique tickers is:\", set(tickers).__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerated tickers in the ticker_file\n",
    "for i, ticker in enumerate(tickers):\n",
    "    print(f\"{i+1}:{ticker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = \"SP500_daily_data_1980_to_2023.csv.gz\"\n",
    "notebooks_dir = Path(\"./../notebooks\")\n",
    "data_file_path = notebooks_dir.parent / \"data\" / data_filename\n",
    "print(data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# passing a function as argument to another function; load all data we got\n",
    "df = process_data(load_data(file_path=data_file_path, compression=\"gzip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ticker\"].unique().__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data fo 501 stocks though in our ticker file we got 503 tickers. We obtained the tickers from a public source and their ticker formatting might be different from `CRSP`. This is not significant for our constex, however, it is worthwhile making sure the metadata of tradable securities is updated regularly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us now compute for how many days per stock we got data for. \n",
    "\n",
    "Companies and be included and taken off the S&P 500 index. Some startups that were not previously listed might prosper and develop into companies large enough to be included in the index, while other may be outcompeted by others, cease to exist, acquired, split up, or taken private and hence either be excluded and/or delisted. Let us investigate for how many days we got data for each stock. While doing so, we find an interesting detour related to Python performance:\n",
    "\n",
    "Python is primarily considered an interpreted language. Python code is executed by an interpreter, which reads the code at runtime and executes it line by line. This process is different from compiled languages, where the source code is transformed into machine code or bytecode before execution, typically resulting in an executable file. However, at a more detailed level, Python code is indeed compiled under the hood. More precisely, when Python code is executed, it is compiled into bytecode, which is a lower-level, platform-independent representation of the source code. This bytecode is then interpreted by the `Python Virtual Machine (PVM)`, however, compared to a purely compiled language such as `C` or `C++`, not turned into a standalone executable file. This process is automatic and transparent to the user, making Python feel like a purely interpreted language. Tools and third-party packages do exist that can package Python programs along with an interpreter into a standalone executable, but this is an additional step beyond Python's standard behavior.\n",
    "\n",
    "The important point is that parsing byte-code through the PVM imposes an overhead which costs time. Hence, `Python` is considered \"slow\". However, you can use `C` and `C++` code within `Python` to leverage performance benefits. This is a common practice for computational heavy tasks where the execution speed of `Python` is a bottleneck. Integrating `C` or `C++` code into `Python` can significantly improve the performance of certain operations, especially those that are CPU-bound, such as numerical computations, data processing, and more.\n",
    "This, however, required more detailed knowlege of the `Python compiler`, is not straight forward, and a topic for another repository. \n",
    "\n",
    "However, that does not mean we cannot speed up our code. Paricularly, we can make use of libraries that are written, at least partially, in `C` and available in Python, such as `numpy`. As `pandas` makes use of `numpy`, it is often possible to enjoy better performance, especially when we compute in-momory like we using `pandas`. Thus, it is generally good advise for the sake of performance, to \"write highl-level code thinking low-level\", and the following is meant to demonstrate this.\n",
    "\n",
    "To compute the number of days we got for each of the S&P500 members, a streight-forward (but slow) method is to loop trough each ticker, filter the data frame according to the ticker, and to compute the number of rows. This will be executed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "days_per_ticker = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    days_per_ticker[ticker] = df.query(\"ticker == @ticker\").shape[\n",
    "        0\n",
    "    ]  # takes about 16.8 seconds\n",
    "#     days_per_ticker[ticker] = df[df['ticker'] == ticker].shape[0]  # takes about 59.6 seconds and is three times slower, still."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see this step took approximately 16.8 seconds on the machine this code was executed on. Making use of the `pandas` native `.goupby()` method, which is written in `C`, and storing the results in a dictionary, achieves the same task in just about 112 ms, i.e, the computation is 150 times, i.e. an order of magnitude \n",
    "faster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "days_per_ticker = df.groupby(\"ticker\").size().to_dict();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now investigate how the length of the history of each stock [in days] is distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.hist(list(days_per_ticker.values()), bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter objects are a part of the collections module in Python's standard library.\n",
    "# They are specialized dictionary subclasses designed to count hashable objects.\n",
    "# A Counter is a collection where elements are stored as dictionary keys and their counts are stored\n",
    "# as dictionary values.\n",
    "\n",
    "Counter(list(days_per_ticker.values())).most_common(3)[\n",
    "    0\n",
    "]  # Counter(list(days_per_ticker.values())).most_common(3)[0][0] then extracts the number that occurs most often.\n",
    "\n",
    "# 81 stocks contain 7881 days of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are the stocks weighted with repect to the one with the longst history in the portfolio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_days_ticker = max(\n",
    "    days_per_ticker, key=days_per_ticker.get\n",
    ")  # find the ticker with the maximum number of days\n",
    "max_days = days_per_ticker[\n",
    "    max_days_ticker\n",
    "]  # retrieve the value (number of days) for this ticker\n",
    "print(\n",
    "    f\"The ticker with the maximum number of days is: {max_days_ticker}, with {max_days} days.\"\n",
    ")\n",
    "\n",
    "max_days = max(days_per_ticker.values())  # find the maximum number of days\n",
    "weights_per_ticker = {\n",
    "    ticker: days / max_days for ticker, days in days_per_ticker.items()\n",
    "}  # Calculate the weight for each ticker, z-transform the weights should you wish to use them in ML applications\n",
    "weights_per_ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively, we could define a start and end date ourselves and make sure to select only those stocks with a densely populated history.\n",
    "\n",
    "Densely here means that the stocks should have the same number of data. This ensures stocks thate were recently taken in are not selected as they do not contain enough data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_start_date = pd.Timestamp(2012, 1, 1)\n",
    "selected_end_date = pd.Timestamp(2022, 12, 31)\n",
    "df_filtered = df[\n",
    "    (df[\"date\"] >= selected_start_date) & (df[\"date\"] <= selected_end_date)\n",
    "]\n",
    "df_filtered.groupby(\n",
    "    \"ticker\"\n",
    ").size().value_counts()  # 374 out of the 500 stocks are of the desired duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to just go for the mode directly, we could have achieved this by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_size = df_filtered.groupby(\"ticker\").size().mode()[0]\n",
    "mode_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And counted the number of stocks of that lenth using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_size = (\n",
    "    df_filtered.groupby(\"ticker\")\n",
    "    .filter(lambda x: len(x) == mode_size)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_common_size[\"ticker\"].unique().__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we selected only those stocks that have an equal amount of days between our start and end day, we have to reduce our universe from 500 stocks to 374. This is a significant reduction that one should be sure to afford.\n",
    "\n",
    "As an alternative way, we appreaciate the different length of the data and conduct the pattern analysis for each of them separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does each stock evolve in time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit ourselves to onem year of data to see how each of the stocks in the portfolio performed relative to their starting price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_normalized_price(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"first_price_indicator\"] = np.where(df.index == 0, 1, 0)\n",
    "    df[\"first_price_value\"] = df[\"first_price_indicator\"] * df[\"close\"]\n",
    "    df[\"first_price_value\"].replace(to_replace=0, method=\"ffill\", inplace=True)\n",
    "    df[\"normalized_price\"] = df[\"close\"] / df[\"first_price_value\"]\n",
    "    df.drop(columns=[\"first_price_indicator\", \"first_price_value\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation using a multi-indexed data frame\n",
    "result = df_common_size.set_index([\"ticker\", \"date\"]).join(\n",
    "    df_common_size.groupby(\"ticker\").first().add_prefix(\"first_\")\n",
    ")  # dg.set_index(['Date','ListingId']) will be equivalent to the vectorized version\n",
    "result[\"normalized_price\"] = result[\"close\"] / result[\"first_close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the data. Note you can select a nunber of stocks via the variable \"counter\" as well.\n",
    "\n",
    "selected_start_date = pd.Timestamp(2022, 1, 1)\n",
    "selected_end_date = pd.Timestamp(2022, 12, 31)\n",
    "\n",
    "df_filtered = df[\n",
    "    (df[\"date\"] >= selected_start_date) & (df[\"date\"] <= selected_end_date)\n",
    "]\n",
    "df_filtered\n",
    "\n",
    "mode_size = df_filtered.groupby(\"ticker\").size().mode()[0]\n",
    "\n",
    "df_common_size = (\n",
    "    df_filtered.groupby(\"ticker\")\n",
    "    .filter(lambda x: len(x) == mode_size)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "result = df_common_size.set_index([\"ticker\", \"date\"]).join(\n",
    "    df_common_size.groupby(\"ticker\").first().add_prefix(\"first_\")\n",
    ")  # dg.set_index(['Date','ListingId']) will be equivalent to the vectorized version\n",
    "result[\"normalized_price\"] = result[\"close\"] / result[\"first_close\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for ticker, data in result.groupby(level=\"ticker\"):\n",
    "    plt.plot(\n",
    "        data.index.get_level_values(\"date\"), data[\"normalized_price\"], label=ticker\n",
    "    )\n",
    "#     counter += 1\n",
    "#     if counter == 20:\n",
    "#         break\n",
    "\n",
    "# plt.legend(title='Ticker', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.title(\"Normalized price by date for the selected stocks\")\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"normalized price\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the stock prices are not adapted to stock splits. As we work with intraday returns, we get away dealing with price-adjustments which, strictly speaking, is a topic in its own right. However, we do see the investment universe evolved, seemingly randomly, and there were constituents that performed positively, neutral, and negatively. \n",
    "\n",
    "Hence, selecting one stock in hindsight, and evaluating its buy-and-hold performance, is subject to bias. We will test in the following, how the candlestick patterns perform for the investment universe, to see whether they allow a a portfolion to be actively managed in a long-short fashion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candlestick analysis\n",
    "\n",
    "We are now in a position to analyse the whole investment universe. Unfortunately, we now need to deal with a problem we so far go around with: \"Big data analysis\" comes with \"big computational resources\". \n",
    "Recall, we need to:\n",
    " - Loading data to memory coming from 500 stocks, with a history of up to 40 years,\n",
    " - Making the pattern recogniton logic act on them, where:\n",
    "     - for every date, we create up to 61 new rows, where 61 is the number of candlestick patterns we are  able to identify using `talib`.\n",
    "     \n",
    "Unfortunately, this exceeds the memory resources of a standard workstation or laptop. \n",
    "\n",
    "In the following we hence limit ourselves to two years of data and outline how to proceed the analysis, but need to leave considering a wider time-interval to the interested reader whos has a more powerful machine available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path to the data file is the same as for the notbook discussing the single-stock case.\n",
    "data_filename = \"SP500_daily_data_1980_to_2023.csv.gz\"\n",
    "notebooks_dir = Path(\"./../notebooks\")\n",
    "data_file_path = notebooks_dir.parent / \"data\" / data_filename\n",
    "\n",
    "df = process_data(\n",
    "    load_data(\n",
    "        data_file_path,\n",
    "        selected_start_date=pd.Timestamp(2019, 1, 1),\n",
    "        selected_end_date=pd.Timestamp(2022, 12, 31),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cs_signals_df = cs_pattern_recognition(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "performance_metrics = cs_performance(cs_signals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all patterns, ranked by number of instances\n",
    "plot_cs_performance(\n",
    "    df=performance_metrics,\n",
    "    criterion=\"total_instances\",\n",
    "    title_suffix=\"across the whole data set.\",\n",
    ")\n",
    "\n",
    "# plot the patterns, ranked by number of instances, with a true-positive rate >50%.\n",
    "plot_cs_performance(\n",
    "    df=performance_metrics.query(\"ci_lower > 0.5\").sort_values(\n",
    "        by=\"total_instances\", ascending=False\n",
    "    ),\n",
    "    criterion=\"total_instances\",\n",
    "    title_suffix=\"with ci_lower > 50%.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notable this time, appreciating more data, we indeed isntances where the lower part of the confidence interval is greater than 50%. This tells us we are 95% sure that these patterns indeed correctly predict the next day's intraday return. Conversely, we are now also in a position to identify counter-signals. These are instances where the upper part of the confidence interval is below the 50% threshold. Hence, it is indicated to take action for these signals in opposite direction of what they suggest, i.e. to take them as contrarian signals. \n",
    "\n",
    "Let us visualise these reults in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics.query(\"ci_lower > 0.5\").sort_values(\n",
    "    by=[\"ci_lower\"], ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics.query(\"ci_upper < 0.5\").sort_values(\n",
    "    by=[\"ci_upper\"], ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cs_performance(\n",
    "    df=performance_metrics, criterion=\"TP_wilson\", plot_performance=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this analysis, we now name signals and contrarian signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics.query(\"ci_lower > 0.5\").index  # signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics.query(\"ci_upper < 0.5\").index  # anti signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can test strategies that contain only those candlestick patterns that have proven to be profitable and/or those which manifsted themselves as anti-signals. \n",
    "\n",
    "Also you now can implement your own Machine-Learning logics to see whether you can come up with your own logic. Also, you can run the logic on a more potent machine, to see how the precision and confidence intervals change per candlestick pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying candlestick analysis across the S&P 500 universe\n",
    "\n",
    "Unfortunately, the data provider does not have OHLC data for the S&P 500 index. At the time of writing, an inquiry is still ongoing. In the following, we illustrate how we obtain a synthetic performance reference nontheless, namely by computing the mean intraday return at any day acroos all the oniverse assuming equal weights. This shoudl serve as an approximate solution that works with the data at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic S&P 500 intraday performacne\n",
    "df_reference_strategy = (\n",
    "    df[[\"ticker\", \"date\", \"intraday_return\"]]\n",
    "    .pivot_table(index=\"date\", columns=\"ticker\")\n",
    "    .mean(axis=1)\n",
    ")\n",
    "df_reference_strategy = df_reference_strategy.rename(\"intraday_return\").reset_index()\n",
    "df_reference_strategy[\"account_curve\"] = (\n",
    "    1 + df_reference_strategy[\"intraday_return\"]\n",
    ").cumprod()\n",
    "df_reference_strategy[\"cumsumret\"] = df_reference_strategy[\"intraday_return\"].cumsum()\n",
    "df_reference_strategy.plot(x=\"date\", y=\"account_curve\", figsize=(8, 8))\n",
    "plt.show()\n",
    "\n",
    "compute_trading_strategy_performance(df=df_reference_strategy, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can also opt for a method that compares the buy-and hold approach against employign the naive candlestick strategy. Recall the naive candlestick approach was to sum up the signal for each day across all the candlestick patterns. We can then compare the Sharpe Ration for both, the buy-and-hold approach, as well as the active candlestick approach and compare which of them delivers higher risk-adjusted returns, if at all.\n",
    "This is carrie dout in the below, whearas we slightly modified the approach of the single-stock method from notebook 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Sharpe Ratios for a passive and acive trading strategy for the S&P 500 universe\n",
    "\n",
    "We now turn our focus to comparing the Sharpe Ratios from both, passive and active trading strategies, examining their distribution through histograms, cumulative distribution functions, and box plots. Additionally, we employ specific functions for a detailed statistical analysis. \n",
    "\n",
    "Precisely, `analyse_Sharpe_Ratios_for_active_and_passive_strategies()` visualizes their distributions, `analyze_sharpe_ratios()` delves into their statistical characteristics, and `compare_sharpe_ratios()` statistically determines if the active strategy's Sharpe Ratios significantly outperform those of a reference strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_Sharpe_Ratios_for_active_and_passive_strategies(\n",
    "    SR_buy_and_hold: np.array, SR_naive_cs: np.array\n",
    ") -> None:\n",
    "    # determine the combined range of both Sharpe Ratios\n",
    "    all_ratios = np.concatenate((SR_buy_and_hold, SR_naive_cs))\n",
    "    min_edge = all_ratios.min()\n",
    "    max_edge = all_ratios.max()\n",
    "    bins = np.linspace(\n",
    "        min_edge, max_edge, 40\n",
    "    )  # 40 equal-width bins across the full range\n",
    "\n",
    "    # compute empirical CDF for SR_buy_and_hold\n",
    "    sorted_SR_buy_and_hold = np.sort(SR_buy_and_hold)\n",
    "    yvals_buy_and_hold = np.arange(1, len(sorted_SR_buy_and_hold) + 1) / float(\n",
    "        len(sorted_SR_buy_and_hold)\n",
    "    )\n",
    "\n",
    "    # compute empirical CDF for SR_naive_cs\n",
    "    sorted_SR_naive_cs = np.sort(SR_naive_cs)\n",
    "    yvals_naive_cs = np.arange(1, len(sorted_SR_naive_cs) + 1) / float(\n",
    "        len(sorted_SR_naive_cs)\n",
    "    )\n",
    "\n",
    "    # plotting\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 18))\n",
    "\n",
    "    # histograms\n",
    "    axs[0].hist(SR_buy_and_hold, bins=bins, alpha=0.5, label=\"Buy and hold\")\n",
    "    axs[0].hist(SR_naive_cs, bins=bins, alpha=0.5, label=\"Naive CS\")\n",
    "    axs[0].set_title(\"Comparison of Sharpe Ratios: Buy and hold vs. Naive CS\")\n",
    "    axs[0].set_xlabel(\"Sharpe Ratio\")\n",
    "    axs[0].set_ylabel(\"Frequency\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    # empirical CDFs\n",
    "    axs[1].plot(\n",
    "        sorted_SR_buy_and_hold,\n",
    "        yvals_buy_and_hold,\n",
    "        label=\"Buy and hold\",\n",
    "        marker=\".\",\n",
    "        linestyle=\"none\",\n",
    "    )\n",
    "    axs[1].plot(\n",
    "        sorted_SR_naive_cs,\n",
    "        yvals_naive_cs,\n",
    "        label=\"Naive CS\",\n",
    "        marker=\".\",\n",
    "        linestyle=\"none\",\n",
    "    )\n",
    "    axs[1].set_title(\"Empirical CDF of Sharpe Ratios\")\n",
    "    axs[1].set_xlabel(\"Sharpe Ratio\")\n",
    "    axs[1].set_ylabel(\"CDF\")\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # box Plots\n",
    "    axs[2].boxplot([SR_buy_and_hold, SR_naive_cs], labels=[\"Buy and hold\", \"Naive CS\"])\n",
    "    axs[2].set_title(\"Box Plot of Sharpe Ratios\")\n",
    "    axs[2].set_ylabel(\"Sharpe Ratio\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def analyze_sharpe_ratios(SR_buy_and_hold: np.array, SR_naive_cs: np.array) -> None:\n",
    "    print(\"Buy and Hold Strategy:\")\n",
    "    print(f\"Mean Sharpe Ratio: {np.mean(SR_buy_and_hold):.4f}\")\n",
    "    print(f\"Median Sharpe Ratio: {np.median(SR_buy_and_hold):.4f}\")\n",
    "    print(f\"Kurtosis: {stats.kurtosis(SR_buy_and_hold):.4f}\")\n",
    "    print(f\"Skewness: {stats.skew(SR_buy_and_hold):.4f}\\n\")\n",
    "\n",
    "    print(\"Naive Candlestick Strategy:\")\n",
    "    print(f\"Mean Sharpe Ratio: {np.mean(SR_naive_cs):.4f}\")\n",
    "    print(f\"Median Sharpe Ratio: {np.median(SR_naive_cs):.4f}\")\n",
    "    print(f\"Kurtosis: {stats.kurtosis(SR_naive_cs):.4f}\")\n",
    "    print(f\"Skewness: {stats.skew(SR_naive_cs):.4f}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def compare_sharpe_ratios(SR_buy_and_hold: np.array, SR_naive_cs: np.array) -> None:\n",
    "    t_stat, p_value = stats.ttest_ind(\n",
    "        SR_naive_cs, SR_buy_and_hold, alternative=\"greater\"\n",
    "    )\n",
    "\n",
    "    print(f\"t-statistic: {t_stat}\")\n",
    "    print(f\"p-value: {p_value}\")\n",
    "\n",
    "    # Interpret the p-value\n",
    "    if p_value < 0.05:\n",
    "        print(\n",
    "            \"The naive candlestick strategy has significantly greater Sharpe Ratios than the buy-and-hold strategy at the 5% significance level.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"There is no significant difference in Sharpe Ratios in favour of the naive candlestick strategy over the buy-and-hold strategy at the 5% significance level.\"\n",
    "        )\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach: Taking into account all candlestick patterns\n",
    "\n",
    "Now, we compute the Sharpe Ratios for the active, as well as passive (reference) strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# we should loop through all of the tickers to create trading signals for each stock\n",
    "\n",
    "StrategyPerformance = namedtuple(\n",
    "    \"StrategyPerformance\", [\"SR_buy_and_hold\", \"SR_naive_cs\"]\n",
    ")\n",
    "naive_cs_vs_buy_and_hold_performance = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    df_single_stock = df[df[\"ticker\"] == ticker]\n",
    "    cs_single_stock_signals_df = cs_signals_df[cs_signals_df[\"ticker\"] == ticker]\n",
    "\n",
    "    trading_signal = (\n",
    "        cs_single_stock_signals_df.query(\"cs_pattern != 0\")\n",
    "        .pivot_table(index=\"date\", columns=\"candle\", values=\"cs_pattern\", aggfunc=\"sum\")\n",
    "        .sum(axis=1)\n",
    "        .loc[lambda x: x != 0]\n",
    "    )\n",
    "\n",
    "    performance_trading_signals = (\n",
    "        df_single_stock[\n",
    "            df_single_stock[\"date\"].isin(\n",
    "                [date + pd.DateOffset(days=1) for date in trading_signal.index]\n",
    "            )\n",
    "        ][[\"date\", \"intraday_return\"]]\n",
    "        .assign(account_curve=lambda x: (1 + x[\"intraday_return\"]).cumprod())\n",
    "        .assign(cumsumret=lambda x: x[\"intraday_return\"].cumsum())\n",
    "        .assign(time_between_signals=lambda x: x[\"date\"].diff().dt.days)\n",
    "    )\n",
    "\n",
    "    (_, _, SR_buy_and_hold) = compute_trading_strategy_performance(df=df_single_stock)\n",
    "    (_, _, SR_naive_cs) = compute_trading_strategy_performance(\n",
    "        df=performance_trading_signals\n",
    "    )\n",
    "\n",
    "    naive_cs_vs_buy_and_hold_performance[ticker] = StrategyPerformance(\n",
    "        SR_buy_and_hold=SR_buy_and_hold, SR_naive_cs=SR_naive_cs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR_buy_and_hold = np.array(\n",
    "    [\n",
    "        performance.SR_buy_and_hold\n",
    "        for performance in naive_cs_vs_buy_and_hold_performance.values()\n",
    "        if not np.isnan(performance.SR_buy_and_hold)\n",
    "    ]\n",
    ")\n",
    "SR_naive_cs = np.array(\n",
    "    [\n",
    "        performance.SR_naive_cs\n",
    "        for performance in naive_cs_vs_buy_and_hold_performance.values()\n",
    "        if not np.isnan(performance.SR_naive_cs)\n",
    "    ]\n",
    ")\n",
    "\n",
    "analyse_Sharpe_Ratios_for_active_and_passive_strategies(\n",
    "    SR_buy_and_hold=SR_buy_and_hold, SR_naive_cs=SR_naive_cs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_sharpe_ratios(SR_buy_and_hold=SR_buy_and_hold, SR_naive_cs=SR_naive_cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, both the mean and the median Sharpe Ratio for the naive candlestick approach (active trading strategy) are higher than for the passive (buy-and-hold) approach. Interestingly, also the skewness is slighly lower for the active approach. Recall that skewness values close to zero suggest a symmetrical distribution of returns around the mean. For the buy-and-hold strategy, a skewness of -0.3861 indicates a skew to the left, suggesting a distribution with a fatter left tail indicating more frequent negative extreme returns than positive ones. The candlestick approach's skewness of -0.2999 also indicates a leftward skew but to a lesser extent, suggesting a slightly more symmetric distribution of returns around the mean compared to the buy-and-hold strategy. \n",
    "\n",
    "Unfortunately, the extra returns obtained form the candlestick approach do not come for free: They are also more risk as indicated by the larger kurtosis. Also note we did not assume any transation costs. For a trading company, a market maker, or a large bank, which are in a position to negotiate lower transaction costs than retail traders, these are less of an issue. However, an active approach involving daily transactions for a retail trader is disadvantageous, to say the least. The active candlestick strategy hence appears more suitable for risk seeking speculatnts who are in pursuit of \"high-risk-high-return\" bets. \n",
    "\n",
    "However, it is noted that the outperformace of mean return, median return, and a smaller skew for the active candlestick approach are indeed interesting observations. In order to determine whether higher Sharpe Ratios for the candlestick approach are statistically significantly greater than the ones for the passive strategy, we perform a one-sided t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sharpe_ratios(SR_buy_and_hold, SR_naive_cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second approach: Filter only statisticaly significant cs patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_signals = performance_metrics.query(\"ci_lower > 0.5\").index  # signals\n",
    "counter_signals = performance_metrics.query(\n",
    "    \"ci_upper < 0.5\"\n",
    ").index  # anti-signals/contrarians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "StrategyPerformance = namedtuple(\n",
    "    \"StrategyPerformance\", [\"SR_buy_and_hold\", \"SR_naive_cs\"]\n",
    ")\n",
    "naive_cs_vs_buy_and_hold_performance = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    df_single_stock = df[df[\"ticker\"] == ticker]\n",
    "    cs_single_stock_signals_df = cs_signals_df[cs_signals_df[\"ticker\"] == ticker]\n",
    "\n",
    "    # create a copy for modification\n",
    "    filtered_signals_df = cs_single_stock_signals_df.copy()\n",
    "\n",
    "    # apply the filter directly to this copy\n",
    "    filter_mask = filtered_signals_df.index.get_level_values(\"candle\").isin(\n",
    "        positive_signals.union(counter_signals)\n",
    "    )\n",
    "    filtered_signals_df = filtered_signals_df.loc[filter_mask]\n",
    "\n",
    "    # adjust 'cs_pattern' by multiplying by -1 for counter signals\n",
    "    counter_signals_mask = filtered_signals_df.index.get_level_values(\"candle\").isin(\n",
    "        counter_signals\n",
    "    )\n",
    "    filtered_signals_df.loc[counter_signals_mask, \"cs_pattern\"] *= -1\n",
    "\n",
    "    trading_signal = (\n",
    "        filtered_signals_df.query(\"cs_pattern != 0\")\n",
    "        .pivot_table(index=\"date\", columns=\"candle\", values=\"cs_pattern\", aggfunc=\"sum\")\n",
    "        .sum(axis=1)\n",
    "        .loc[lambda x: x != 0]\n",
    "    )\n",
    "\n",
    "    performance_trading_signals = (\n",
    "        df_single_stock[\n",
    "            df_single_stock[\"date\"].isin(\n",
    "                [date + pd.DateOffset(days=1) for date in trading_signal.index]\n",
    "            )\n",
    "        ][[\"date\", \"intraday_return\"]]\n",
    "        .assign(account_curve=lambda x: (1 + x[\"intraday_return\"]).cumprod())\n",
    "        .assign(cumsumret=lambda x: x[\"intraday_return\"].cumsum())\n",
    "        .assign(time_between_signals=lambda x: x[\"date\"].diff().dt.days)\n",
    "    )\n",
    "\n",
    "    (_, _, SR_buy_and_hold) = compute_trading_strategy_performance(df=df_single_stock)\n",
    "    (_, _, SR_naive_cs) = compute_trading_strategy_performance(\n",
    "        df=performance_trading_signals\n",
    "    )\n",
    "\n",
    "    naive_cs_vs_buy_and_hold_performance[ticker] = StrategyPerformance(\n",
    "        SR_buy_and_hold=SR_buy_and_hold, SR_naive_cs=SR_naive_cs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR_buy_and_hold = np.array(\n",
    "    [\n",
    "        performance.SR_buy_and_hold\n",
    "        for performance in naive_cs_vs_buy_and_hold_performance.values()\n",
    "        if not np.isnan(performance.SR_buy_and_hold)\n",
    "    ]\n",
    ")\n",
    "SR_naive_cs = np.array(\n",
    "    [\n",
    "        performance.SR_naive_cs\n",
    "        for performance in naive_cs_vs_buy_and_hold_performance.values()\n",
    "        if not np.isnan(performance.SR_naive_cs)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_sharpe_ratios(SR_buy_and_hold=SR_buy_and_hold, SR_naive_cs=SR_naive_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sharpe_ratios(SR_buy_and_hold, SR_naive_cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from above, the naive Candlestick strategy utilizing all patterns was characterised by the following performacne metrics:\n",
    "\n",
    "```\n",
    "Mean Sharpe Ratio: 0.2325\n",
    "Median Sharpe Ratio: 0.2617\n",
    "Kurtosis: 1.7315\n",
    "Skewness: -0.2999\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Upon filtering the candlestick signals and contrarian signals that were found to be indicating a price move on the next day at the 5% significance level, we could improve the all performance statistics, based on the Sharpe Ratio, in all four categories examined. The full set consists of 61 candlestick patterns, whereas the filtered approach consists of 12 signals and 15 counter signals. \n",
    "\n",
    "The signals were identified at the 5% significance level to be:\n",
    "\n",
    "```\n",
    "Index(['CDLDOJI', 'CDLLONGLEGGEDDOJI', 'CDLRICKSHAWMAN', 'CDLHAMMER',\n",
    "       'CDLMATCHINGLOW', 'CDLINVERTEDHAMMER', 'CDLGRAVESTONEDOJI',\n",
    "       'CDLSEPARATINGLINES', 'CDLSTICKSANDWICH', 'CDLUNIQUE3RIVER',\n",
    "       'CDLCOUNTERATTACK', 'CDL3LINESTRIKE'],\n",
    "      dtype='object', name='candle')\n",
    "```\n",
    "\n",
    "The contrarian signals were identified at the 5% significance level to be:\n",
    "```\n",
    "Index(['CDLSPINNINGTOP', 'CDLLONGLINE', 'CDLBELTHOLD', 'CDLCLOSINGMARUBOZU',\n",
    "       'CDLSHORTLINE', 'CDLENGULFING', 'CDLMARUBOZU', 'CDL3OUTSIDE',\n",
    "       'CDLSHOOTINGSTAR', 'CDLEVENINGSTAR', 'CDLDARKCLOUDCOVER', 'CDLPIERCING',\n",
    "       'CDLEVENINGDOJISTAR', 'CDLTRISTAR', 'CDL2CROWS'],\n",
    "      dtype='object', name='candle')\n",
    "```\n",
    "\n",
    "Notably, the filtered candlestick approach outperforms the naive buy strategy in all four performance categories investigated. Moreover, a one-sided `t-test` revealed that the Sharpe Ratios obtained by the filtered candlestick approach is greated than those obtained by the naive buy approach at the 5% level.\n",
    "\n",
    "In further research, one could run the very same code on a more potent machine and simply select a longer data range when loading in the data, to see whether the results reported here still hold. Also, one could attempt an expanding-window approach regarding the considered time-frame to investigate how performance changes in time and whether there are stocks for which the candlestick approach works particularly good or bad. The data source considered for this analysis were the past two years for all S&P 500 components, although for some stocks, there exists data dating back to the 1980ies. Assuming a densely populated data this equates to an upper boundary of 20 000 years of daily stock OHLC data.\n",
    "\n",
    "\n",
    "It remains, however, that the level of analysis carried out and presented here, required access to proprietary data and significant computing power. These could be, for example, the High Performance Computing (HPC) facilities at Imperial College, or a potent private institution. Moreover, an active trading approach is predominantly aimed at players like large hedge funds and investment banks which still have proprietary trading teams, and that are in a position to negotiate low transaction costs. They should also be potent enough to observe, and act, on data streams across the entire S&P 500 universe. An extention to any other index, such as the STOXX600 or any Asian index is easily diable using the existing code. \n",
    "\n",
    "For fund managers, with a more passive approach, the presented analysis can be interesting to optimise their entry points opon which to accummulate or offload positions. \n",
    "\n",
    "For brokers, the presented analysis is useful to craft an arrival strategies to which to adopt their execution logics based on an oppinion whether a stock goes up or down. In case of no signal on a particularly stock, one then would simply fall back to a default behaviour.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML approach\n",
    "\n",
    "The author cannot run the ML approach as we do not have enough memory available to load the required history of the stocks. ML methods are inherently data hungy, so loading just two years of data per stock will not be enough for meaningful results. Also, we cannot mix the hostory of one stock with the history of another, as financial data is chronological in nature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
